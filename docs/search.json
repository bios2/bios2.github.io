[
  {
    "objectID": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html",
    "href": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html",
    "title": "Sensibilisation aux réalités autochtones et recherche collaborative",
    "section": "",
    "text": "Améliorer notre compréhension du passé et de ses impacts sur nos relations entre le avec les Peuples Autochtones.\nDévelopper des notions et compétences afin d’agir contre les préjugés et le racisme.\n\n\n\n\n\nFaire un survol des événements historiques importants et de leurs impacts à ce jour (Loi sur les Indiens, politiques d’assimilation, les pensionnats, etc.). \nAcquérir des connaissances sur la terminologie autochtone.\nFaire un survol de certains procès et contextes légaux et voir comment ils affectent notre travail en territoire autochtone.\nDans une optique de réconciliation, faire une prise de conscience des préjugés persistants et discuter de stratégies pour améliorer nos relations avec les communautés."
  },
  {
    "objectID": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#objectifs-de-la-formation-1",
    "href": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#objectifs-de-la-formation-1",
    "title": "Sensibilisation aux réalités autochtones et recherche collaborative",
    "section": "Objectifs de la formation :",
    "text": "Objectifs de la formation :\n\nEntamer une réflexion collective envers nos pratiques de recherche et comment s’engager de manière significative avec les communautés autochtones.\nDévelopper une meilleure compréhension des perceptions et attentes des communautés envers la recherche et les chercheurs."
  },
  {
    "objectID": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#durant-ce-webminaire-nous-allons",
    "href": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#durant-ce-webminaire-nous-allons",
    "title": "Sensibilisation aux réalités autochtones et recherche collaborative",
    "section": "Durant ce webminaire, nous allons: ",
    "text": "Durant ce webminaire, nous allons: \n\nMieux comprendre la nécessité de prendre en compte les connaissances autochtones dans divers aspects de la gestion environnementale au Canada; \nDiscuter du désir des communauté d’avoir une présence accrue dans le milieu de la recherche : comment faire?\nAborder et débattre des différentes approches méthodologiques pour établir des ponts en les connaissances autochtones et scientifiques."
  },
  {
    "objectID": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#formatrice",
    "href": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/index.html#formatrice",
    "title": "Sensibilisation aux réalités autochtones et recherche collaborative",
    "section": "Formatrice :",
    "text": "Formatrice :\nCatherine-Alexandra Gagnon possède une expertise dans le travail collaboratif en milieux autochtones. Elle s’intéresse particulièrement à la mise en commun des savoirs locaux, autochtones et scientifiques. Elle détient un doctorat en Sciences de l’environnement et une maîtrise en Gestion de la faune de l’Université du Québec à Rimouski, un baccalauréat en biologie faunique de l’université McGill ainsi qu’un certificat en Études autochtones de l’université de Montréal. Durant ses études, elle a travaillé sur les connaissances locales et ancestrales des Aîné(e)s et chasseurs Inuit, Inuvialuit et Gwich’in du Nunavut, des Territoires du Nord-Ouest et du Yukon."
  },
  {
    "objectID": "posts/2020-09-21-data-visualization/index.html",
    "href": "posts/2020-09-21-data-visualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Welcome!\nThis training covers the general principles of visualization and graphic design, and techniques of tailored visualization. More specifically, the objectives of the training are:"
  },
  {
    "objectID": "posts/2020-09-21-data-visualization/index.html#training-material",
    "href": "posts/2020-09-21-data-visualization/index.html#training-material",
    "title": "Data Visualization",
    "section": "Training material",
    "text": "Training material\nClick on “Show code” to learn how to do each plot!\n\nInteractive examples\n\n\n\n\nStreamgraph\n\n\nShow code\n# Script to make a streamgraph of the top 10 most popular dog breeds in \n# New York City from 1999 to 2015\n\n# load libraries\nlibrary(lubridate) # dealing with dates\nlibrary(dplyr) # data manipulation\nlibrary(streamgraph) #devtools::install_github(\"hrbrmstr/streamgraph\")\nlibrary(htmlwidgets) # to save the widget!\n\n# load the dataset\n# more information about this dataset can be found here:\n# https://www.kaggle.com/smithaachar/nyc-dog-licensing-clean\nnyc_dogs <- read.csv(\"data/nyc_dogs.csv\")\n\n# convert birth year to date format (and keep only the year)\nnyc_dogs$AnimalBirthYear <- mdy_hms(nyc_dogs$AnimalBirthMonth) %>% year()\n\n# identify 10 most common dogs\ntopdogs <- nyc_dogs %>% count(BreedName) \ntopdogs <- topdogs[order(topdogs$n, decreasing = TRUE),]\n# keep 10 most common breeds (and remove last year of data which is incomplete)\ndf <- filter(nyc_dogs, BreedName %in% topdogs$BreedName[2:11] & AnimalBirthYear < 2016) %>% \n  group_by(AnimalBirthYear) %>% \n  count(BreedName) %>% ungroup()\n\n# get some nice colours from viridis (magma)\ncols <- viridis::viridis_pal(option = \"magma\")(length(unique(df$BreedName)))\n\n# make streamgraph!\npp <- streamgraph(df, \n                  key = BreedName, value = n, date = AnimalBirthYear, \n                  height=\"600px\", width=\"1000px\") %>%\n  sg_legend(show=TRUE, label=\"names: \") %>%\n  sg_fill_manual(values = cols) \n# saveWidget(pp, file=paste0(getwd(), \"/figures/dogs_streamgraph.html\"))\n\n# plot\npp\n\n\n\n\n\n\n\n\n\n\nInteractive plot\n\n\nShow code\n# Script to generate plots to demonstrate how combinations of information dimensions\n# can become overwhelming and difficult to interpret.\n\n# set-up & data manipulation ---------------------------------------------------\n\n# load packages\nlibrary(ggplot2) # for plots, built layer by layer\nlibrary(dplyr) # for data manipulation\nlibrary(magrittr) # for piping\nlibrary(plotly) # interactive plots\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# import data\n# more info on this dataset: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-28/readme.md\npenguins <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv') \n\n# get some nice colours from viridis (magma)\nsp_cols <- viridis::viridis_pal(option = \"magma\")(5)[2:4]\n\n\n#### Day 1 ####\n\n# 1. Similarity\n\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, col = species), size = 2.5) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Species\") + # labels\n  scale_color_manual(values = sp_cols) # sets the colour scale we created above \n\n\n\n\n\nShow code\nggsave(\"figures/penguins_similarity.png\", width = 6, height = 3, units = \"in\")\n\n# 2. Proximity\n\ndf <- penguins %>% group_by(sex, species) %>% \n  summarise(mean_mass = mean(body_mass_g, na.rm = TRUE)) %>% na.omit() \nggplot(df) +\n  geom_bar(aes(y = mean_mass, x = species, fill = sex), \n           position = \"dodge\", stat = \"identity\") +\n  labs(x = \"Species\", y = \"Mean body mass (g)\", col = \"Sex\") + # labels\n  scale_fill_manual(values = sp_cols) # sets the colour scale we created above\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_proximity.png\", width = 6, height = 3, units = \"in\")\n\n# 3. Enclosure (Ellipses over a fake PCA)\nggplot(data = penguins, \n       aes(y = bill_length_mm, x = bill_depth_mm)) +\n  geom_point(size = 2.1, col = \"grey30\") +\n  stat_ellipse(aes(col = species), lwd = .7) +\n  labs(x = \"PCA1\", y = \"PCA2\", col = \"Species\") + # labels\n  scale_color_manual(values = sp_cols) + # sets the colour scale we created above\n  theme(axis.text = element_blank(), axis.ticks = element_blank())\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_enclosure.png\", width = 6, height = 3, units = \"in\")\n\n# 4. Mismatched combination of principles\ntemp_palette <- rev(c(sp_cols, \"#1f78b4\", \"#33a02c\"))\nggplot(data = penguins, \n       aes(y = bill_length_mm, x = bill_depth_mm)) +\n  geom_point(aes(col = sex), size = 2.1) +\n  stat_ellipse(aes(col = species), lwd = .7) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"?\") + # labels\n  scale_color_manual(values = temp_palette) # sets the colour scale we created above\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_mismatchedgestalt.png\", width = 6, height = 3, units = \"in\")\n\n\n\n#### Day 2 ####\n\n# 1. Ineffective combinations: Luminance & shading -----------------------------\n\n# create the plot\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, \n                 col = species, # hue\n                 alpha = log(body_mass_g)), # luminance\n             size = 2.5) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       col = \"Species\", alpha = \"Body mass (g)\") +\n  scale_color_manual(values = sp_cols)\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_incompatible1.png\", width = 6, height = 3, units = \"in\")\n\n# 2. Ineffective combinations: Sizes and shapes --------------------------------\n\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, \n                 shape = species, # shape\n                 size = log(body_mass_g)), alpha = .7) + # size\n  scale_size(range = c(.1, 5)) + # make sure the sizes are scaled by area and not by radius\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       shape = \"Species\", size = \"Body mass (g)\") \n\n\n\n\n\nShow code\nggsave(\"figures/penguins_incompatible2.png\", width = 6, height = 3, units = \"in\")\n\n# 3. Cognitive overload --------------------------------------------------------\n\n# get some nice colours from viridis (magma)\nsex_cols <- viridis::viridis_pal(option = \"magma\")(8)[c(3,6)]\n\nggplot(na.omit(penguins)) +\n  geom_point(aes(y = bill_length_mm, # dimension 1: position along y scale\n                 x = bill_depth_mm, # dimension 2: position along x scale\n                 shape = species, # dimension 3: shape\n                 size = log(body_mass_g), # dimension 4: size\n                 col = sex), # dimension 5: hue\n             alpha = .7) + # size\n  scale_size(range = c(.1, 5)) + # make sure the sizes are scaled by area and not by radius\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       shape = \"Species\", size = \"Body mass (g)\", col = \"Sex\") +\n  scale_color_manual(values = sex_cols)\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_5dimensions.png\", width = 7, height = 4, units = \"in\")\n\n\n# 4. Panels -------------------------------------------------------------------\n\nggplot(na.omit(penguins)) +\n  geom_point(aes(y = bill_length_mm, # dimension 1: position along y scale\n                 x = bill_depth_mm, # dimension 2: position along x scale\n                 col = log(body_mass_g)), # dimension 3: hue\n             alpha = .7, size = 2) + \n  facet_wrap(~ species) + # dimension 4: species!\n  # this will create a separate panel for each species\n  # note: this also automatically uses the same axes for all panels! If you want \n  # axes to vary between panels, use the argument scales = \"free\"\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"magma\", end = .9, direction = -1) +\n  theme_linedraw() + theme(panel.grid = element_blank()) # making the panels prettier\n\n\n\n\n\nShow code\nggsave(\"figures/penguins_dimensions_facets.png\", width = 7, height = 4, units = \"in\")\n\n\n# 5. Interactive ---------------------------------------------------------------\n\np <- na.omit(penguins) %>%\n  ggplot(aes(y = bill_length_mm, \n             x = bill_depth_mm, \n             col = log(body_mass_g))) +\n  geom_point(size = 2, alpha = .7) + \n  facet_wrap(~ species) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"magma\", end = .9, direction = -1) +\n  theme_linedraw() + theme(panel.grid = element_blank()) # making the panels prettier\np <- ggplotly(p)\n#setwd(\"figures\")\nhtmlwidgets::saveWidget(as_widget(p), \"figures/penguins_interactive.html\")\np\n\n\n\n\n\n\n\n\n\nExample figures\n\n\nShow code\n# Script to make animated plot of volcano eruptions over time\n\n# Load libraries:\nlibrary(dplyr) # data manipulation\nlibrary(ggplot2) # plotting\nlibrary(gganimate) # animation\nlibrary(gifski) # creating gifs\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# function to floor a year to the decade\nfloor_decade = function(value){return(value - value %% 10)}\n\n# read data \neruptions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/eruptions.csv')\n\n# select top 5 most frequently exploding volcanoes\ntemp <- group_by(eruptions, volcano_name) %>% tally() \ntemp <- temp[order(temp$n, decreasing = TRUE),]\n\n# make a time series dataset (number of eruptions per year)\neruptions$start_decade = floor_decade(eruptions$start_year)\n\n# filter dataset to subset we want to visualize\ndf <- eruptions %>% \n  filter(between(start_decade, 1900, 2019)) %>%\n  filter(volcano_name %in% temp$volcano_name[1:5]) %>%\n  group_by(start_decade) %>%\n  count(volcano_name) %>% ungroup()\n\n# plot!\np <- ggplot(df, aes(x = start_decade, y = n, fill = volcano_name)) +\n  geom_area() +\n  geom_vline(aes(xintercept = start_decade)) + # line that follows the current decade\n  scale_fill_viridis_d(option = \"magma\", end = .8) +\n  labs(x = \"\", y = \"Number of eruptions\", fill = \"Volcano\",\n       title = 'Eruptions of the top 5 most frequently erupting volcanos worldwide') +\n  # gganimate part: reveals each decade\n  transition_reveal(start_decade) \nanimate(p, duration = 5, fps = 20, width = 800, height = 300, renderer = gifski_renderer())\n\n\n\n\n\nShow code\n#anim_save(\"figures/volcano_eruptions.gif\")\n\n\n\n\nShow code\n# Script to generate plots with various ways of representing uncertainty, based \n# Coffee & Code dataset from https://www.kaggle.com/devready/coffee-and-code/data\n\n# set-up & data manipulation ---------------------------------------------------\n\n# load packages\nlibrary(ggplot2) # for plots, built layer by layer\nlibrary(dplyr) # for data manipulation\nlibrary(magrittr) # for piping\nlibrary(ggridges) # for density ridge plots\nlibrary(patchwork) # great package for \"patching\" plots together!\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# import data\ndf <- read.csv(\"data/coffee_code.csv\")\n\n# set labels to be used in all plots\ncoffee_labels <- labs(title = \"Does coffee help programmers code?\",\n                      x = \"Coffee while coding\", \n                      y = \"Time spent coding \\n(hours/day)\") \n\n# the variable CodingWithoutCoffee is negative, which is harder to understand\n# (i.e. \"No\" means they drink coffee...). So, let's transform it into a more \n# intuitive variable!\ndf$CodingWithCoffee <- gsub(\"No\", \"Usually\", df$CodingWithoutCoffee)\ndf$CodingWithCoffee <- gsub(\"Yes\", \"Rarely\\n or never\", df$CodingWithCoffee)\n# convert to factor and set levels so they show up in a logical order\ndf$CodingWithCoffee <- factor(df$CodingWithCoffee,\n                              levels = c(\"Rarely\\n or never\", \n                                         \"Sometimes\", \n                                         \"Usually\"))\n\n# calculate summary statistics for the variable of choice\ndf_summary <- group_by(df, CodingWithCoffee) %>%\n  summarise(\n    # mean\n    mean_codinghours = mean(CodingHours), \n    # standard deviation\n    sd_codinghours = sd(CodingHours), \n    # standard error\n    se_codinghours = sd(CodingHours)/sqrt(length(CodingHours)))\n\n\n# 1. Error bars (standard error) -----------------------------------------------\n\nggplot(df_summary) +\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - se_codinghours,\n                    ymax = mean_codinghours + se_codinghours), \n                width = .2) +\n  geom_point(aes(x = CodingWithCoffee, y = mean_codinghours), \n             size = 3) +\n  coffee_labels + ylim(0,10)\n\n\n\n\n\nShow code\nggsave(\"figures/coffee_errorbars.png\", width = 5, height = 3, units = \"in\")\n\n# 2. Boxplot -------------------------------------------------------------------\n\nggplot(df) +\n  geom_boxplot(aes(x = CodingWithCoffee, y = CodingHours)) +\n  coffee_labels\n\n\n\n\n\nShow code\nggsave(\"figures/coffee_boxplot.png\", width = 5, height = 3, units = \"in\")\n\n\n# 3. Error bar demonstration ---------------------------------------------------\n\n# get some nice colours from viridis (magma)\nerror_cols <- viridis::viridis_pal(option = \"magma\")(5)[2:4]\n# set labels to be used in the palette\nerror_labels = c(\"standard deviation\",\"95% confidence interval\",\"standard error\")\n\nggplot(df_summary) +\n  # show the raw data\n  geom_jitter(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours),\n              alpha = .5, width = .05, col = \"grey\") +\n  # standard deviation\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - sd_codinghours,\n                    ymax = mean_codinghours + sd_codinghours,\n                    col = \"SD\"), width = .2, lwd = 1) +\n  # 95% confidence interval\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - 1.96*se_codinghours,\n                    ymax = mean_codinghours + 1.96*se_codinghours, \n                    col = \"CI\"), width = .2, lwd = 1) +\n  # standard error\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - se_codinghours,\n                    ymax = mean_codinghours + se_codinghours, \n                    col = \"SE\"), width = .2, lwd = 1) +\n  geom_point(aes(x = CodingWithCoffee, y = mean_codinghours), \n             size = 3) +\n  coffee_labels + ylim(c(0,11)) +\n  # manual palette/legend set-up!\n  scale_colour_manual(name = \"Uncertainty metric\", \n                      values = c(SD = error_cols[1], \n                                 CI = error_cols[2], \n                                 SE = error_cols[3]),\n                      labels = error_labels) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nShow code\nggsave(\"figures/coffee_bars_demo.png\", width = 7, height = 5, units = \"in\")\n\n\n# 4. Jitter plot with violin ---------------------------------------------------\n\nggplot() +\n  geom_jitter(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours),\n              alpha = .5, width = .05, col = \"grey\") +\n  geom_violin(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours), alpha = 0) +\n  geom_linerange(data = df_summary,\n                 aes(x = CodingWithCoffee, \n                     ymin = mean_codinghours - se_codinghours,\n                     ymax = mean_codinghours + se_codinghours)) +\n  geom_point(data = df_summary, \n             aes(x = CodingWithCoffee, \n                 y = mean_codinghours), size = 3) +\n  coffee_labels\n\n\n\n\n\nShow code\nggsave(\"figures/coffee_violin_jitter.png\", width = 5, height = 3, units = \"in\")\n\n\n# 5. Density ridge plot --------------------------------------------------------\n\nggplot(df) + \n  aes(y = CodingWithCoffee, x = CodingHours, fill = stat(x)) +\n  geom_density_ridges_gradient(scale = 1.9, size = .2, rel_min_height = 0.005) +\n  # colour palette (gradient according to CodingHours)\n  scale_fill_viridis_c(option = \"magma\", direction = -1) +\n  # remove legend - it's not necessary here!\n  theme(legend.position = \"none\") +\n  labs(title = coffee_labels$title, \n       x = coffee_labels$y, \n       y = \"Coffee \\nwhile coding\") + \n  theme(axis.title.y = element_text(angle=0, hjust = 1, vjust = .9, \n                                    margin = margin(t = 0, r = -50, b = 0, l = 0)))\n\n\n\n\n\nShow code\nggsave(\"figures/coffee_density_ridges.png\", width = 5, height = 3, units = \"in\")\n\n# 6. Jitter vs. Rug plot ------------------------------------------------------------------\n\njitterplot <- ggplot(df, aes(x = CoffeeCupsPerDay, y = CodingHours)) +\n  geom_jitter(alpha = .2) +\n  geom_smooth(fill = error_cols[1], col = \"black\", method = lm, lwd = .7) +\n  coffee_labels + ylim(c(0,11)) + labs(x = \"Cups of coffee (per day)\")\n\nrugplot <- ggplot(df, aes(x = CoffeeCupsPerDay, y = CodingHours)) +\n  geom_smooth(fill = error_cols[1], col = \"black\", method = lm, lwd = .7) +\n  geom_rug(position=\"jitter\", alpha = .7) + ylim(c(0,11)) +\n  coffee_labels + labs(x = \"Cups of coffee (per day)\")\n\n# patch the two plots together\njitterplot + rugplot\n\n\n\n\n\nShow code\n#ggsave(\"figures/coffee_jitter_vs_rugplot.png\", width = 10, height = 4, units = \"in\")\n\n\n\n\nShow code\n# Script to generate 95% confidence intervals of a generated random normal distribution\n# as an example in Day 2: Visualizing uncertainty.\n\n# load library\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(dplyr)\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# set random seed\nset.seed(22)\n\n# generate population (random normal distribution)\ndf <- data.frame(\"value\" = rnorm(50, mean = 0, sd = 1))\n\n# descriptive stats for each distribution\ndesc_stats = df %>% \n  summarise(mean_val = mean(value, na.rm = TRUE),\n            se_val = sqrt(var(value)/length(value)))\n\n# build density plot!\np <- ggplot(data = df, aes(x = value, y = ..count..)) +\n  geom_density(alpha = .2, lwd = .3) +\n  xlim(c(min(df$value-1), max(df$value+1))) \n# extract plotted values\nbase_p <- ggplot_build(p)$data[[1]]\n# shade the 95% confidence interval\np + \n  geom_area(data = subset(base_p, \n                          between(x, \n                                  left = (desc_stats$mean_val - 1.96*desc_stats$se_val),\n                                  right = (desc_stats$mean_val + 1.96*desc_stats$se_val))),\n            aes(x = x, y = y), fill = \"cadetblue3\", alpha = .6) +\n  # add vertical line to show population mean\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  annotate(\"text\", x = 0.9, y = 19, label = \"True mean\", fontface = \"italic\") +\n  # label axis!\n  labs(x = \"Variable of interest\", y = \"\") \n\n\n\n\n\nShow code\n#ggsave(\"figures/confidenceinterval_example.png\", width = 5, height = 3.5, units = \"in\")\n\n\n\n\nAnnotated resource library\nThis is an annotated library of data visualization resources we used to build the BIOS² Data Visualization Training, as well as some bonus resources we didn’t have the time to include. Feel free to save this page as a reference for your data visualization adventures!\n\n\nBooks & articles\nFundamentals of Data Visualization A primer on making informative and compelling figures. This is the website for the book “Fundamentals of Data Visualization” by Claus O. Wilke, published by O’Reilly Media, Inc.\nData Visualization: A practical introduction An accessible primer on how to create effective graphics from data using R (mainly ggplot). This book provides a hands-on introduction to the principles and practice of data visualization, explaining what makes some graphs succeed while others fail, how to make high-quality figures from data using powerful and reproducible methods, and how to think about data visualization in an honest and effective way.\nData Science Design (Chapter 6: Visualizing Data) Covers the principles that make standard plot designs work, show how they can be misleading if not properly used, and develop a sense of when graphs might be lying, and how to construct better ones.\nGraphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods Cleveland, William S., and Robert McGill. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association, vol. 79, no. 387, 1984, pp. 531–554. JSTOR, www.jstor.org/stable/2288400. Accessed 9 Oct. 2020.\nGraphical Perception and Graphical Methods for Analyzing Scientific Data Cleveland, William S., and Robert McGill. “Graphical perception and graphical methods for analyzing scientific data.” Science 229.4716 (1985): 828-833.\nFrom Static to Interactive: Transforming Data Visualization to Improve Transparency Weissgerber TL, Garovic VD, Savic M, Winham SJ, Milic NM (2016) designed an interactive line graph that demonstrates how dynamic alternatives to static graphics for small sample size studies allow for additional exploration of empirical datasets. This simple, free, web-based tool demonstrates the overall concept and may promote widespread use of interactive graphics.\nData visualization: ambiguity as a fellow traveler Research that is being done about how to visualize uncertainty in data visualizations. Marx, V. Nat Methods 10, 613–615 (2013). https://doi.org/10.1038/nmeth.2530\nData visualization standards Collection of guidance and resources to help create better data visualizations with less effort.\n\n\n\nDesign principles\nGestalt Principles for Data Visualization: Similarity, Proximity & Enclosure Short visual guide to the Gestalt Principles.\nWhy scientists need to be better at data visualization A great overview of principles that could improve how we visualize scientific data and results.\nA collection of graphic pitfalls A collection of short articles about common issues with data visualizations that can mislead or obscure your message.\n\n\n\nChoosing a visualization\nData Viz Project This is a great place to get inspiration and guidance about how to choose an appropriate visualization. There are many visualizations we are not used to seeing in ecology!\nFrom data to Viz | Find the graphic you need Interactive tool to choose an appropriate visualization type for your data.\n\n\n\nColour\nWhat to consider when choosing colors for data visualization A short, visual guide on things to keep in mind when using colour, such as when and how to use colour gradients, the colour grey, etc.\nColorBrewer: Color Advice for Maps Tool to generate colour palettes for visualizations with colorblind-friendly options. You can also use these palettes in R using the RColorBrewer package, and the scale_*_brewer() (for discrete palettes) or scale_*_distiller() (for continuous palettes) functions in ggplot2.\nColor.review Tool to pick or verify colour palettes with high relative contrast between colours, to ensure your information is readable for everyone.\nCoblis — Color Blindness Simulator Tool to upload an image and view it as they would appear to a colorblind person, with the option to simulate several color-vision deficiencies.\n500+ Named Colours with rgb and hex values List of named colours along with their hex values.\nCartoDB/CartoColor CARTOColors are a set of custom color palettes built on top of well-known standards for color use on maps, with next generation enhancements for the web and CARTO basemaps. Choose from a selection of sequential, diverging, or qualitative schemes for your next CARTO powered visualization using their online module.\n\n\n\nTools\n\nR\nThe R Graph Gallery A collection of charts made with the R programming language. Hundreds of charts are displayed in several sections, always with their reproducible code available. The gallery makes a focus on the tidyverse and ggplot2.\n\nBase R\nCheatsheet: Margins in base R Edit your margins in base R to accommodate axis titles, legends, captions, etc.!\nCustomizing tick marks in base R Seems like a simple thing, but it can be so frustrating! This is a great post about customizing tick marks with base plot in R.\nAnimations in R (for time series) If you want to use animations but don’t want to use ggplot2, this demo might help you!\n\n\nggplot2\nCheatsheet: ggplot2 Cheatsheet for ggplot2 in R - anything you want to do is probably covered here!\nCoding Club tutorial: Data Viz Part 1 - Beautiful and informative data visualization Great tutorial demonstrating how to customize titles, subtitles, captions, labels, colour palettes, and themes in ggplot2.\nCoding Club tutorial: Data Viz Part 2 - Customizing your figures Great tutorial demonstrating how to customize titles, subtitles, captions, labels, colour palettes, and themes in ggplot2.\nggplot flipbook A flipbook-style demonstration that builds and customizes plots line by line using ggplot in R.\ngganimate: A Grammar of Animated Graphics Package to create animated graphics in R (with ggplot2).\n\n\n\nPython\nThe Python Graph Gallery This website displays hundreds of charts, always providing the reproducible python code.\nPython Tutorial: Intro to Matplotlib Introduction to basic functionalities of the Python’s library Matplotlib covering basic plots, plot attributes, subplots and plotting the iris dataset.\nThe Art of Effective Visualization of Multi-dimensional Data Covers both univariate (one-dimension) and multivariate (multi-dimensional) data visualization strategies using the Python machine learning ecosystem.\n\n\nJulia\nJulia Plots Gallery Display of various plots with reproducible code in Julia.\nPlots in Julia Documentation for the Plots package in Julia, including demonstrations for animated plots, and links to tutorials.\nAnimations in Julia How to start making animated plots in Julia.\n\n\n\n\nCustomization\nChart Studio Web editor to create interactive plots with plotly. You can download the image as .html, or static images, without coding the figure yourself.\nPhyloPic Vector images of living organisms. This is great for ecologists who want to add silhouettes of their organisms onto their plots - search anything, and you will likely find it!\nAdd icons on your R plot Add special icons to your plot as a great way to customize it, and save space with labels!\n\n\n\nInspiration (pretty things!)\nInformation is Beautiful Collection of beautiful original visualizations about a variety of topics!\nTidyTuesday A weekly data project aimed at the R ecosystem, where people wrangle and visualize data in loads of creative ways. Browse what people have created (#TidyTuesday on Twitter is great too!), and the visualizations that have inspired each week’s theme.\nWind currents on Earth Dynamic and interactive map of wind currents on Earth.\nA Day in the Life of Americans Dynamic visualisation of how Americans spend their time in an average day.\n2019: The Year in Visual Stories and Graphics Collection of the most popular visualizations by the New York Times in 2019."
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html",
    "title": "Bayes for the Busy Ecologist",
    "section": "",
    "text": "This training covers how to write down a model, how to translate that into computer code, how to fit it to data and finally, how to work with the resulting posterior distribution. We’ll use Stan, which is a language for writing Bayesian models, and practice with some of the tools that help us work with Stan: rstanarm, brms, tidybayes, shinystan.\nThis training is intended for experienced users of Bayesian tools and for complete beginners who want to use such tools.\nThis 6 to 8h online workshop was conducted in 4 sessions: March 22, 24, 29, 31, 2022, from 11AM – 1 PM Pacific Time / 12 – 2 PM Mountain Time / 2-4 PM Eastern Time. The training was built and presented by Dr. Andrew MacDonald in English with bilingual support throughout.\nAndrew MacDonald is the Training Coordinator of the BIOS² program. He is a quantitative ecologist who works mostly in R and a well-experienced trainer in teaching quantitative and computational methods. He is currently a research professional at Université de Sherbrooke."
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#outline",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#outline",
    "title": "Bayes for the Busy Ecologist",
    "section": "Outline",
    "text": "Outline\n\nReturn to previous model: Poisson regression\nPanel regression version of this model\nBayesian workflow\nBrief foray into moment matching\nNonlinear model\nNonlinear model with random effects"
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#quick-review",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#quick-review",
    "title": "Bayes for the Busy Ecologist",
    "section": "Quick review",
    "text": "Quick review\n\nBird masses\nThis example is based on work by Marie-Eve at UdeS!\nWe imagine a model like the following:\n\\[\n\\begin{align}\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times \\text{Mass}_i \\\\\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\n\\beta_1 & \\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\n\\(i\\) keeps track of which bird we are talking about. You can think of it as “bird number i”\nNote: We could also write the model like this:\n\\[\n\\begin{align}\n\\text{Nestlings}_i & \\sim \\text{Poisson}(e^{\\beta_0} \\times e^{\\beta_1 \\times \\text{Mass}_i}) \\\\\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\n\\beta_1 & \\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\n\n\nCentering variables\nCentering variables is one of the most important things we can do to help our models be more interpretable. This also helps us to set good priors.\nCentering a variable means to subtract the mean from the variable:\n\\[\n\\begin{align}\n\\text{Nestlings}_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\beta_0 + \\beta_1 \\times (\\text{Mass}_i - \\overline{\\text{Mass}}) \\\\\n\\beta_0 & \\sim \\text{Normal}(??, ??) \\\\\n\\beta_1 & \\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\nQuestion How does this change the meaning of \\(\\beta_0\\) and/or \\(\\beta_1\\), if at all? (Hint: what will be the equation for a bird who has exactly average mass?)\n\nset.seed(1234)\n\nn_birds <- 15\navg_nestlings_at_avg_mass <- log(4.2)\neffect_of_one_gram <- .2\n\nmother_masses_g <- rnorm(n_birds, mean = 15, sd = 3)\navg_mother_mass <- mean(mother_masses_g)\n\nlog_average_nestlings <- avg_nestlings_at_avg_mass + \n  effect_of_one_gram * (mother_masses_g - avg_mother_mass)\n\nnestlings <- rpois(n = n_birds, lambda = exp(log_average_nestlings))\n\nPlot these to get an idea of it:\n\nsuppressPackageStartupMessages(library(tidyverse))\nimaginary_birds <- tibble(mother_masses_g, nestlings)\n\nggplot(imaginary_birds, aes(x = mother_masses_g, y = nestlings)) + \n  geom_point()\n\n\n\n\nNOTE We can also fit this very same model by frequentist statistics, using lm\n\ncoef(glm(nestlings ~ 1 + I(mother_masses_g - mean(mother_masses_g)), family = \"poisson\"))\n\n                               (Intercept) \n                                 1.4138103 \nI(mother_masses_g - mean(mother_masses_g)) \n                                 0.1727791 \n\n# compare to known values\navg_nestlings_at_avg_mass\n\n[1] 1.435085\n\neffect_of_one_gram\n\n[1] 0.2\n\n\n\n\nBayesian workflow: define a model and priors\n\nlibrary(brms)\n\nimaginary_birds_centered <- imaginary_birds |> \n  mutate(mother_mass_g_cen = mother_masses_g - mean(mother_masses_g))\n\nbird_form <- bf(nestlings ~ 1 + mother_mass_g_cen, family = poisson(link = \"log\"))\n\nget_prior(bird_form, data = imaginary_birds_centered)\n\n                  prior     class              coef group resp dpar nlpar lb ub\n                 (flat)         b                                              \n                 (flat)         b mother_mass_g_cen                            \n student_t(3, 1.4, 2.5) Intercept                                              \n       source\n      default\n (vectorized)\n      default\n\n\nWe set a prior for each parameter.\n\nbird_priors <- c(\n  prior(normal(1, .5), class = \"Intercept\"),\n  prior(normal(.1, .1), class = \"b\", coef = \"mother_mass_g_cen\")\n)\n\n\nPrior predictive checks\n\nprior_predictions <- brm(bird_form,\n                         data = imaginary_birds_centered,\n                         prior = bird_priors, \n                         sample_prior = \"only\", \n                         file = \"bird_model\",\n                         file_refit = \"on_change\",\n                         refresh = FALSE)\n\nPlot a few of these:\n\nlibrary(tidybayes)\nimaginary_birds_centered |> \n  add_predicted_draws(prior_predictions, ndraws = 6, seed = 4321) |> \n  ggplot(aes(x = mother_masses_g, y = .prediction)) + geom_point() + facet_wrap(~.draw)\n\n\n\n\nQuestion are we satisfied with these priors?\n\n\nFit to the data\n\nbird_posterior <- update(prior_predictions, sample_prior = \"yes\", \n                         file = \"bird_posterior\", \n                         file_refit = \"on_change\", refresh = FALSE)\n\nThe desired updates require recompiling the model\n\n\n\nsummary(bird_posterior)\n\n Family: poisson \n  Links: mu = log \nFormula: nestlings ~ 1 + mother_mass_g_cen \n   Data: imaginary_birds_centered (Number of observations: 15) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             1.39      0.13     1.14     1.64 1.00     1979     2070\nmother_mass_g_cen     0.16      0.04     0.07     0.25 1.00     2376     2463\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nknitr::kable(head(tidybayes::tidy_draws(bird_posterior)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.chain\n.iteration\n.draw\nb_Intercept\nb_mother_mass_g_cen\nprior_Intercept\nprior_b_mother_mass_g_cen\nlprior\nlp__\naccept_stat__\nstepsize__\ntreedepth__\nn_leapfrog__\ndivergent__\nenergy__\n\n\n\n\n1\n1\n1\n1.450523\n0.1357033\n0.7548442\n0.2027851\n0.6881775\n-29.18266\n0.9686242\n0.8998242\n2\n7\n0\n29.84270\n\n\n1\n2\n2\n1.415657\n0.1138594\n0.7622771\n0.0676872\n0.8027096\n-29.58861\n0.9127170\n0.8998242\n2\n3\n0\n29.93907\n\n\n1\n3\n3\n1.456546\n0.1777444\n0.7965764\n0.1959839\n0.4387761\n-29.24064\n0.9937692\n0.8998242\n2\n3\n0\n29.87044\n\n\n1\n4\n4\n1.440214\n0.1352149\n0.7316238\n0.1870379\n0.7082742\n-29.17389\n0.9942078\n0.8998242\n2\n7\n0\n29.34977\n\n\n1\n5\n5\n1.436886\n0.1975367\n1.2892340\n0.0588215\n0.3004459\n-29.50809\n0.9286261\n0.8998242\n2\n7\n0\n29.73433\n\n\n1\n6\n6\n1.450670\n0.0961187\n0.4208806\n0.1258187\n0.7508956\n-30.07357\n0.9696699\n0.8998242\n2\n3\n0\n30.30790\n\n\n\n\n\nHow do our priors and posteriors compare?\n\nlibrary(ggridges)\ntidybayes::tidy_draws(bird_posterior) |> \n  select(.draw, b_Intercept:prior_b_mother_mass_g_cen) |> \n  pivot_longer(-.draw) |> \n  ggplot(aes(x = value, y = name)) + geom_density_ridges()\n\n\n\n\nCan we draw the regression line?\n\naverage_mom <- mean(mother_masses_g)\n\nrange(imaginary_birds_centered$mother_mass_g_cen)\n\n[1] -6.025202  4.265214\n\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \n                                             n = 10)) |> \n  tidybayes::add_epred_draws(bird_posterior) |> \n  ungroup() |> \n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .epred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  geom_point(aes(x = mother_masses_g, y = nestlings),\n             data = imaginary_birds_centered, pch = 21,\n             fill = \"orange\", size = 3)\n\n\n\n\nLet’s also try drawing the prediction intervals.\n\naverage_mom <- mean(mother_masses_g)\n\nrange(imaginary_birds_centered$mother_mass_g_cen)\n\n[1] -6.025202  4.265214\n\ntibble(mother_mass_g_cen = modelr::seq_range(imaginary_birds_centered$mother_mass_g_cen, \n                                             n = 10)) |> \n  tidybayes::add_predicted_draws(bird_posterior) |> \n  ungroup() |> \n  ggplot(aes(x = average_mom + mother_mass_g_cen, y = .prediction)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  geom_point(aes(x = mother_masses_g, y = nestlings),\n             data = imaginary_birds_centered, pch = 21,\n             fill = \"orange\", size = 3)\n\n\n\n\nOther checks we can do:\n\nbird_posterior_onlyparam <- update(prior_predictions, sample_prior = \"no\", \n                         file = \"bird_posterior\", \n                         file_refit = \"on_change\", refresh = FALSE)\n\nshinystan::launch_shinystan(bird_posterior_onlyparam)"
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#multilevel-models",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#multilevel-models",
    "title": "Bayes for the Busy Ecologist",
    "section": "Multilevel models",
    "text": "Multilevel models\nBased on the awesome vignette for vignette for tidybayes\nWe begin by sampling some data from five different “conditions”:\n\nlibrary(modelr)\nset.seed(5)\nn <- 10\nn_condition <- 5\nABC <-\n  data_frame(\n    condition = rep(c(\"A\", \"B\", \"C\", \"D\", \"E\"), n),\n    response = rnorm(n * 5, c(0, 1, 2, 1, -1), 0.5)\n  )\n\nABC %>%\n  ggplot(aes(y = condition, x = response)) +\n  geom_point(pch = 21, size = 4, stroke = 1.4, fill = \"#41b6c4\")\n\n\n\n\nAnd by fitting a model to these data, with varying intercepts for each group:\n\nm <- brm(\n  response ~ (1 | condition), data = ABC, \n  control = list(adapt_delta = .99),\n  prior = c(\n    prior(normal(0, 1), class = Intercept),\n    prior(student_t(3, 0, 1), class = sd),\n    prior(student_t(3, 0, 1), class = sigma)\n  )\n)\n\nAn easy way to visualize these results is with a ridgeline plot as above\n\nABC %>%\n  modelr::data_grid(condition) %>%\n  tidybayes::add_predicted_draws(m) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") + \n  theme_minimal()\n\nPicking joint bandwidth of 0.102\n\n\n\n\n\nAlright. This used the simple vanilla option, add_predicted_samples(m). This uses the default options for making predictions, which recall is “NULL (default), include all group-level effects”. If you set add_predicted_samples(m, re_formula = NULL), you’ll get exactly the same figure.\nSo we can see that to “include” an effect is to take the actual estimated intercepts for each specific group we studied and use them to make new predictions for the same groups. This is Case 1 from McElreath’s list (though in this case, because we only have groups and nothing else, Case 1 and 2 are the same).\nWe can also say the exact same thing using a formula:\n\nABC %>%\n  data_grid(condition) %>%\n  add_predicted_draws(m, re_formula = ~(1|condition)) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") +  \n  theme_minimal()\n\nPicking joint bandwidth of 0.1\n\n\n\n\n\nThat’s right, there are three ways to say the exact same thing: say nothing, say NULL, or say the original “random effects” formula1. You go with what you feel in your heart is right, but I prefer the formula.\nIn all three cases, we are using the model to predict the means for the groups in our varying-intercepts model. This is what the documentation means by “including” these varying intercepts.\n\nSquishing those random effects\nOK, so that was three separate ways to make predictions for the same groups. What else can we do? Let’s try that thing with the NA argument, which means “include no group-level effects”:\n\nABC %>%\n  data_grid(condition) %>%\n  add_predicted_draws(m, re_formula = NA,\n                        n = 2000) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") +    theme_minimal()\n\nPicking joint bandwidth of 0.142\n\n\n\n\n\nAh, so if you do this, all the groups come out the same! But if they’re all the same, what do they represent? It seems reasonable that they represent the model’s intercept, as if the varying intercepts were all 0. Let’s calculate predicitons that ignore the varying effects – that is, using only the model intercept and the standard deviation of the response – using a bit of [handy purrr magic]2:\n\nm %>% \n  spread_draws(b_Intercept, sigma) %>% \n  mutate(prediction = rnorm(length(b_Intercept), b_Intercept, sigma),\n         #map2_dbl(b_Intercept, sigma, ~ rnorm(1, mean = .x, sd = .y)),\n         Prediction = \"prediction\") %>% #glimpse %>% \n  ggplot(aes(x = prediction, y = Prediction)) +\n  geom_density_ridges(fill = \"#41b6c4\") +    \n  theme_minimal()\n\nPicking joint bandwidth of 0.119\n\n\n\n\n\nAs you can see, this distribution has exactly the same shape as the five in the previous figure! It is as if we calculated the predictions for a group which was exactly at the average (in other words, it had a varying intercept of 0.) In the Rethinking book, readers are taught to do this in a much more explicit way: you actually generate all the 0 intercepts yourself, and give that to the model in place of the estimated intercepts! A very manual and concrete way to “set something to 0”.\nbrms does this too. As the documentation says >NA values within factors in newdata, are interpreted as if all dummy variables of this factor are zero.\nThe brms phrasing certainly takes less space, though it also requires you to remember that this is what NA gets you!\nWe can also remove random effects from our predictions by excluding them from the re_formula. In our model, we have only one varying effect – yet an even simpler formula is possible, a model with no intercept at all:\n\nABC %>%\n  data_grid(condition) %>%\n  add_predicted_draws(m, re_formula = ~ 0,\n                        n = 2000) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") + theme_minimal() \n\nPicking joint bandwidth of 0.14\n\n\n\n\n\nOnce again, the same distribution appears: it is as if all group effects had been set to zero. If we had two random effects and omitted one, this is what we would get for the omitted effect – the expected value if all its effects were 0.\n\n\nNew levels\nI’m going to show how to create predictions for new levels, but first I’m going to show two mistakes that I made frequently while learning:\nFirst, asking for new levels without specifying allow_new_levels = TRUE:\n\n# this does not work at all!!\ndata_frame(condition = \"bugaboo\") %>%\n  add_predicted_draws(m, re_formula = ~(1|condition),\n                        n = 2000)\n\nError: Levels 'bugaboo' of grouping factor 'condition' cannot be found in the fitted model. Consider setting argument 'allow_new_levels' to TRUE.\n\n\nThat fails because I tried to pass in a level of my grouping variable that wasn’t in the original model!\nSecond, passing in new levels – but telling the function to just ignore them:\n\ndata_frame(condition = \"bugaboo\") %>%\n  add_predicted_draws(m, re_formula = NA,#~(1|condition),\n                        n = 2000) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") + \n  theme_minimal()\n\nPicking joint bandwidth of 0.142\n\n\n\n\n\nHere, I’m still passing in the unknown level – but the function doesn’t complain, because I’m not including random effects at all! This is the same result from above, when we used NA or ~0 to remove varying effects altogether. This is definitely something to watch for if you are passing in new data (I made this mistake, and it cost me an afternoon!)\nIf we avoid both of these errors, we get what we expect: our means for our original groups, and a new predicted mean for \"bugaboo\":\n\nABC %>%\n  data_grid(condition) %>% \n  add_row(condition = \"bugaboo\") %>%\n  add_predicted_draws(m, re_formula = ~(1|condition),\n                        allow_new_levels = TRUE,\n                        n = 2000) %>%\n  ggplot(aes(x = .prediction, y = condition)) +\n  geom_density_ridges(fill = \"#41b6c4\") +    theme_minimal()\n\nPicking joint bandwidth of 0.131\n\n\n\n\n\nHere you can see that the new level is much flatter than the other original five. It comes from the same population as the others, which is rather variable (the group means are sort of different to each other). As a result, this new distribution is quite wide, including all that uncertainty.\nAn ecologist might do something like this if we were had data on some species in a community, but wanted to make predictions for new, as yet unobserved, species we might find next year."
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#outline-1",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#outline-1",
    "title": "Bayes for the Busy Ecologist",
    "section": "Outline",
    "text": "Outline\n\nPoisson count model\nBayesian Poisson count model"
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#todays-data-dandelions",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#todays-data-dandelions",
    "title": "Bayes for the Busy Ecologist",
    "section": "Today’s data: Dandelions",
    "text": "Today’s data: Dandelions\nLet’s imagine that we have counted dandelions.\nDandelions occur on average 6 per square meter\nHowever we have five kinds of quadrat: 1, 4, 9 and 25 square meters\n\nlibrary(tidyverse)\n\nimaginary_dandelions <- tibble(quadrat_size = rep(c(1,4, 9, 25), each = 15),\n       n_per_m2 = purrr::map(quadrat_size, rpois, lambda = 6),\n       obs_dandelions = map_dbl(n_per_m2, sum))\n\nggplot(imaginary_dandelions, aes(x = obs_dandelions)) + geom_histogram() + \n  facet_wrap(~quadrat_size)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHow can we get the correct number of dandelions?"
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#poisson-count-model",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#poisson-count-model",
    "title": "Bayes for the Busy Ecologist",
    "section": "Poisson count model",
    "text": "Poisson count model\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda) \\\\\n\\text{log}(\\lambda) &= \\beta\n\\end{align}\n\\] \\(\\lambda\\) is the average response. If we want to measure the average per unit effort, we can do that too:\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda) \\\\\n\\text{log}(\\lambda/Q) &= \\beta\n\\end{align}\n\\]\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda) \\\\\n\\text{log}(\\lambda) - \\text{log}(Q) &= \\beta\n\\end{align}\n\\]\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(\\lambda) \\\\\n\\text{log}(\\lambda) &= \\beta + \\text{log}(Q)\n\\end{align}\n\\]\nIn other words, we need a way to add a log coefficient to a model and give it a slope of exactly one. Fortunately the function offset() is here to do exactly this:\n\ndandelion_model <- glm(obs_dandelions ~ 1, family = poisson(link = \"log\"), data = imaginary_dandelions)\nsummary(dandelion_model) \n\n\nCall:\nglm(formula = obs_dandelions ~ 1, family = poisson(link = \"log\"), \n    data = imaginary_dandelions)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.802  -6.227  -2.658   2.539  11.652  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  4.03836    0.01714   235.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2909.7  on 59  degrees of freedom\nResidual deviance: 2909.7  on 59  degrees of freedom\nAIC: 3230.4\n\nNumber of Fisher Scoring iterations: 5\n\n\nThis gives the wrong answer!\n\ndandelion_model <- glm(obs_dandelions ~ 1 + offset(log(quadrat_size)),\n                       family = poisson(link = \"log\"),\n                       data = imaginary_dandelions)\nsummary(dandelion_model) \n\n\nCall:\nglm(formula = obs_dandelions ~ 1 + offset(log(quadrat_size)), \n    family = poisson(link = \"log\"), data = imaginary_dandelions)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.83462  -0.45999   0.07473   0.46032   2.07858  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.76109    0.01714   102.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 43.464  on 59  degrees of freedom\nResidual deviance: 43.464  on 59  degrees of freedom\nAIC: 364.13\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficient should be close to 6, after we reverse the link function:\n\nexp(coef(dandelion_model)[[1]])\n\n[1] 5.818803"
  },
  {
    "objectID": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#do-it-the-bayes-way",
    "href": "posts/2022-03-22-bayes-for-the-busy-ecologist/index.html#do-it-the-bayes-way",
    "title": "Bayes for the Busy Ecologist",
    "section": "Do it the Bayes Way",
    "text": "Do it the Bayes Way\n\nlibrary(brms)\n\ndandelion_bf <- bf(obs_dandelions ~ 1 + offset(log(quadrat_size)), \n                   family = poisson(link = \"log\"))\n\nget_prior(dandelion_bf, data = imaginary_dandelions)\n\nIntercept ~ student_t(3, 1.89940130916892, 2.5)\n\ndandelion_prior <- prior(normal(2, 1), class = \"Intercept\")\n\ndandelion_model <- brm(formula = dandelion_bf,\n                       data = imaginary_dandelions, \n                       prior = dandelion_prior)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '712e52da308896603a6860da7d30e1d5' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028059 seconds (Warm-up)\nChain 1:                0.026169 seconds (Sampling)\nChain 1:                0.054228 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '712e52da308896603a6860da7d30e1d5' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.02398 seconds (Warm-up)\nChain 2:                0.024673 seconds (Sampling)\nChain 2:                0.048653 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '712e52da308896603a6860da7d30e1d5' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.029274 seconds (Warm-up)\nChain 3:                0.02456 seconds (Sampling)\nChain 3:                0.053834 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '712e52da308896603a6860da7d30e1d5' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.023883 seconds (Warm-up)\nChain 4:                0.021166 seconds (Sampling)\nChain 4:                0.045049 seconds (Total)\nChain 4: \n\n\nLook at the Stan code:\n\nstancode(dandelion_model)\n\n// generated with brms 2.18.0\nfunctions {\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  int Y[N];  // response variable\n  vector[N] offsets;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += normal_lpdf(Intercept | 2, 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept + offsets;\n    target += poisson_log_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n\n\nLook at posterior distribution of parameter:\n\n# as.matrix(dandelion_model) |> head()\n\nlibrary(tidybayes)\n\ntidy_draws(dandelion_model) |> \n  ggplot(aes(x = exp(b_Intercept))) + \n  geom_histogram() + \n  geom_vline(xintercept = 6, col = \"red\", lwd = 3)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/2021-11-02-introduction-to-gams/index.html",
    "href": "posts/2021-11-02-introduction-to-gams/index.html",
    "title": "Introduction to Generalized Additive Models (GAMs)",
    "section": "",
    "text": "A short course on how to fit, plot, and evaluate GAMs\nThe course website URL available at pedersen-fisheries-lab.github.io/one-day-gam-workshop/\n\n\nThis is a 3-session, one-day workshop. It was developed with the goal of giving you enough GAM knowledge to feel comfortable fitting and working with GAMs in your day-to-day modelling practice, with just enough of more advanced applications to give a flavour of what GAMs can do. I will be covering a basic intro to GAM theory, with the rest focused on practical applications and a few advanced topics that I think might be interesting.\n\n\n\n\nUnderstand the basic GAM model, basis functions, and penalties\nFit 1D, 2D, and tensor-product GAMs to normal and non-normal data\nPlot GAM fits, and understand how to explain GAM outputs\nDiagnose common mispecification problems when fitting GAMs\nUse GAMs to make predictions about new data, and assess model uncertainty\nSee how more complicated GAM models can be used as part of a modern workflow\n\n\n\n\n\nYou will need to install R and I recommend using RStudio. The latest version of R can be downloaded here. RStudio is an application (an integrated development environment or IDE) that facilitates the use of R and offers a number of nice additional features. It can be downloaded here. You will need the free Desktop version for your computer.\nDownload the course materials as a ZIP file here. Alternatively, if you have the usethis, R package, running the following command will download the course materials and open them:\nusethis::use_course('pedersen-fisheries-lab/one-day-gam-workshop')\nInstall the R packages required for this course by running the following line of code your R console:\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"remotes\", \"mgcv\", \"tidyr\"))\nremotes::install_github(\"gavinsimpson/gratia\")"
  },
  {
    "objectID": "posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution/index.html",
    "href": "posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution/index.html",
    "title": "Mathematical Modeling in Ecology and Evolution",
    "section": "",
    "text": "In this workshop, I introduce various modelling techniques, using mostly ecological and evolutionary examples, with a focus on how computer software programs can help biologists analyze such models.\n\nContent\nPart 1: Classic one-variable models in ecology and evolution\nPart 2: Equilibria and their stability\nPart 3: Beyond equilibria\nPart 4: Example of building a model from scratch\nPart 5: Extending to models with more than one variable\nPart 6: Another example of building a model from scratch\n\n\nSoftware\nIn my research, I primarily use Mathematica, which is a powerful software package to organize and conduct analytical modelling, but it is not free (at UBC, we have some licenses available). I will also show some example code and provide translation of most of what I present in a free software package called Maxima.\n\nMathematica installation\nThere is a free trial version that you can use for 15 days, if you don’t have a copy (click here to access), or you can buy a student version online. If you want to make sure that all is working, copy the code below, put your cursor over each of the following lines and press enter (on some computers, “enter” is a separate button, on others, press “shift” and “return” at the same time):\nD[x^3,x]\nListPlot[Table[x, {x,1,10}],Joined->True]\nRSolve[{x[t+1]\\[Equal]A x[t],x[0]\\[Equal]x0},x[t],t]\nPDF[NormalDistribution[0,1],x]\nYou should see (a) \\(3x^2\\), (b) a plot of a line, (c) \\({{x[t]->A^t x0}}\\), and (d) \\(\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi }}\\).\n\n\nMaxima installation:\nOn a Mac, install using the instructions here. For other file systems, download here.\n\n\nMaxima testing\nWhen you first open Maxima, it will give you a choice of GUIs, chose wxMaxima. Once wxMaxima is launched type this command and hit return to see if it answers 4:\n2+2;\nIf it doesn’t, then scan the installation document for the error that you run into.\nIf it does return 4, then type in and enter these commands:\ndiff(x^3, x);\n\nwxplot2d (3*x, [x, 0, 2*%pi]);\n\nload(\"solve_rec\")$\nsolve_rec(x[t+1] = A*x[t], x[t], x[0]=x0);\n\nload(\"distrib\")$\npdf_normal(x,0,1);\nYou should see (a) \\(3x^2\\), (b) a plot of a line, (c) \\({{x[t]->A^t x0}}\\), and (d) \\(\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi }}\\).\n\n\n\nMaterial\n\n\n\nMathematica\nMaxima\nPDF\n\n\n\n\nNotebook\nNotebook\nEmbeded below\n\n\nHints and solutions\nHints and solutions\n\n\n\n\n\nHomework\n\n\nHomework answers\n\nHomework answers\n\n\nGuide\nGuide\n\n\n\n\n\nFollow along PDF\nThis PDF was generated from the Mathematica notebook linked above. It doesn’t include dynamic plots, but it’s a good alternative if you want to print out or have a quick reference at hand.\n\n\n \n\n\n\nStability analysis of a recursion equation in a discrete-time model.\n\n\n\n\n\nOther resources\n\nAn Introduction to Mathematical Modeling in Ecology and Evolution (Otto and Day 2007).\nBiomathematical modeling lecture notes.\nMathematica labs UBC.\n\n\n\nThanks\nNiki Love and Gil Henriques did a great job of translating the code into wxMaxima, with limited help from me. Thanks, Niki and Gil!!\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nReferences\n\nOtto, Sarah P, and Troy Day. 2007. A Biologist’s Guide to Mathematical Modeling in Ecology and Evolution. Vol. 13. Princeton University Press.\n\nCitationBibTeX citation:@online{p.otto2020,\n  author = {Sarah P. Otto},\n  title = {Mathematical {Modeling} in {Ecology} and {Evolution}},\n  date = {2020-01-14},\n  url = {https://bios2.github.io/posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSarah P. Otto. 2020. “Mathematical Modeling in Ecology and\nEvolution.” BIOS2 Education Resources. January 14,\n2020. https://bios2.github.io/posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution."
  },
  {
    "objectID": "posts/2020-06-15-science-communication/index.html",
    "href": "posts/2020-06-15-science-communication/index.html",
    "title": "Science Communication",
    "section": "",
    "text": "The objective of this training is to share and discuss the concepts and tools that contribute to effective science communication. The training is split into two sessions, which cover the basic concepts of effective science communication and how social media tools can be used to boost the signal of your research and extend your research network. Each training takes the form of a presentation interspersed with several short activity modules, where participants are invited to use the tools we will be discussing to kickstart their own science communication.\nThis training was given on June 1 and 2, 2020. You can view recordings of each session here:"
  },
  {
    "objectID": "posts/2020-06-15-science-communication/index.html#session-1-the-basics-of-science-communication",
    "href": "posts/2020-06-15-science-communication/index.html#session-1-the-basics-of-science-communication",
    "title": "Science Communication",
    "section": "Session 1: The basics of science communication",
    "text": "Session 1: The basics of science communication\n\nObjectives:\n\nDiscuss what science communication (or SciComm) can be, and its potential role in boosting the signal of your research\nMake an overview of basic concepts and tools that you can use in any medium (blog posts, presentations, conversations, twitter, etc.) to do effective science communication\n\nDuring this session, we:\n\nDiscuss the potential pitfalls of science communication (notably, diversity and inclusivity problems).\nCover the basic concepts of science communication, including the Golden Circle method, the creation of personas, and storytelling techniques.\nHave short activities where participants can try to use some of the techniques we will be covering, such as filling in their own Golden Circle and explaining a blog post as a storyboard."
  },
  {
    "objectID": "posts/2020-06-15-science-communication/index.html#session-2-social-media-as-a-science-communication-tool",
    "href": "posts/2020-06-15-science-communication/index.html#session-2-social-media-as-a-science-communication-tool",
    "title": "Science Communication",
    "section": "Session 2: Social media as a science communication tool",
    "text": "Session 2: Social media as a science communication tool\n\nObjectives:\n\nRethink the way we write about science by exploring the world of blog posts\nClarify the mechanics of Twitter and how it can be used effectively for science communication\n\nDuring this session, we:\n\nDiscuss how to create a story structure using titles and the flow of ideas in blog posts, especially when we are used to writing scientific articles\nCover the basics of how Twitter works (retweets, threads, replies, hashtags, photo captions, etc.) and how to find helpful connections\nHave short activities where participants will be invited to write their own Twitter biographies and to create a Twitter thread explaining a project of their choice."
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html",
    "title": "Introduction to Shiny Apps",
    "section": "",
    "text": "There are many reasons to consider using Shiny for a project:\n\nSharing results from a paper with your readers;\nHelping you explore a model, mathematics, simulations;\nLetting non R users use R."
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#hello-shiny",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#hello-shiny",
    "title": "Introduction to Shiny Apps",
    "section": "Hello Shiny!",
    "text": "Hello Shiny!\nHere is an example of a Shiny app that RStudio generates when you open a new Shiny Web App file:\n\n# Define UI for app that draws a histogram ----\nui <- fluidPage(\n\n  # App title ----\n  titlePanel(\"Hello Shiny!\"),\n\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n\n      # Input: Slider for the number of bins ----\n      sliderInput(inputId = \"bins\",\n                  label = \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n\n    ),\n\n    # Main panel for displaying outputs ----\n    mainPanel(\n\n      # Output: Histogram ----\n      plotOutput(outputId = \"distPlot\")\n\n    )\n  )\n)"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#building-blocks",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#building-blocks",
    "title": "Introduction to Shiny Apps",
    "section": "Building blocks",
    "text": "Building blocks\nWe’ve now seen the basic building blocks of a Shiny app:\n\nThe user interface, which determines how the app “looks”. This is how we tell Shiny where to ask for user inputs, and where to put any outputs we create.\nReactive values, which are values that change according to user inputs. These are values that affect the outputs we create in the Shiny app, such as tables or plots.\nThe server, where we use reactive values to generate some outputs.\n\n\nIDs\nThe user interface and server communicate through IDs that we assign to inputs from the user and outputs from the server.\n\nWe use an ID (in orange) to link the user input in the UI to the reactive values used in the server:\n\nWe use another ID (in blue) to link the output created in the server to the output shown in the user interface:\n\n\n\nOrganisation\nThese elements can all be placed in one script named app.R or separately in scripts named ui.R and server.R. The choice is up to you, although it becomes easier to work in separate ui.R and server.R scripts when the Shiny app becomes more complex.\nExample 1: Everything in app.R\n Example 2: Split things into ui.R and server.R"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#plots",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#plots",
    "title": "Introduction to Shiny Apps",
    "section": "Plots",
    "text": "Plots\nShiny is an excellent tool for visual exploration - it is at its most useful when a user can see something change before their eyes according to some selections. This is a great way to allow users to explore a dataset, explore the results of some analyses according to different parameters, and so on!\nLet’s now add a plot to our Shiny app, to visualize the distribution of a variable depending on user input. We’ll be adding the ggplot2 and ggridges packages in the set-up step at the top of our app.R to allow us to make a plot.\n\n# load packages\nlibrary(shiny)\nlibrary(ggridges)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(readr)\n\n\nUser interface\nTo add a plot in our Shiny, we need to indicate where the plot should appear in the app. We can do this with plotOutput(), a similar function to tableOutput() in the previous section that is meant for plot outputs, as the name suggests.\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  )\n)\n\nNow our Shiny app knows where we want to place our plot.\n\n\nServer\nWe now need to create the plot we want to show in our app. This plot will change depending on one or several reactive values that the user can input or select in our UI.\nWe link the UI and server together with IDs that are assigned to each object. Above, we told the UI to expect a plot output with the ID \"ridgePlot\". In the server, we will create a plot and render it as a plot object using renderPlot(), and we will assign this plot output to the ID we call in the UI (as output$ridgePlot).\n\n# Define server logic required to make your output(s)\nserver <- function(input, output) {\n\n  \n  # prepare the data\n  # ----------------------------------------------------------\n  \n  # read the dataset\n  eruptions <- readr::read_rds(here::here(\"data\", \"eruptions.rds\"))\n  \n  # filter the dataset to avoid overloading the plot \n  eruptions <- eruptions[which(eruptions$volcano_name %in% names(which(table(eruptions$volcano_name) > 30))),]\n  # this subsets to volcanoes that have erupted more than 30 times\n  \n  \n  # make reactive dataset\n  # ----------------------------------------------------------\n  \n  # subset volcano data with input year range\n  eruptions_filtered <- reactive({\n    subset(eruptions, start_year >= input$years[1] & end_year <= input$years[2])\n  })\n  \n    \n  # create and render the outputs\n  # ----------------------------------------------------------\n  \n  # create the table of volcanoes\n  output$erupt_table <- renderTable({\n    head(eruptions_filtered())\n  })\n  \n  # render the plot output\n  output$ridgePlot <- renderPlot({\n    \n    # create the plot\n    ggplot(data = eruptions_filtered(),\n           aes(x = vei,\n               y = volcano_name,\n               fill = volcano_name)) +\n      # we are using a ridgeplot geom here, from the ggridges package\n      geom_density_ridges( size = .5) + # line width\n      \n      # label the axes\n      labs(x = \"Volcano Explosivity Index\", y = \"\") +\n      \n      # adjust the ggplot theme to make the plot \"prettier\"\n      theme_classic() + \n      theme(legend.position = \"none\",\n            axis.text = element_text(size = 12, face = \"bold\"),\n            axis.title = element_text(size = 14, face = \"bold\"))\n  })\n}\n\n\n\nThe Shiny app\nNow, if we run the Shiny app, we have a plot above the table we made previously. They are positioned in this way because the plotOutput() comes before the tableOutput() in the UI.\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#customising-the-theme",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#customising-the-theme",
    "title": "Introduction to Shiny Apps",
    "section": "Customising the theme",
    "text": "Customising the theme\nIf you’d like to go one step further, you can also customize the appearance of your Shiny app using built-in themes, or creating your own themes.\n\nUsing built-in themes\nThere are several built-in themes in Shiny, which allow you to quickly change the appearance of your app. You can browse a gallery of available themes here here, or test themes out interactively here.\nLet’s try the darkly theme on our Shiny app. To do this, we will need the shinythemes package.\n\nlibrary(shinythemes)\n\nWe can change the theme of our previous app with one line of code:\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  ),\n  \n  # Customize the theme ----------------------\n  \n  # Use the darkly theme\n  theme = shinythemes::shinytheme(\"darkly\")\n)\n\nNow, if we run the app, it looks a little different:\n\n\n\nUsing a custom theme\nYou can also go beyond the built-in themes, and create your own custom theme with the fonts and colours of your choice. You can also apply this theme to the outputs rendered in the app, to bring all the visuals together for a more cohesive look.\n\nCustomizing a theme\nTo create a custom theme, we will be using the bs_theme() function from the bslib package.\n\nlibrary(bslib)\n\n\n# Create a custom theme \ncute_theme <- bslib::bs_theme(\n  \n  bg = \"#36393B\", # background colour\n  fg = \"#FFD166\", # most of the text on your app\n  primary = \"#F26430\", # buttons, ...\n  \n  # you can also choose fonts\n  base_font = font_google(\"Open Sans\"),\n  heading_font = font_google(\"Open Sans\")\n)\n\nTo apply this theme to our Shiny app (and the outputs), we will be using the thematic package.\n\nlibrary(thematic)\n\nThere are two essential steps to apply a custom theme to a Shiny app:\n\nActivating thematic.\nSetting the user interface’s theme to the custom theme (cute_theme).\n\n\n# Activate thematic\n# so your R outputs will be changed to match up with your chosen styling\nthematic::thematic_shiny()\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  ),\n  \n  # Customize the theme ----------------------\n  \n  # Use our custom theme\n  theme = cute_theme\n)\n\nNow, if we run the app, the user interface and plot theme is set to the colours and fonts we set in cute_theme:\n\nHere, thematic is not changing the colours used to represent a variable in our plot, because this is an informative colour scale (unlike the colour of axis labels, lines, and the plot background). However, if we remove this colour variable in our ridgeplot in the server, thematic will change the plot colours as well. Here is a simplified example of our server to see what these changes would look like:\n\n# Define server logic required to make your output(s)\nserver <- function(input, output) {\n  \n  #... (all the good stuff we wrote above)\n  \n  # render the plot output\n  output$ridgePlot <- renderPlot({\n    \n    # create the plot\n    ggplot(data = eruptions_filtered(),\n           aes(x = vei,\n               y = volcano_name)) + # we are no longer setting \n             # the fill argument to a variable\n             \n             # we are using a ridgeplot geom here, from the ggridges package\n             geom_density_ridges(size = .5) + \n             \n             # label the axes\n             labs(x = \"Volcano Explosivity Index\", y = \"\") +\n             \n             # remove the \"classic\" ggplot2 so it doesn't override thematic's changes\n             # theme_classic() + \n             theme(legend.position = \"none\",\n                   axis.text = element_text(size = 12, face = \"bold\"),\n                   axis.title = element_text(size = 14, face = \"bold\"))\n           })\n    }\n\nNow, our plot’s theme follows the app’s custom theme as well:"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#taking-advantage-of-good-defaults",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#taking-advantage-of-good-defaults",
    "title": "Introduction to Shiny Apps",
    "section": "Taking advantage of good defaults",
    "text": "Taking advantage of good defaults\nHere, we will use shiny extension shinyDashboards and leaflet to construct a custom Shiny App to map volcanoes of the world. First, we need a few additional packages.\nNote: All Source code for this app can be found here on the BIOS2 Github.\n\n# load packages\nlibrary(shiny)\nlibrary(shinydashboard)  # dashboard layout package\nlibrary(shinyWidgets)  # fancy widgets package\nlibrary(leaflet)  # interactive maps package\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nUsing ShinyDashboard\nWe will create our app using defaults from the ShinyDashboard package, which always includes three main components: a header, using dashboardHeader(), a sidebar, using dashboardSidebar(), and a body, using dashboardBody(). These are then added together using the dashboardPage() function.\nBuilding these elements is less like usual R coding, and more like web design, since we are, in fact, designing a unser interface for a web app. Here, we’ll make a basic layout before populating it.\n\n# create the header of our app\nheader <- dashboardHeader(\n    title = \"Exploring Volcanoes of the World\",\n    titleWidth = 350 # since we have a long title, we need to extend width element in pixels\n)\n\n\n# create dashboard body - this is the major UI element\nbody <- dashboardBody(\n\n    # make first row of elements (actually, this will be the only row)\n    fluidRow(\n        \n        # make first column, 25% of page - width = 3 of 12 columns\n        column(width = 3,\n               \n               \n               # Box 1: text explaining what this app is\n               #-----------------------------------------------\n               box( width = NULL,\n                    status=\"primary\", # this line can change the automatic color of the box.\n                    title = NULL,\n                    p(\"here, we'll include some info about this app\")\n\n                 \n               ), # end box 1\n               \n               \n               # box 2 : input for selecting volcano type\n               #-----------------------------------------------\n               box(width = NULL, status = \"primary\",\n                   title  = \"Selection Criteria\", solidHeader = T, \n                   \n                   p(\"here, we'll add a UI element for selecting volcano types\"),\n\n               ), # end box 2\n               \n               \n               \n               # box 3: ggplot of selected volcanoes by continent\n               #------------------------------------------------\n               box(width = NULL, status = \"primary\",\n                   solidHeader = TRUE, collapsible = T,\n                   title = \"Volcanoes by Continent\",\n                   p(\"here, we'll add a bar plot of volcanoes in each continent\")\n               ) # end box 3\n               \n        ), # end column 1\n         \n        # second column - 75% of page (9 of 12 columns)\n        #--------------------------------------------------\n        column(width = 9,\n               # Box 4: leaflet map\n               box(width = NULL, background = \"light-blue\", height = 850,\n                   p(\"here, we'll show volcanoes on a map\"),\n               ) # end box with map\n        ) # end second column\n        \n    ) # end fluidrow\n) # end body\n\n\n# add elements together\ndashboardPage(\n    skin = \"blue\",\n    header = header,\n    sidebar = dashboardSidebar(disable = TRUE), # here, we only have one tab of our app, so we don't need a sidebar\n    body = body\n)"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#populating-the-layout",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#populating-the-layout",
    "title": "Introduction to Shiny Apps",
    "section": "Populating the Layout",
    "text": "Populating the Layout\nNow, we are going to fill out app with elements. In this app, we will only have one user input: a selection of the volcano type to show. We will use this input (input$volcano_type), which will be used to filter data in the server (i.e. make a smaller dataset using only volcanoes of the selected types), then use this filtered dataset to create output elements (plots and maps).\nBelow, we show the necessary code to include in both the UI and the Server to create each plot element. Notice that after the reactive value selected_volcanoes is created in the selection box, this is the only object that is used to create the other elements in the app.\n\n\n\n\n\n\n\n\n\nLocation\nElement\nUI\nServer\n\n\n\n\nBox 1\nIntro Textbox\nMarkdown/HTML text code\n\n\n\nBox 2\nSelection Wigets\ncheckboxGroupButtons( inputID = \"volcano_type\")\nselected_volcanoes <- reactive({ volcano_df %>% filter(type %in% input$volcano_type)}) to create a filtered dataset that will react to user input\n\n\nBox 3\nBar Graph\nplotOutput(\"continentplot\")\noutput$continentplot <- renderPlot(...)) which will plot from the selectied_volcanoes reactive object\n\n\nBox 4\nLeaflet Map\nleafletOutput(\"volcanomap\")\noutput$volcanomap <- renderLeaflet(...) to map points from the selectied_volcanoes reactive object"
  },
  {
    "objectID": "posts/2021-06-22-introduction-to-shiny-apps/index.html#challenge",
    "href": "posts/2021-06-22-introduction-to-shiny-apps/index.html#challenge",
    "title": "Introduction to Shiny Apps",
    "section": "Challenge!",
    "text": "Challenge!\nUse the code provided to add your own additional user input to the Shiny App. The code (which you can access here leaves a space for an additional UI input inside box 2). Then, you’ll need to use your new input element to the reactive value in the Server, as noted in the server code.\nUse the Default Shiny Widgets or shinyWidgets extended package galleries to explore the types of elements you can add.\n\n\nSee the completed app\nSee our completed app HERE"
  },
  {
    "objectID": "posts/2021-05-04-building-r-packages/index.html",
    "href": "posts/2021-05-04-building-r-packages/index.html",
    "title": "Building R packages",
    "section": "",
    "text": "via GIPHY\nR packages! they are kind of like cookies:\nBut most of all: cookies are delicious for what they contain: chocolate chunks, candy, oats, cocoa. However, all cookies share some fundamental ingredients and nearly identical structure. Flour, saturated with fat and sugar hydrated only with an egg, flavoured with vanilla and salt. The basic formula is invariant and admits only slight deviation – otherwise, it becomes something other than a cookie.\nThis workshop is devoted to the study of cookie dough."
  },
  {
    "objectID": "posts/2021-05-04-building-r-packages/index.html#the-structure-flour-and-sugar",
    "href": "posts/2021-05-04-building-r-packages/index.html#the-structure-flour-and-sugar",
    "title": "Building R packages",
    "section": "The structure: flour and sugar",
    "text": "The structure: flour and sugar\n\nNo cookies without carbs\n\nAn R package is essentially a folder on your computer with specific structure. We will begin by creating an empty R package and taking a tour!\nOpen your R code editor, and find out where you are:\ngetwd()\nThis is to prepare for the next step, where we will choose a location for our R package folder. Please be intentional about where you place your R package! Do not place it in the same space as another package, Rstudio project or other project. Create a new and isolated location for it.\nI am working from an existing R project in my typical R Projects folder, so I go up one level:\nusethis::create_package(\"../netwerk\")\n\nwe are sticking with usethis because we want to keep this general. All of these steps can be manual, and indeed for many years they were!\n\n\nLet’s run R CMD CHECK right away. We will do this MANY TIMES.\ndevtools::check()\nWe should see some warnings! let’s keep these in mind as we continue our tour.\n\nThe DESCRIPTION file\nThe most important file to notice is the DESCRIPTION. This gives general information about the entire package. It is written in a specific file format\nPackage: netwerk\nTitle: Werks with Networks\nVersion: 0.0.0.9000\nAuthors@R: \n    person(given = \"Andrew\",\n           family = \"MacDonald\",\n           role = c(\"aut\", \"cre\"),\n           email = \"<you@email.com>\")\nDescription: it does networks.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nLazyData: true\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.1\nSuggests: \n    testthat (>= 3.0.0)\nConfig/testthat/edition: 3\nHere are some things to edit manually in DESCRIPTION:\n\npackage name [tk naming of R packages] – make it short and convenient if you can!\nTitle: write this part In Title Case. Don’t end the title with a period.\nDescription: Describe the package in a short block of text. This should end with a period.\nAuthors: Add your name here and the name of anyone building the package with you. usethis will have done the first step for you, and filled in the structure. Only “aut” (author) and “cre” (creator) are essential. but many others are possible\n\nAdd your name here.\nAdd a license\nusethis::use_mit_license(copyright_holder = \"\")\nnote about the different roles that R package authors can have. Funny ones. but creator and maintainer are the key ones.\nNote the R folder. We’ll get much more into that later\n\nRbuildignore"
  },
  {
    "objectID": "posts/2021-05-04-building-r-packages/index.html#keeping-notes",
    "href": "posts/2021-05-04-building-r-packages/index.html#keeping-notes",
    "title": "Building R packages",
    "section": "Keeping notes",
    "text": "Keeping notes\ncreate an R file\nusethis::use_build_ignore(\"dev.R\")\nthe docs folder\nhere we have a very minimal version of an R packages we’re going to be adding to it as the course progresses.\nOne thing we can do right away is build and check the R package\nWhat exactly is happining here? slide from R package tutorial.\nLots of checkpoints and progress confrimations along the way.\nOK so what is that all about? we have compiled the R package and it has gone to where the R packages on our computer go.\nThere is a natural cycle to how the different steps in an R package workflow proceed – see the documentation for this lesson – we will be following this process (TK another pictures?\nOk so now that we ahve the basic structure, let’s talk about some content for the R package. I received the donation of a little R function already that we can use to create this workflow in a nice way\nThis R function (explain what the function does)\nOK so let’s focus on just one part of this function.\nload all – shortcut\n\nhow do we do this in VScode?\n\n\nhow to add something to the .Rbuildignore? it would be nice to have a little .dev script as a space to create all the ohter dependencies that are involved in making an R package."
  },
  {
    "objectID": "posts/2021-05-04-building-r-packages/index.html#useful-links",
    "href": "posts/2021-05-04-building-r-packages/index.html#useful-links",
    "title": "Building R packages",
    "section": "Useful links",
    "text": "Useful links\nThis workshop borrows heavily from some excellent sources:\n\nthe R packages book especially the “Whole Game” chapter!\nrOpenSci Packages: Development, Maintenance, and Peer Review\n\nhttps://builder.r-hub.io/about.html"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html",
    "title": "Spatial Statistics in Ecology",
    "section": "",
    "text": "Version en français à la suite."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#course-outline",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#course-outline",
    "title": "Spatial Statistics in Ecology",
    "section": "Course outline",
    "text": "Course outline\n\n\n\nDay\nTopics (EN)\n\n\n\n\n1\n• Introduction to spatial statistics  • Point pattern analysis\n\n\n2\n• Spatial correlation  • Geostatistical models\n\n\n3\n• Areal data  • Moran’s I  • Spatial autoregression models  • Analysis of areal data in R\n\n\n4\n• GLMM with spatial Gaussian process  • GLMM with spatial autoregression"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#types-of-spatial-analyses",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#types-of-spatial-analyses",
    "title": "Spatial Statistics in Ecology",
    "section": "Types of spatial analyses",
    "text": "Types of spatial analyses\nIn this training, we will discuss three types of spatial analyses: point pattern analysis, geostatistical models and models for areal data.\nIn point pattern analysis, we have point data representing the position of individuals or events in a study area and we assume that all individuals or events have been identified in that area. That analysis focuses on the distribution of the positions of the points themselves. Here are some typical questions for the analysis of point patterns:\n\nAre the points randomly arranged or clustered?\nAre two types of points arranged independently?\n\nGeostatistical models represent the spatial distribution of continuous variables that are measured at certain sampling points. They assume that measurements of those variables at different points are correlated as a function of the distance between the points. Applications of geostatistical models include the smoothing of spatial data (e.g., producing a map of a variable over an entire region based on point measurements) and the prediction of those variables for non-sampled points.\nAreal data are measurements taken not at points, but for regions of space represented by polygons (e.g. administrative divisions, grid cells). Models representing these types of data define a network linking each region to its neighbours and include correlations in the variable of interest between neighbouring regions."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#stationarity-and-isotropy",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#stationarity-and-isotropy",
    "title": "Spatial Statistics in Ecology",
    "section": "Stationarity and isotropy",
    "text": "Stationarity and isotropy\nSeveral spatial analyses assume that the variables are stationary in space. As with stationarity in the time domain, this property means that summary statistics (mean, variance and correlations between measures of a variable) do not vary with translation in space. For example, the spatial correlation between two points may depend on the distance between them, but not on their absolute position.\nIn particular, there cannot be a large-scale trend (often called gradient in a spatial context), or this trend must be taken into account before modelling the spatial correlation of residuals.\nIn the case of point pattern analysis, stationarity (also called homogeneity) means that point density does not follow a large-scale trend.\nIn a isotropic statistical model, the spatial correlations between measurements at two points depend only on the distance between the points, not on the direction. In this case, the summary statistics do not change under a spatial rotation of the data."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#georeferenced-data",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#georeferenced-data",
    "title": "Spatial Statistics in Ecology",
    "section": "Georeferenced data",
    "text": "Georeferenced data\nEnvironmental studies increasingly use data from geospatial data sources, i.e. variables measured over a large part of the globe (e.g. climate, remote sensing). The processing of these data requires concepts related to Geographic Information Systems (GIS), which are not covered in this workshop, where we focus on the statistical aspects of spatially varying data.\nThe use of geospatial data does not necessarily mean that spatial statistics are required. For example, we will often extract values of geographic variables at study points to explain a biological response observed in the field. In this case, the use of spatial statistics is only necessary when there is a spatial correlation in the residuals, after controlling for the effect of the predictors."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#point-pattern-and-point-process",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#point-pattern-and-point-process",
    "title": "Spatial Statistics in Ecology",
    "section": "Point pattern and point process",
    "text": "Point pattern and point process\nA point pattern describes the spatial position (most often in 2D) of individuals or events, represented by points, in a given study area, often called the observation “window”.\nIt is assumed that each point has a negligible spatial extent relative to the distances between the points. More complex methods exist to deal with spatial patterns of objects that have a non-negligible width, but this topic is beyond the scope of this workshop.\nA point process is a statistical model that can be used to simulate point patterns or explain an observed point pattern."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#complete-spatial-randomness",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#complete-spatial-randomness",
    "title": "Spatial Statistics in Ecology",
    "section": "Complete spatial randomness",
    "text": "Complete spatial randomness\nComplete spatial randomness (CSR) is one of the simplest point patterns, which serves as a null model for evaluating the characteristics of real point patterns. In this pattern, the presence of a point at a given position is independent of the presence of points in a neighbourhood.\nThe process creating this pattern is a homogeneous Poisson process. According to this model, the number of points in any area \\(A\\) follows a Poisson distribution: \\(N(A) \\sim \\text{Pois}(\\lambda A)\\), where \\(\\lambda\\) is the intensity of the process (i.e. the density of points per unit area). \\(N\\) is independent between two disjoint regions, no matter how those regions are defined.\nIn the graph below, only the pattern on the right is completely random. The pattern on the left shows point aggregation (higher probability of observing a point close to another point), while the pattern in the center shows repulsion (low probability of observing a point very close to another)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exploratory-or-inferential-analysis-for-a-point-pattern",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exploratory-or-inferential-analysis-for-a-point-pattern",
    "title": "Spatial Statistics in Ecology",
    "section": "Exploratory or inferential analysis for a point pattern",
    "text": "Exploratory or inferential analysis for a point pattern\nSeveral summary statistics are used to describe the characteristics of a point pattern. The simplest is the intensity \\(\\lambda\\), which as mentioned above represents the density of points per unit area. If the point pattern is heterogeneous, the intensity is not constant, but depends on the position: \\(\\lambda(x, y)\\).\nCompared to intensity, which is a first-order statistic, second-order statistics describe how the probability of the presence of a point in a region depends on the presence of other points. The Ripley’s \\(K\\) function presented in the next section is an example of a second-order summary statistic.\nStatistical inferences on point patterns usually consist of testing the hypothesis that the point pattern corresponds to a given null model, such as CSR or a more complex null model. Even for the simplest null models, we rarely know the theoretical distribution for a summary statistic of the point pattern under the null model. Hypothesis tests on point patterns are therefore performed by simulation: a large number of point patterns are simulated from the null model and the distribution of the summary statistics of interest for these simulations is compared to their values for the observed point pattern."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#ripleys-k-function",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#ripleys-k-function",
    "title": "Spatial Statistics in Ecology",
    "section": "Ripley’s K function",
    "text": "Ripley’s K function\nRipley’s K function \\(K(r)\\) is defined as the mean number of points within a circle of radius \\(r\\) around a point in the pattern, standardized by the intensity \\(\\lambda\\).\nUnder the CSR null model, the mean number of points in any circle of radius \\(r\\) is \\(\\lambda \\pi r^2\\), thus in theory \\(K(r) = \\pi r^2\\) for that model. A higher value of \\(K(r)\\) means that there is an aggregation of points at the scale \\(r\\), whereas a lower value means that there is repulsion.\nIn practice, \\(K(r)\\) is estimated for a specific point pattern by the equation:\n\\[ K(r) = \\frac{A}{n(n-1)} \\sum_i \\sum_{j > i} I \\left( d_{ij} \\le r \\right) w_{ij}\\]\nwhere \\(A\\) is the area of the observation window and \\(n\\) is the number of points in the pattern, so \\(n(n-1)\\) is the number of distinct pairs of points. We take the sum for all pairs of points of the indicator function \\(I\\), which takes a value of 1 if the distance between points \\(i\\) and \\(j\\) is less than or equal to \\(r\\). Finally, the term \\(w_{ij}\\) is used to give extra weight to certain pairs of points to account for edge effects, as discussed in the next section.\nFor example, the graphs below show the estimated \\(K(r)\\) function for the patterns shown above, for values of \\(r\\) up to 1/4 of the window width. The red dashed curve shows the theoretical value for CSR and the gray area is an “envelope” produced by 99 simulations of that null pattern. The aggregated pattern shows an excess of neighbours up to \\(r = 0.25\\) and the pattern with repulsion shows a significant deficit of neighbours for small values of \\(r\\).\n\n\n\n\n\nIn addition to \\(K\\), there are other statistics to describe the second-order properties of point patterns, such as the mean distance between a point and its nearest \\(N\\) neighbours. You can refer to the Wiegand and Moloney (2013) textbook in the references to learn more about different summary statistics for point patterns."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#edge-effects",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#edge-effects",
    "title": "Spatial Statistics in Ecology",
    "section": "Edge effects",
    "text": "Edge effects\nIn the context of point pattern analysis, edge effects are due to the fact that we have incomplete knowledge of the neighbourhood of points near the edge of the observation window, which can induce a bias in the calculation of statistics such as Ripley’s \\(K\\).\nDifferent methods have been developed to correct the bias due to edge effects. In Ripley’s edge correction method, the contribution of a neighbour \\(j\\) located at a distance \\(r\\) from a point \\(i\\) receives a weight \\(w_{ij} = 1/\\phi_i(r)\\), where \\(\\phi_i(r)\\) is the fraction of the circle of radius \\(r\\) around \\(i\\) contained in the observation window. For example, if 2/3 of the circle is in the window, this neighbour counts as 3/2 neighbours in the calculation of a statistic like \\(K\\).\n\nRipley’s method is one of the simplest to correct for edge effects, but is not necessarily the most efficient; in particular, larger weights given to certain pairs of points tend to increase the variance of the calculated statistic. Other correction methods are presented in specialized textbooks, such as Wiegand and Moloney (2013)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#example",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#example",
    "title": "Spatial Statistics in Ecology",
    "section": "Example",
    "text": "Example\nFor this example, we use the dataset semis_xy.csv, which represents the \\((x, y)\\) coordinates for seedlings of two species (sp, B = birch and P = poplar) in a 15 x 15 m plot.\n\nsemis <- read.csv(\"data/semis_xy.csv\")\nhead(semis)\n\n      x    y sp\n1 14.73 0.05  P\n2 14.72 1.71  P\n3 14.31 2.06  P\n4 14.16 2.64  P\n5 14.12 4.15  B\n6  9.88 4.08  B\n\n\nThe spatstat package provides tools for point pattern analysis in R. The first step consists in transforming our data frame into a ppp object (point pattern) with the function of the same name. In this function, we specify which columns contain the coordinates x and y as well as the marks, which here will be the species codes. We also need to specify an observation window (window) using the owin function, where we provide the plot limits in x and y.\n\nlibrary(spatstat)\n\nsemis <- ppp(x = semis$x, y = semis$y, marks = as.factor(semis$sp),\n             window = owin(xrange = c(0, 15), yrange = c(0, 15)))\nsemis\n\nMarked planar point pattern: 281 points\nMultitype, with levels = B, P \nwindow: rectangle = [0, 15] x [0, 15] units\n\n\nMarks can be numeric or categorical. Note that for categorical marks as is the case here, the variable must be explicitly converted to a factor.\nThe plot function applied to a point pattern shows a diagram of the pattern.\n\nplot(semis)\n\n\n\n\nThe intensity function calculates the density of points of each species by unit area (here, by \\(m^2\\)).\n\nintensity(semis)\n\n        B         P \n0.6666667 0.5822222 \n\n\nTo first analyze the distribution of each species separately, we split the pattern with split. Since the pattern contains categorical marks, it is automatically split according to the values of those marks. The result is a list of two point patterns.\n\nsemis_split <- split(semis)\nplot(semis_split)\n\n\n\n\nThe Kest function calculates Ripley’s \\(K\\) for a series of distances up to (by default) 1/4 of the width of the window. Here we apply it to the first pattern (birch) by choosing semis_split[[1]]. Note that double square brackets are necessary to choose an item from a list in R.\nThe argument correction = \"iso\" tells the function to apply Ripley’s correction for edge effects.\n\nk <- Kest(semis_split[[1]], correction = \"iso\")\nplot(k)\n\n\n\n\nAccording to this graph, there seems to be an excess of neighbours for distances of 1 m and above. To check if this is a significant difference, we produce a simulation envelope with the envelope function. The first argument of envelope is a point pattern to which the simulations will be compared, the second one is a function to be computed (here, Kest) for each simulated pattern, then we add the arguments of the Kest function (here, only correction).\n\nplot(envelope(semis_split[[1]], Kest, correction = \"iso\"))\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\n\n\nAs indicated by the message, by default the function performs 99 simulations of the null model corresponding to complete spatial randomness (CSR).\nThe observed curve falls outside the envelope of the 99 simulations near \\(r = 2\\). We must be careful not to interpret too quickly a result that is outside the envelope. Although there is about a 1% probability of obtaining a more extreme result under the null hypothesis at a given distance, the envelope is calculated for a large number of values of \\(r\\) and is not corrected for multiple comparisons. Thus, a significant difference for a very small range of values of \\(r\\) may be simply due to chance.\n\nExercise 1\nLooking at the graph of the second point pattern (poplar seedlings), can you predict where Ripley’s \\(K\\) will be in relation to the null hypothesis of complete spatial randomness? Verify your prediction by calculating Ripley’s \\(K\\) for this point pattern in R."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effect-of-heterogeneity",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effect-of-heterogeneity",
    "title": "Spatial Statistics in Ecology",
    "section": "Effect of heterogeneity",
    "text": "Effect of heterogeneity\nThe graph below illustrates a heterogeneous point pattern, i.e. it shows an density gradient (more points on the left than on the right).\n\n\n\n\n\nA density gradient can be confused with an aggregation of points, as can be seen on the graph of the corresponding Ripley’s \\(K\\). In theory, these are two different processes:\n\nHeterogeneity: The density of points varies in the study area, for example due to the fact that certain local conditions are more favorable to the presence of the species of interest.\nAggregation: The mean density of points is homogeneous, but the presence of one point increases the presence of other points in its vicinity, for example due to positive interactions between individuals.\n\nHowever, it may be difficult to differentiate between the two in practice, especially since some patterns may be both heterogeneous and aggregated.\nLet’s take the example of the poplar seedlings from the previous exercise. The density function applied to a point pattern performs a kernel density estimation of the density of the seedlings across the plot. By default, this function uses a Gaussian kernel with a standard deviation sigma specified in the function, which determines the scale at which density fluctuations are “smoothed”. Here, we use a value of 2 m for sigma and we first represent the estimated density with plot, before overlaying the points (add = TRUE means that the points are added to the existing plot rather than creating a new plot).\n\ndens_p <- density(semis_split[[2]], sigma = 2)\nplot(dens_p)\nplot(semis_split[[2]], add = TRUE)\n\n\n\n\nTo measure the aggregation or repulsion of points in a heterogeneous pattern, we must use the inhomogeneous version of the \\(K\\) statistic (Kinhom in spatstat). This statistic is still equal to the mean number of neighbours within a radius \\(r\\) of a point in the pattern, but rather than standardizing this number by the overall intensity of the pattern, it is standardized by the local estimated density. As above, we specify sigma = 2 to control the level of smoothing for the varying density estimate.\n\nplot(Kinhom(semis_split[[2]], sigma = 2, correction = \"iso\"))\n\n\n\n\nTaking into account the heterogeneity of the pattern at a scale sigma of 2 m, there seems to be a deficit of neighbours starting at a radius of about 1.5 m. We can now check whether this deviation is significant.\nAs before, we use envelope to simulate the Kinhom statistic under the null model. However, the null model here is not a homogeneous Poisson process (CSR). It is instead a heterogeneous Poisson process simulated by the function rpoispp(dens_p), i.e. the points are independent of each other, but their density is heterogeneous and given by dens_p. The simulate argument of the envelope function specifies the function used for simulations under the null model; this function must have one argument, here x, even if it is not used.\nFinally, in addition to the arguments needed for Kinhom, i.e. sigma and correction, we also specify nsim = 199 to perform 199 simulations and nrank = 5 to eliminate the 5 most extreme results on each side of the envelope, i.e. the 10 most extreme results out of 199, to achieve an interval containing about 95% of the probability under the null hypothesis.\n\nkhet_p <- envelope(semis_split[[2]], Kinhom, sigma = 2,  correction = \"iso\",\n                   nsim = 199, nrank = 5, simulate = function(x) rpoispp(dens_p))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(khet_p)\n\n\n\n\nNote: For a hypothesis test based on simulations of a null hypothesis, the \\(p\\)-value is estimated by \\((m + 1)/(n + 1)\\), where \\(n\\) is the number of simulations and \\(m\\) is the number of simulations where the value of the statistic is more extreme than that of the observed data. This is why the number of simulations is often chosen to be 99, 199, etc.\n\nExercise 2\nRepeat the heterogeneous density estimation and Kinhom calculation with a standard deviation sigma of 5 rather than 2. How does the smoothing level for the density estimation influence the conclusions?\nTo differentiate between a variation in the density of points from an interaction (aggregation or repulsion) between these points with this type of analysis, it is generally assumed that the two processes operate at different scales. Typically, we can test whether the points are aggregated at a small scale after accounting for a variation in density at a larger scale."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#relationship-between-two-point-patterns",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#relationship-between-two-point-patterns",
    "title": "Spatial Statistics in Ecology",
    "section": "Relationship between two point patterns",
    "text": "Relationship between two point patterns\nLet’s consider a case where we have two point patterns, for example the position of trees of two species in a plot (orange and green points in the graph below). Each of the two patterns may or may not present an aggregation of points.\n\n\n\n\n\nRegardless of whether points are aggregated at the species level, we want to determine whether the two species are arranged independently. In other words, does the probability of observing a tree of one species depend on the presence of a tree of the other species at a given distance?\nThe bivariate version of Ripley’s \\(K\\) allows us to answer this question. For two patterns noted 1 and 2, the function \\(K_{12}(r)\\) calculates the mean number of points in pattern 2 within a radius \\(r\\) from a point in pattern 1, standardized by the density of pattern 2.\nIn theory, this function is symmetrical, so \\(K_{12}(r) = K_{21}(r)\\) and the result would be the same whether the points of pattern 1 or 2 are chosen as “focal” points for the analysis. However, the estimation of the two quantities for an observed pattern may differ, in particular because of edge effects. The variance of \\(K_{12}\\) and \\(K_{21}\\) between simulations of a null model may also differ, so the null hypothesis test may have more or less power depending on the choice of the focal species.\nThe choice of an appropriate null model is important here. In order to determine whether there is a significant attraction or repulsion between the two patterns, the position of one of the patterns must be randomly moved relative to that of the other pattern, while keeping the spatial structure of each pattern taken in isolation.\nOne way to do this randomization is to shift one of the two patterns horizontally and/or vertically by a random distance. The part of the pattern that “comes out” on one side of the window is attached to the other side. This method is called a toroidal shift, because by connecting the top and bottom as well as the left and right of a rectangular surface, we obtain the shape of a torus (a three-dimensional “donut”).\n\n\n\n\n\nThe graph above shows a translation of the green pattern to the right, while the orange pattern remains in the same place. The green points in the shaded area are brought back on the other side. Note that while this method generally preserves the structure of each pattern while randomizing their relative position, it can have some drawbacks, such as dividing point clusters that are near the cutoff point.\nLet’s now check whether the position of the two species (birch and poplar) is independent in our plot. The function Kcross calculates the bivariate \\(K_{ij}\\), we must specify which type of point (mark) is considered as the focal species \\(i\\) and the neighbouring species \\(j\\).\n\nplot(Kcross(semis, i = \"P\", j = \"B\", correction = \"iso\"))\n\n\n\n\nHere, the observed \\(K\\) is lower than the theoretical value, indicating a possible repulsion between the two patterns.\nTo determine the envelope of the \\(K\\) under the null hypothesis of independence of the two patterns, we must specify that the simulations are based on a translation of the patterns. We indicate that the simulations use the function rshift (random translation) with the argument simulate = function(x) rshift(x, which = \"B\"); here, the x argument in simulate corresponds to the original point pattern and the which argument indicates which of the patterns is translated. As in the previous case, the arguments needed for Kcross, i.e. i, j and correction, must be repeated in the envelope function.\n\nplot(envelope(semis, Kcross, i = \"P\", j = \"B\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = function(x) rshift(x, which = \"B\")))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nHere, the observed curve is totally within the envelope, so we do not reject the null hypothesis of independence of the two patterns.\n\nQuestions\n\nWhat would be one reason for our choice to translate the points of the birch rather than poplar?\nWould the simulations generated by random translation be a good null model if the two patterns were heterogeneous?"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#marked-point-patterns",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#marked-point-patterns",
    "title": "Spatial Statistics in Ecology",
    "section": "Marked point patterns",
    "text": "Marked point patterns\nThe fir.csv dataset contains the \\((x, y)\\) coordinates of 822 fir trees in a 1 hectare plot and their status (A = alive, D = dead) following a spruce budworm outbreak.\n\nfir <- read.csv(\"data/fir.csv\")\nhead(fir)\n\n      x     y status\n1 31.50  1.00      A\n2 85.25 30.75      D\n3 83.50 38.50      A\n4 84.00 37.75      A\n5 83.00 33.25      A\n6 33.25  0.25      A\n\n\n\nfir <- ppp(x = fir$x, y = fir$y, marks = as.factor(fir$status),\n           window = owin(xrange = c(0, 100), yrange = c(0, 100)))\nplot(fir)\n\n\n\n\nSuppose that we want to check whether fir mortality is independent or correlated between neighbouring trees. How does this question differ from the previous example, where we wanted to know if the position of the points of two species was independent?\nIn the previous example, the independence or interaction between the species referred to the formation of the pattern itself (whether or not seedlings of one species establish near those of the other species). Here, the characteristic of interest (survival) occurs after the establishment of the pattern, assuming that all those trees were alive at first and that some died as a result of the outbreak. So we take the position of the trees as fixed and we want to know whether the distribution of status (dead, alive) among those trees is random or shows a spatial pattern.\nIn Wiegand and Moloney’s textbook, the first situation (establishment of seedlings of two species) is called a bivariate pattern, so it is really two interacting patterns, while the second is a single pattern with a qualitative mark. The spatstat package in R does not differentiate between the two in terms of pattern definition (types of points are always represented by the marks argument), but the analysis methods applied to the two questions differ.\nIn the case of a pattern with a qualitative mark, we can define a mark connection function \\(p_{ij}(r)\\). For two points separated by a distance \\(r\\), this function gives the probability that the first point has the mark \\(i\\) and the second the mark \\(j\\). Under the null hypothesis where the marks are independent, this probability is equal to the product of the proportions of each mark in the entire pattern, \\(p_{ij}(r) = p_i p_j\\) independently of \\(r\\).\nIn spatstat, the mark connection function is computed with the markconnect function, where the marks \\(i\\) and \\(j\\) and the type of edge correction must be specified. In our example, we see that two closely spaced points are less likely to have a different status (A and D) than expected under the assumption of random and independent distribution of marks (red dotted line).\n\nplot(markconnect(fir, i = \"A\", j = \"D\", correction = \"iso\"))\n\n\n\n\nIn this graph, the fluctuations in the function are due to the estimation error of a continuous \\(r\\) function from a limited number of discrete point pairs.\nTo simulate the null model in this case, we use the rlabel function, which randomly reassigns the marks among the points of the pattern, keeping the points’ positions fixed.\n\nplot(envelope(fir, markconnect, i = \"A\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nNote that since the rlabel function has only one required argument corresponding to the original point pattern, it was not necessary to specify: simulate = function(x) rlabel(x).\nHere are the results for tree pairs of the same status A or D:\n\npar(mfrow = c(1, 2))\nplot(envelope(fir, markconnect, i = \"A\", j = \"A\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(envelope(fir, markconnect, i = \"D\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nIt therefore appears that fir mortality due to this outbreak is spatially aggregated, since trees located in close proximity to each other have a greater probability of sharing the same status than predicted by the null hypothesis."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#references",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#references",
    "title": "Spatial Statistics in Ecology",
    "section": "References",
    "text": "References\nFortin, M.-J. and Dale, M.R.T. (2005) Spatial Analysis: A Guide for Ecologists. Cambridge University Press: Cambridge, UK.\nWiegand, T. and Moloney, K.A. (2013) Handbook of Spatial Point-Pattern Analysis in Ecology, CRC Press.\nThe dataset in the last example is a subet of the Lake Duparquet Research and Teaching Forest (LDRTF) data, available on Dryad here."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#intrinsic-or-induced-dependence",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#intrinsic-or-induced-dependence",
    "title": "Spatial Statistics in Ecology",
    "section": "Intrinsic or induced dependence",
    "text": "Intrinsic or induced dependence\nThere are two basic types of spatial dependence on a measured variable \\(y\\): an intrinsic dependence on \\(y\\), or a dependence induced by external variables influencing \\(y\\), which are themselves spatially correlated.\nFor example, suppose that the abundance of a species is correlated between two sites located near each other:\n\nthis spatial dependence can be induced if it is due to a spatial correlation of habitat factors that are favorable or unfavorable to the species;\nor it can be intrinsic if it is due to the dispersion of individuals to nearby sites.\n\nIn many cases, both types of dependence affect a given variable.\nIf the dependence is simply induced and the external variables that cause it are included in the model explaining \\(y\\), then the model residuals will be independent and we can use all the methods already seen that ignore spatial correlation.\nHowever, if the dependence is intrinsic or due to unmeasured external factors, then the spatial correlation of the residuals in the model will have to be taken into account."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#different-ways-to-model-spatial-effects",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#different-ways-to-model-spatial-effects",
    "title": "Spatial Statistics in Ecology",
    "section": "Different ways to model spatial effects",
    "text": "Different ways to model spatial effects\nIn this training, we will directly model the spatial correlations of our data. It is useful to compare this approach to other ways of including spatial aspects in a statistical model.\nFirst, we could include predictors in the model that represent position (e.g., longitude, latitude). Such predictors may be useful for detecting a systematic large-scale trend or gradient, whether or not the trend is linear (e.g., with a generalized additive model).\nIn contrast to this approach, the models we will see now serve to model a spatial correlation in the random fluctuations of a variable (i.e., in the residuals after removing any systematic effect).\nMixed models use random effects to represent the non-independence of data on the basis of their grouping, i.e., after accounting for systematic fixed effects, data from the same group are more similar (their residual variation is correlated) than data from different groups. These groups were sometimes defined according to spatial criteria (observations grouped into sites).\nHowever, in the context of a random group effect, all groups are as different from each other, e.g., two sites within 100 km of each other are no more or less similar than two sites 2 km apart.\nThe methods we will see here and in the next parts of the training therefore allow us to model non-independence on a continuous scale (closer = more correlated) rather than just discrete (hierarchy of groups)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogram",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogram",
    "title": "Spatial Statistics in Ecology",
    "section": "Variogram",
    "text": "Variogram\nA central aspect of geostatistics is the estimation of the variogram \\(\\gamma_z\\) . The variogram is equal to half the mean square difference between the values of \\(z\\) for two points \\((x_i, y_i)\\) and \\((x_j, y_j)\\) separated by a distance \\(h\\).\n\\[\\gamma_z(h) = \\frac{1}{2} \\text{E} \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h}\\]\nIn this equation, the \\(\\text{E}\\) function with the index \\(d_{ij}=h\\) designates the statistical expectation (i.e., the mean) of the squared deviation between the values of \\(z\\) for points separated by a distance \\(h\\).\nIf we want instead to express the autocorrelation \\(\\rho_z(h)\\) between measures of \\(z\\) separated by a distance \\(h\\), it is related to the variogram by the equation:\n\\[\\gamma_z = \\sigma_z^2(1 - \\rho_z)\\] ,\nwhere \\(\\sigma_z^2\\) is the global variance of \\(z\\).\nNote that \\(\\gamma_z = \\sigma_z^2\\) when we reach a distance where the measurements of \\(z\\) are independent, so \\(\\rho_z = 0\\). In this case, we can see that \\(\\gamma_z\\) is similar to a variance, although it is sometimes called “semivariogram” or “semivariance” because of the 1/2 factor in the above equation."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#theoretical-models-for-the-variogram",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#theoretical-models-for-the-variogram",
    "title": "Spatial Statistics in Ecology",
    "section": "Theoretical models for the variogram",
    "text": "Theoretical models for the variogram\nSeveral parametric models have been proposed to represent the spatial correlation as a function of the distance between sampling points. Let us first consider a correlation that decreases exponentially:\n\\[\\rho_z(h) = e^{-h/r}\\]\nHere, \\(\\rho_z = 1\\) for \\(h = 0\\) and the correlation is multiplied by \\(1/e \\approx 0.37\\) each time the distance increases by \\(r\\). In this context, \\(r\\) is called the range of the correlation.\nFrom the above equation, we can calculate the corresponding variogram.\n\\[\\gamma_z(h) = \\sigma_z^2 (1 - e^{-h/r})\\]\nHere is a graphical representation of this variogram.\n\n\n\n\n\nBecause of the exponential function, the value of \\(\\gamma\\) at large distances approaches the global variance \\(\\sigma_z^2\\) without exactly reaching it. This asymptote is called a sill in the geostatistical context and is represented by the symbol \\(s\\).\nFinally, it is sometimes unrealistic to assume a perfect correlation when the distance tends towards 0, because of a possible variation of \\(z\\) at a very small scale. A nugget effect, denoted \\(n\\), can be added to the model so that \\(\\gamma\\) approaches \\(n\\) (rather than 0) if \\(h\\) tends towards 0. The term nugget comes from the mining origin of these techniques, where a nugget could be the source of a sudden small-scale variation in the concentration of a mineral.\nBy adding the nugget effect, the remainder of the variogram is “compressed” to keep the same sill, resulting in the following equation.\n\\[\\gamma_z(h) = n + (s - n) (1 - e^{-h/r})\\]\nIn the gstat package that we use below, the term \\((s-n)\\) is called a partial sill or psill for the exponential portion of the variogram.\n\n\n\n\n\nIn addition to the exponential model, two other common theoretical models for the variogram are the Gaussian model (where the correlation follows a half-normal curve), and the spherical model (where the variogram increases linearly at the start and then curves and reaches the plateau at a distance equal to its range \\(r\\)). The spherical model thus allows the correlation to be exactly 0 at large distances, rather than gradually approaching zero in the case of the other models.\n\n\n\n\n\n\n\n\nModel\n\\(\\rho(h)\\)\n\\(\\gamma(h)\\)\n\n\n\n\nExponential\n\\(\\exp\\left(-\\frac{h}{r}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h}{r}\\right)\\right)\\)\n\n\nGaussian\n\\(\\exp\\left(-\\frac{h^2}{r^2}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h^2}{r^2}\\right)\\right)\\)\n\n\nSpherical \\((h < r)\\) *\n\\(1 - \\frac{3}{2}\\frac{h}{r} + \\frac{1}{2}\\frac{h^3}{r^3}\\)\n\\(s \\left(\\frac{3}{2}\\frac{h}{r} - \\frac{1}{2}\\frac{h^3}{r^3} \\right)\\)\n\n\n\n* For the spherical model, \\(\\rho = 0\\) and \\(\\gamma = s\\) if \\(h \\ge r\\)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#empirical-variogram",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#empirical-variogram",
    "title": "Spatial Statistics in Ecology",
    "section": "Empirical variogram",
    "text": "Empirical variogram\nTo estimate \\(\\gamma_z(h)\\) from empirical data, we need to define distance classes, thus grouping different distances within a margin of \\(\\pm \\delta\\) around a distance \\(h\\), then calculating the mean square deviation for the pairs of points in that distance class.\n\\[\\hat{\\gamma_z}(h) = \\frac{1}{2 N_{\\text{paires}}} \\sum \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h \\pm \\delta}\\]\nWe will see in the next section how to estimate a variogram in R."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#regression-model-with-spatial-correlation",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#regression-model-with-spatial-correlation",
    "title": "Spatial Statistics in Ecology",
    "section": "Regression model with spatial correlation",
    "text": "Regression model with spatial correlation\nThe following equation represents a multiple linear regression including residual spatial correlation:\n\\[v = \\beta_0 + \\sum_i \\beta_i u_i + z + \\epsilon\\]\nHere, \\(v\\) designates the response variable and \\(u\\) the predictors, to avoid confusion with the spatial coordinates \\(x\\) and \\(y\\).\nIn addition to the residual \\(\\epsilon\\) that is independent between observations, the model includes a term \\(z\\) that represents the spatially correlated portion of the residual variance.\nHere are suggested steps to apply this type of model:\n\nFit the regression model without spatial correlation.\nVerify the presence of spatial correlation from the empirical variogram of the residuals.\nFit one or more regression models with spatial correlation and select the one that shows the best fit to the data."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#regression-with-spatial-correlation",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#regression-with-spatial-correlation",
    "title": "Spatial Statistics in Ecology",
    "section": "Regression with spatial correlation",
    "text": "Regression with spatial correlation\nWe have seen above that the gstat package allows us to estimate the variogram of the residuals of a linear model. In our example, the magnesium concentration was modeled as a function of pH, with spatially correlated residuals.\nAnother tool to fit this same type of model is the gls function of the nlme package, which is included with the installation of R.\nThis function applies the generalized least squares method to fit linear regression models when the residuals are not independent or when the residual variance is not the same for all observations. Since the estimates of the coefficients depend on the estimated correlations between the residuals and the residuals themselves depend on the coefficients, the model is fitted by an iterative algorithm:\n\nA classical linear regression model (without correlation) is fitted to obtain residuals.\nThe spatial correlation model (variogram) is fitted with those residuals.\nThe regression coefficients are re-estimated, now taking into account the correlations.\n\nSteps 2 and 3 are repeated until the estimates are stable at a desired precision.\nHere is the application of this method to the same model for the magnesium concentration in the oxford dataset. In the correlation argument of gls, we specify an exponential correlation model as a function of our spatial coordinates and we include a possible nugget effect.\nIn addition to the exponential correlation corExp, the gls function can also estimate a Gaussian (corGaus) or spherical (corSpher) model.\n\nlibrary(nlme)\ngls_mg <- gls(MG1 ~ PH1, oxford, \n              correlation = corExp(form = ~ XCOORD + YCOORD, nugget = TRUE))\nsummary(gls_mg)\n\nGeneralized least squares fit by REML\n  Model: MG1 ~ PH1 \n  Data: oxford \n      AIC      BIC   logLik\n  1278.65 1292.751 -634.325\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~XCOORD + YCOORD \n Parameter estimate(s):\n      range      nugget \n478.0322964   0.2944753 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 391.1387  50.42343  7.757084       0\nPH1         -41.0836   6.15662 -6.673079       0\n\n Correlation: \n    (Intr)\nPH1 -0.891\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.1846957 -0.6684520 -0.3687813  0.4627580  3.1918604 \n\nResidual standard error: 53.8233 \nDegrees of freedom: 126 total; 124 residual\n\n\nTo compare this result with the adjusted variogram above, the parameters given by gls must be transformed. The range has the same meaning in both cases and corresponds to 478 m for the result of gls. The global variance of the residuals is the square of Residual standard error. The nugget effect here (0.294) is expressed as a fraction of that variance. Finally, to obtain the partial sill of the exponential part, the nugget effect must be subtracted from the total variance.\nAfter performing these calculations, we can give these parameters to the vgm function of gstat to superimpose this variogram estimated by gls on our variogram of the residuals of the classical linear model.\n\ngls_range <- 478\ngls_var <- 53.823^2\ngls_nugget <- 0.294 * gls_var\ngls_psill <- gls_var - gls_nugget\n\ngls_vgm <- vgm(\"Exp\", psill = gls_psill, range = gls_range, nugget = gls_nugget)\n\nplot(var_mg, gls_vgm, col = \"black\", ylim = c(0, 4000))\n\n\n\n\nDoes the model fit the data less well here? In fact, this empirical variogram represented by the points was obtained from the residuals of the linear model ignoring the spatial correlation, so it is a biased estimate of the actual spatial correlations. The method is still adequate to quickly check if spatial correlations are present. However, to simultaneously fit the regression coefficients and the spatial correlation parameters, the generalized least squares (GLS) approach is preferable and will produce more accurate estimates.\nFinally, note that the result of the gls model also gives the AIC, which we can use to compare the fit of different models (with different predictors or different forms of spatial correlation)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercise",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercise",
    "title": "Spatial Statistics in Ecology",
    "section": "Exercise",
    "text": "Exercise\nThe bryo_belg.csv dataset is adapted from the data of this study:\n\nNeyens, T., Diggle, P.J., Faes, C., Beenaerts, N., Artois, T. et Giorgi, E. (2019) Mapping species richness using opportunistic samples: a case study on ground-floor bryophyte species richness in the Belgian province of Limburg. Scientific Reports 9, 19122. https://doi.org/10.1038/s41598-019-55593-x\n\nThis data frame shows the specific richness of ground bryophytes (richness) for different sampling points in the Belgian province of Limburg, with their position (x, y) in km, in addition to information on the proportion of forest (forest) and wetlands (wetland) in a 1 km^2$ cell containing the sampling point.\n\nbryo_belg <- read.csv(\"data/bryo_belg.csv\")\nhead(bryo_belg)\n\n  richness    forest   wetland        x        y\n1        9 0.2556721 0.5036614 228.9516 220.8869\n2        6 0.6449114 0.1172068 227.6714 219.8613\n3        5 0.5039905 0.6327003 228.8252 220.1073\n4        3 0.5987329 0.2432942 229.2775 218.9035\n5        2 0.7600775 0.1163538 209.2435 215.2414\n6       10 0.6865434 0.0000000 210.4142 216.5579\n\n\nFor this exercise, we will use the square root of the specific richness as the response variable. The square root transformation often allows to homogenize the variance of the count data in order to apply a linear regression.\n\nFit a linear model of the transformed species richness to the proportion of forest and wetlands, without taking into account spatial correlations. What is the effect of the two predictors in this model?\nCalculate the empirical variogram of the model residuals in (a). Does there appear to be a spatial correlation between the points?\n\nNote: The cutoff argument to the variogram function specifies the maximum distance at which the variogram is calculated. You can manually adjust this value to get a good view of the sill.\n\nRe-fit the linear model in (a) with the gls function in the nlme package, trying different types of spatial correlations (exponential, Gaussian, spherical). Compare the models (including the one without spatial correlation) with the AIC.\nWhat is the effect of the proportion of forests and wetlands according to the model in (c)? Explain the differences between the conclusions of this model and the model in (a)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#conditional-autoregressive-car-model",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#conditional-autoregressive-car-model",
    "title": "Spatial Statistics in Ecology",
    "section": "Conditional autoregressive (CAR) model",
    "text": "Conditional autoregressive (CAR) model\nIn the conditional autoregressive model, the value of \\(z_i\\) for the region \\(i\\) follows a normal distribution: its mean depends on the value \\(z_j\\) of neighbouring regions, multiplied by the weight \\(w_{ij}\\) and a correlation coefficient \\(\\rho\\); its standard deviation \\(\\sigma_{z_i}\\) may vary from one region to another.\n\\[z_i \\sim \\text{N}\\left(\\sum_j \\rho w_{ij} z_j,\\sigma_{z_i} \\right)\\]\nIn this model, if \\(w_{ij}\\) is a binary matrix (0 for non-neighbours, 1 for neighbours), then \\(\\rho\\) is the coefficient of partial correlation between neighbouring regions. This is similar to a first-order autoregressive model in the context of time series, where the autoregression coefficient indicates the partial correlation."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#simultaneous-autoregressive-sar-model",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#simultaneous-autoregressive-sar-model",
    "title": "Spatial Statistics in Ecology",
    "section": "Simultaneous autoregressive (SAR) model",
    "text": "Simultaneous autoregressive (SAR) model\nIn the simultaneous autoregressive model, the value of \\(z_i\\) is given directly by the sum of contributions from neighbouring values \\(z_j\\), multiplied by \\(\\rho w_{ij}\\), with an independent residual \\(\\nu_i\\) of standard deviation \\(\\sigma_z\\).\n\\[z_i = \\sum_j \\rho w_{ij} z_j + \\nu_i\\]\nAt first glance, this looks like a temporal autoregressive model. However, there is an important conceptual difference. For temporal models, the causal influence is directed in only one direction: \\(v(t-2)\\) affects \\(v(t-1)\\) which then affects \\(v(t)\\). For a spatial model, each \\(z_j\\) that affects \\(z_i\\) depends in turn on \\(z_i\\). Thus, to determine the joint distribution of \\(z\\), a system of equations must be solved simultaneously (hence the name of the model).\nFor this reason, although this model resembles the formula of CAR model, the solutions of the two models differ and in the case of SAR, the coefficient \\(\\rho\\) is not directly equal to the partial correlation due to each neighbouring region.\nFor more details on the mathematical aspects of these models, see the article by Ver Hoef et al. (2018) suggested in reference.\nFor the moment, we will consider SAR and CAR as two types of possible models to represent a spatial correlation on a network. We can always fit several models and compare them with the AIC to choose the best form of correlation or the best weight matrix.\nThe CAR and SAR models share an advantage over geostatistical models in terms of efficiency. In a geostatistical model, spatial correlations are defined between each pair of points, although they become negligible as distance increases. For a CAR or SAR model, only neighbouring regions contribute and most weights are equal to 0, making these models faster to fit than a geostatistical model when the data are massive."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#definition-of-the-neighbourhood-network",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#definition-of-the-neighbourhood-network",
    "title": "Spatial Statistics in Ecology",
    "section": "Definition of the neighbourhood network",
    "text": "Definition of the neighbourhood network\nThe poly2nb function of the spdep package defines a neighbourhood network from polygons. The result vois is a list of 125 elements where each element contains the indices of the neighbouring (bordering) polygons of a given polygon.\n\nvois <- poly2nb(elect2018)\nvois[[1]]\n\n[1]   2  37  63  88 101 117\n\n\nThus, the first riding (Abitibi-Est) has 6 neighbouring ridings, for which the names can be found as follows:\n\nelect2018$circ[vois[[1]]]\n\n[1] \"Abitibi-Ouest\"               \"Gatineau\"                   \n[3] \"Laviolette-Saint-Maurice\"    \"Pontiac\"                    \n[5] \"Rouyn-Noranda-Témiscamingue\" \"Ungava\"                     \n\n\nWe can illustrate this network by extracting the coordinates of the center of each district, creating a blank map with plot(elect2018[\"geometry\"]), then adding the network as an additional layer with plot(vois, add = TRUE, coords = coords).\n\ncoords <- st_centroid(elect2018) %>%\n    st_coordinates()\nplot(elect2018[\"geometry\"])\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nWe can “zoom” on southern Québec by choosing the limits xlim and ylim.\n\nplot(elect2018[\"geometry\"], \n     xlim = c(400000, 800000), ylim = c(100000, 500000))\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nWe still have to add weights to each network link with the nb2listw function. The style of weights “B” corresponds to binary weights, i.e. 1 for the presence of link and 0 for the absence of link between two ridings.\nOnce these weights are defined, we can verify with Moran’s test whether there is a significant autocorrelation of votes obtained by the CAQ between neighbouring ridings.\n\npoids <- nb2listw(vois, style = \"B\")\n\nmoran.test(elect2018$propCAQ, poids)\n\n\n    Moran I test under randomisation\n\ndata:  elect2018$propCAQ  \nweights: poids    \n\nMoran I statistic standard deviate = 13.148, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.680607768      -0.008064516       0.002743472 \n\n\nThe value \\(I = 0.68\\) is very significant judging by the \\(p\\)-value of the test.\nLet’s verify if the spatial correlation persists after taking into account the four characteristics of the population, therefore by inspecting the residuals of a linear model including these four predictors.\n\nelect_lm <- lm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, data = elect2018)\nsummary(elect_lm)\n\n\nCall:\nlm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.9890  -4.4878   0.0562   6.2653  25.8146 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.354e+01  1.836e+01   0.737    0.463    \nage_moy     -9.170e-01  3.855e-01  -2.378    0.019 *  \npct_frn      4.588e+01  5.202e+00   8.820 1.09e-14 ***\npct_prp      3.582e+01  6.527e+00   5.488 2.31e-07 ***\nrev_med     -2.624e-05  2.465e-04  -0.106    0.915    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.409 on 120 degrees of freedom\nMultiple R-squared:  0.6096,    Adjusted R-squared:  0.5965 \nF-statistic: 46.84 on 4 and 120 DF,  p-value: < 2.2e-16\n\nmoran.test(residuals(elect_lm), poids)\n\n\n    Moran I test under randomisation\n\ndata:  residuals(elect_lm)  \nweights: poids    \n\nMoran I statistic standard deviate = 6.7047, p-value = 1.009e-11\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.340083290      -0.008064516       0.002696300 \n\n\nMoran’s \\(I\\) has decreased but remains significant, so some of the previous correlation was induced by these predictors, but there remains a spatial correlation due to other factors."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#spatial-autoregression-models",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#spatial-autoregression-models",
    "title": "Spatial Statistics in Ecology",
    "section": "Spatial autoregression models",
    "text": "Spatial autoregression models\nFinally, we fit SAR and CAR models to these data with the spautolm (spatial autoregressive linear model) function of spatialreg. Here is the code for a SAR model including the effect of the same four predictors.\n\nelect_sar <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids)\nsummary(elect_sar)\n\n\nCall: spautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-23.08342  -4.10573   0.24274   4.29941  23.08245 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 15.09421119 16.52357745  0.9135   0.36098\nage_moy     -0.70481703  0.32204139 -2.1886   0.02863\npct_frn     39.09375061  5.43653962  7.1909 6.435e-13\npct_prp     14.32329345  6.96492611  2.0565   0.03974\nrev_med      0.00016730  0.00023209  0.7208   0.47101\n\nLambda: 0.12887 LR test value: 42.274 p-value: 7.9339e-11 \nNumerical Hessian standard error of lambda: 0.012069 \n\nLog likelihood: -433.8862 \nML residual variance (sigma squared): 53.028, (sigma: 7.282)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 881.77\n\n\nThe value given by Lambda in the summary corresponds to the coefficient \\(\\rho\\) in our description of the model. The likelihood-ratio test (LR test) confirms that this residual spatial correlation (after controlling for the effect of predictors) is significant.\nThe estimated effects for the predictors are similar to those of the linear model without spatial correlation. The effects of mean age, fraction of francophones and fraction of homeowners remain significant, although their magnitude has decreased somewhat.\nTo fit a CAR rather than SAR model, we must specify family = \"CAR\".\n\nelect_car <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids, family = \"CAR\")\nsummary(elect_car)\n\n\nCall: spautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids, family = \"CAR\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-21.73315  -4.24623  -0.24369   3.44228  23.43749 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 16.57164696 16.84155327  0.9840  0.325128\nage_moy     -0.79072151  0.32972225 -2.3981  0.016478\npct_frn     38.99116707  5.43667482  7.1719 7.399e-13\npct_prp     17.98557474  6.80333470  2.6436  0.008202\nrev_med      0.00012639  0.00023106  0.5470  0.584364\n\nLambda: 0.15517 LR test value: 40.532 p-value: 1.9344e-10 \nNumerical Hessian standard error of lambda: 0.0026868 \n\nLog likelihood: -434.7573 \nML residual variance (sigma squared): 53.9, (sigma: 7.3416)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 883.51\n\n\nFor a CAR model with binary weights, the value of Lambda (which we called \\(\\rho\\)) directly gives the partial correlation coefficient between neighbouring districts. Note that the AIC here is slightly higher than the SAR model, so the latter gave a better fit."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercise-3",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercise-3",
    "title": "Spatial Statistics in Ecology",
    "section": "Exercise",
    "text": "Exercise\nThe rls_covid dataset, in shapefile format, contains data on detected COVID-19 cases (cas), number of cases per 1000 people (taux_1k) and the population density (dens_pop) in each of Quebec’s local health service networks (RLS) (Source: Data downloaded from the Institut national de santé publique du Québec as of January 17, 2021).\n\nrls_covid <- read_sf(\"data/rls_covid.shp\")\nhead(rls_covid)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 785111.2 ymin: 341057.8 xmax: 979941.5 ymax: 541112.7\nProjected CRS: Conique_conforme_de_Lambert_du_MTQ_utilis_e_pour_Adresse_Qu_be\n# A tibble: 6 × 6\n  RLS_code RLS_nom                 cas taux_1k dens_…¹                  geometry\n  <chr>    <chr>                 <dbl>   <dbl>   <dbl>        <MULTIPOLYGON [m]>\n1 0111     RLS de Kamouraska       152    7.34    6.76 (((827028.3 412772.4, 82…\n2 0112     RLS de Rivière-du-Lo…   256    7.34   19.6  (((855905 452116.9, 8557…\n3 0113     RLS de Témiscouata       81    4.26    4.69 (((911829.4 441311.2, 91…\n4 0114     RLS des Basques          28    3.3     5.35 (((879249.6 471975.6, 87…\n5 0115     RLS de Rimouski         576    9.96   15.5  (((917748.1 503148.7, 91…\n6 0116     RLS de La Mitis          76    4.24    5.53 (((951316 523499.3, 9525…\n# … with abbreviated variable name ¹​dens_pop\n\n\nFit a linear model of the number of cases per 1000 as a function of population density (it is suggested to apply a logarithmic transform to the latter). Check whether the model residuals are correlated between bordering RLS with a Moran’s test and then model the same data with a conditional autoregressive model."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#reference",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#reference",
    "title": "Spatial Statistics in Ecology",
    "section": "Reference",
    "text": "Reference\nVer Hoef, J.M., Peterson, E.E., Hooten, M.B., Hanks, E.M. and Fortin, M.-J. (2018) Spatial autoregressive models for statistical inference from ecological data. Ecological Monographs 88: 36-59."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#data",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#data",
    "title": "Spatial Statistics in Ecology",
    "section": "Data",
    "text": "Data\nThe gambia dataset found in the geoR package presents the results of a study of malaria prevalence among children of 65 villages in The Gambia. We will use a slightly transformed version of the data found in the file gambia.csv.\n\nlibrary(geoR)\n\ngambia <- read.csv(\"data/gambia.csv\")\nhead(gambia)\n\n  id_village        x        y pos  age netuse treated green phc\n1          1 349.6313 1458.055   1 1783      0       0 40.85   1\n2          1 349.6313 1458.055   0  404      1       0 40.85   1\n3          1 349.6313 1458.055   0  452      1       0 40.85   1\n4          1 349.6313 1458.055   1  566      1       0 40.85   1\n5          1 349.6313 1458.055   0  598      1       0 40.85   1\n6          1 349.6313 1458.055   1  590      1       0 40.85   1\n\n\nHere are the fields in that dataset:\n\nid_village: Identifier of the village.\nx and y: Spatial coordinates of the village (in kilometers, based on UTM coordinates).\npos: Binary response, whether the child tested positive for malaria.\nage: Age of the child in days.\nnetuse: Whether or not the child sleeps under a bed net.\ntreated: Whether or not the bed net is treated.\ngreen: Remote sensing based measure of greenness of vegetation (measured at the village level).\nphc: Presence or absence of a public health centre for the village.\n\nWe can count the number of positive cases and total children tested by village to map the fraction of positive cases (or prevalence, prev).\n\n# Create village-level dataset\ngambia_agg <- group_by(gambia, id_village, x, y, green, phc) %>%\n    summarize(pos = sum(pos), total = n()) %>%\n    mutate(prev = pos / total) %>%\n    ungroup()\n\n`summarise()` has grouped output by 'id_village', 'x', 'y', 'green'. You can\noverride using the `.groups` argument.\n\nhead(gambia_agg)\n\n# A tibble: 6 × 8\n  id_village     x     y green   phc   pos total  prev\n       <int> <dbl> <dbl> <dbl> <int> <int> <int> <dbl>\n1          1  350. 1458.  40.8     1    17    33 0.515\n2          2  359. 1460.  40.8     1    19    63 0.302\n3          3  360. 1460.  40.1     0     7    17 0.412\n4          4  364. 1497.  40.8     0     8    24 0.333\n5          5  366. 1460.  40.8     0    10    26 0.385\n6          6  367. 1463.  40.8     0     7    18 0.389\n\n\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nWe use the gambia.borders dataset from the geoR package to trace the country boundaries with geom_path. Since those boundaries are in meters, we divide by 1000 to get the same scale as our points. We also use coord_fixed to ensure a 1:1 aspect ratio between the axes and use the viridis color scale, which makes it easier to visualize a continuous variable compared with the default gradient scale in ggplot2.\nBased on this map, there seems to be spatial correlation in malaria prevalence, with the eastern cluster of villages showing more high prevalence values (yellow-green) and the middle cluster showing more low prevalence values (purple)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#non-spatial-glmm",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#non-spatial-glmm",
    "title": "Spatial Statistics in Ecology",
    "section": "Non-spatial GLMM",
    "text": "Non-spatial GLMM\nFor this first example, we will ignore the spatial aspect of the data and model the presence of malaria (pos) as a function of the use of a bed net (netuse) and the presence of a public health centre (phc). Since we have a binary response, we need to use a logistic regression model (a GLM). Since we have predictors at both the individual and village level, and we expect that children of the same village have more similar probabilities of having malaria even after accounting for those predictors, we need to add a random effect of the village. The result is a GLMM that we fit using the glmer function in the lme4 package.\n\nlibrary(lme4)\n\nmod_glmm <- glmer(pos ~ netuse + phc + (1 | id_village), \n                  data = gambia, family = binomial)\nsummary(mod_glmm)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: pos ~ netuse + phc + (1 | id_village)\n   Data: gambia\n\n     AIC      BIC   logLik deviance df.resid \n  2428.0   2450.5  -1210.0   2420.0     2031 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1286 -0.7120 -0.4142  0.8474  3.3434 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n id_village (Intercept) 0.8149   0.9027  \nNumber of obs: 2035, groups:  id_village, 65\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.1491     0.2297   0.649   0.5164    \nnetuse       -0.6044     0.1442  -4.190 2.79e-05 ***\nphc          -0.4985     0.2604  -1.914   0.0556 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) netuse\nnetuse -0.422       \nphc    -0.715 -0.025\n\n\nAccording to these results, both netuse and phc result in a decrease of malaria prevalence, although the effect of phc is not significant at a threshold \\(\\alpha = 0.05\\). The intercept (0.149) is the logit of the probability of malaria presence for a child with no bednet and no public health centre, but it is the mean intercept across all villages, and there is a lot of variation between villages, based on the random effect standard deviation of 0.90. We can get the estimated intercept for each village with the function coef:\n\nhead(coef(mod_glmm)$id_village)\n\n  (Intercept)     netuse        phc\n1  0.93727515 -0.6043602 -0.4984835\n2  0.09204843 -0.6043602 -0.4984835\n3  0.22500620 -0.6043602 -0.4984835\n4 -0.46271089 -0.6043602 -0.4984835\n5  0.13680037 -0.6043602 -0.4984835\n6 -0.03723346 -0.6043602 -0.4984835\n\n\nSo for example, the intercept for village 1 is around 0.94, equivalent to a probability of 72%:\n\nplogis(0.937)\n\n[1] 0.7184933\n\n\nwhile the intercept in village 2 is equivalent to a probability of 52%:\n\nplogis(0.092)\n\n[1] 0.5229838\n\n\nThe DHARMa package provides a general method for checking whether the residuals of a GLMM are distributed according to the specified model and whether there is any residual trend. The package works by simulating replicates of each observation according to the fitted model and then determining a “standardized residual”, which is the relative position of the observed value with respect to the simulated values, e.g. 0 if the observation is smaller than all the simulations, 0.5 if it is in the middle, etc. If the model represents the data well, each value of the standardized residual between 0 and 1 should be equally likely, so the standardized residuals should produce a uniform distribution between 0 and 1.\nThe simulateResiduals function performs the calculation of the standardized residuals, then the plot function plots the diagnostic graphs with the results of certain tests.\n\nlibrary(DHARMa)\nres_glmm <- simulateResiduals(mod_glmm)\nplot(res_glmm)\n\n\n\n\nThe graph on the left is a quantile-quantile plot of standardized residuals. The results of three statistical tests also also shown: a Kolmogorov-Smirnov (KS) test which checks whether there is a deviation from the theoretical distribution, a dispersion test that checks whether there is underdispersion or overdispersion, and an outlier test based on the number of residuals that are more extreme than all the simulations. Here, we get a significant result for the outliers, though the message indicates that this result might have an inflated type I error rate in this case.\nOn the right, we generally get a graph of standardized residuals (in y) as a function of the rank of the predicted values, in order to check for any leftover trend in the residual. Here, the predictions are binned by quartile, so it might be better to instead aggregate the predictions and residuals by village, which we can do with the recalculateResiduals function.\n\nplot(recalculateResiduals(res_glmm, group = gambia$id_village))\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nThe plot to the right now shows individual points, along with a quantile regression for the 1st quartile, the median and the 3rd quartile. In theory, these three curves should be horizontal straight lines (no leftover trend in the residuals vs. predictions). The curve for the 3rd quartile (in red) is significantly different from a horizontal line, which could indicate some systematic effect that is missing from the model."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#spatial-glmm-with-spamm",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#spatial-glmm-with-spamm",
    "title": "Spatial Statistics in Ecology",
    "section": "Spatial GLMM with spaMM",
    "text": "Spatial GLMM with spaMM\nThe spaMM (spatial mixed models) package is a relatively new R package that can perform approximate maximum likelihood estimation of parameters for GLMM with spatial dependence, modelled either as a Gaussian process or with a CAR (we will see the latter in the last section). The package implements different algorithms, but there is a single fitme function that chooses the appropriate algorithm for each model type. For example, here is the same (non-spatial) model as above fit with spaMM.\n\nlibrary(spaMM)\n\nmod_spamm_glmm <- fitme(pos ~ netuse + phc + (1 | id_village),\n                        data = gambia, family = binomial)\nsummary(mod_spamm_glmm)\n\nformula: pos ~ netuse + phc + (1 | id_village)\nEstimation of lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)   0.1491   0.2287  0.6519\nnetuse       -0.6045   0.1420 -4.2567\nphc          -0.4986   0.2593 -1.9231\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   id_village  :  0.8151  \n             --- Coefficients for log(lambda):\n      Group        Term Estimate Cond.SE\n id_village (Intercept)  -0.2045  0.2008\n# of obs: 2035; # of groups: id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1210.016\n\n\nNote that the estimates of the fixed effects as well as the variance of random effects are nearly identical to those obtained by glmer above.\nWe can now use spaMM to fit the same model with the addition of spatial correlations between villages. In the formula of the model, this is represented as a random effect Matern(1 | x + y), which means that the intercepts are spatially correlated between villages following a Matérn correlation function of coordinates (x, y). The Matérn function is a flexible function for spatial correlation that includes a shape parameter \\(\\nu\\) (nu), so that when \\(\\nu = 0.5\\) it is equivalent to the exponential correlation but as \\(\\nu\\) grows to large values, it approaches a Gaussian correlation. We could let the function estimate \\(\\nu\\), but here we will fix it to 0.5 with the fixed argument of fitme.\n\nmod_spamm <- fitme(pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village),\n                   data = gambia, family = binomial, fixed = list(nu = 0.5))\n\nIncrease spaMM.options(separation_max=<.>) to at least 21 if you want to check separation (see 'help(separation)').\n\nsummary(mod_spamm)\n\nformula: pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village)\nEstimation of corrPars and lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nEstimation of lambda by 'outer' ML, maximizing logL.\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3352  0.2047\nnetuse      -0.51719   0.1407 -3.6757\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu      1.rho \n0.50000000 0.05128692 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.6421 \n   id_village  :  0.1978  \n# of obs: 2035; # of groups: x + y, 65; id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1197.968\n\n\nLet’s first check the random effects of the model. The spatial correlation function has a parameter rho equal to 0.0513. This parameter in spaMM is the inverse of the range, so here the range of exponential correlation is 1/0.0513 or around 19.5 km. There are now two variance prameters, the one identified as x + y is the long-range variance (i.e. sill) for the exponential correlation model whereas the one identified as id_village shows the non-spatially correlated portion of the variation between villages.\nIn fact, while we left the random effects (1 | id_village) in the formula to represent the non-spatial portion of variation between villages, we could also represent this with a nugget effect in the geostatistical model. In both cases, it would represent the idea that even two villages very close to each other would have different baseline prevalences in the model.\nBy default, the Matern function has no nugget effect, but we can add one by specifying a non-zero Nugget in the initial parameter list init.\n\nmod_spamm2 <- fitme(pos ~ netuse + phc + Matern(1 | x + y),\n                    data = gambia, family = binomial, fixed = list(nu = 0.5),\n                    init = list(Nugget = 0.1))\n\nIncrease spaMM.options(separation_max=<.>) to at least 21 if you want to check separation (see 'help(separation)').\n\nsummary(mod_spamm2)\n\nformula: pos ~ netuse + phc + Matern(1 | x + y)\nEstimation of corrPars and lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nEstimation of lambda by 'outer' ML, maximizing logL.\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3352  0.2047\nnetuse      -0.51719   0.1407 -3.6757\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu   1.Nugget      1.rho \n0.50000000 0.23551027 0.05128692 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.8399  \n# of obs: 2035; # of groups: x + y, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1197.968\n\n\nAs you can see, all estimates are the same, except that the variance of the spatial portion (sill) is now 0.84 and the nugget is equal to a fraction 0.235 of that sill, so a variance of 0.197, which is the same as the id_village random effect in the version above. Thus the two formulations are equivalent.\nNow, recall the coefficients we obtained for the non-spatial GLMM:\n\nsummary(mod_glmm)$coefficients\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept)  0.1490596  0.2296971  0.6489399 5.163772e-01\nnetuse      -0.6043602  0.1442448 -4.1898243 2.791706e-05\nphc         -0.4984835  0.2604083 -1.9142382 5.558973e-02\n\n\nIn the spatial version, both fixed effects have moved slightly towards zero, but the standard error of the effect of phc has decreased. It is interesting that the inclusion of spatial dependence has allowed us to estimate more precisely the effect of having a public health centre in the village. This would not always be the case: for a predictor that is also strongly correlated in space, spatial correlation in the response makes it harder to estimate the effect of this predictor, since it is confounded with the spatial effect. However, for a predictor that is not correlated in space, including the spatial effect reduces the residual (non-spatial) variance and may thus increase the precision of the predictor’s effect.\nThe spaMM package is also compatible with DHARMa for residual diagnostics. (You can in fact ignore the warning that it is not in the class of supported models, this is due to using the fitme function rather than a specific algorithm function in spaMM.)\n\nres_spamm <- simulateResiduals(mod_spamm2)\nplot(res_spamm)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\nplot(recalculateResiduals(res_spamm, group = gambia$id_village))\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nFinally, while we will show how to make and visualize spatial predictions below, we can produce a quick map of the estimated spatial effects in a spaMM model with the filled.mapMM function.\n\nfilled.mapMM(mod_spamm2)"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#gaussian-process-models-vs.-smoothing-splines",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#gaussian-process-models-vs.-smoothing-splines",
    "title": "Spatial Statistics in Ecology",
    "section": "Gaussian process models vs. smoothing splines",
    "text": "Gaussian process models vs. smoothing splines\nIf you are familiar with generalized additive models (GAM), you might think that the spatial variation in malaria prevalence (as shown in the map above) could be represented by a 2D smoothing spline (as a function of \\(x\\) and \\(y\\)) within a GAM.\nThe code below fits the GAM equivalent of our Gaussian process GLMM above with the gam function in the mgcv package. The spatial effect is represented by the 2D spline s(x, y) whereas the non-spatial random effect of village is represented by s(id_village, bs = \"re\"), which is the same as (1 | id_village) in the previous models. Note that for the gam function, categorical variables must be explicitly converted to factors.\n\nlibrary(mgcv)\ngambia$id_village <- as.factor(gambia$id_village)\nmod_gam <- gam(pos ~ netuse + phc + s(id_village, bs = \"re\") + s(x, y), \n               data = gambia, family = binomial)\n\nTo visualize the 2D spline, we will use the gratia package.\n\nlibrary(gratia)\ndraw(mod_gam)\n\n\n\n\nNote that the plot of the spline s(x, y) (top right) does not extend too far from the locations of the data (other areas are blank). In this graph, we can also see that the village random effects follow the expected Gaussian distribution (top left).\nNext, we will use both the spatial GLMM from the previous section and this GAMM to predict the mean prevalence on a spatial grid of points contained in the file gambia_pred.csv. The graph below adds those prediction points (in black) on the previous map of the data points.\n\ngambia_pred <- read.csv(\"data/gambia_pred.csv\")\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(data = gambia_pred) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nTo make predictions from the GAMM model at those points, the code below goes through the following steps:\n\nAll predictors in the model must be in the prediction data frame, so we add constant values of netuse and phc (both equal to 1) for all points. Thus, we will make predictions of malaria prevalence in the case where a net is used and a public health centre is present. We also add a constant id_village, although it will not be used in predictions (see below).\nWe call the predict function on the output of gam to produce predictions at the new data points (argument newdata), including standard errors (se.fit = TRUE) and excluding the village random effects, so the prediction is made for an “average village”. The resulting object gam_pred will have columns fit (mean prediction) and se.fit (standard error). Those predictions and standard errors are on the link (logit) scale.\nWe add the original prediction data frame to gam_pred with cbind.\nWe add columns for the mean prediction and 50% confidence interval boundaries (mean \\(\\pm\\) 0.674 standard error), converted from the logit scale to the probability scale with plogis. We choose a 50% interval since a 95% interval may be too wide here to contrast the different predictions on the map at the end of this section.\n\n\ngambia_pred <- mutate(gambia_pred, netuse = 1, phc = 1, id_village = 1)\n\ngam_pred <- predict(mod_gam, newdata = gambia_pred, se.fit = TRUE, \n                    exclude = \"s(id_village)\")\ngam_pred <- cbind(gambia_pred, as.data.frame(gam_pred))\ngam_pred <- mutate(gam_pred, pred = plogis(fit), \n                   lo = plogis(fit - 0.674 * se.fit), # 50% CI\n                   hi = plogis(fit + 0.674 * se.fit))\n\nNote: The reason we do not make predictions directly on the probability (response) scale is that the normal formula for confidence intervals applies more accurately on the logit scale. Adding a certain number of standard errors around the mean on the probability scale would lead to less accurate intervals and maybe even confidence intervals outside the possible range (0, 1) for a probability.\nWe apply the same strategy to make predictions from the spaMM spatial GLMM model. There are a few differences in the predict method compared with the GAMM case.\n\nThe argument binding = \"fit\" means that mean predictions (fit column) will be attached to the prediction dataset and returned as spamm_pred.\nThe variances = list(linPred = TRUE) tells predict to calculate the variance of the linear predictor (so the square of the standard error). However, it appears as an attribute predVar in the output data frame rather than a se.fit column, so we move it to a column on the next line.\n\n\nspamm_pred <- predict(mod_spamm, newdata = gambia_pred, type = \"link\",\n                      binding = \"fit\", variances = list(linPred = TRUE))\nspamm_pred$se.fit <- sqrt(attr(spamm_pred, \"predVar\"))\nspamm_pred <- mutate(spamm_pred, pred = plogis(fit), \n                     lo = plogis(fit - 0.674 * se.fit),\n                     hi = plogis(fit + 0.674 * se.fit))\n\nFinally, we combine both sets of predictions as different rows of a pred_all dataset with bind_rows. The name of the dataset each prediction originates from (gam or spamm) will appear in the “model” column (argument .id). To simplify production of the next plot, we then use pivot_longer in the tidyr package to change the three columns “pred”, “lo” and “hi” to two columns, “stat” and “value” (pred_tall has thus three rows for every row in pred_all).\n\npred_all <- bind_rows(gam = gam_pred, spamm = spamm_pred, .id = \"model\")\n\nlibrary(tidyr)\npred_tall <- pivot_longer(pred_all, c(pred, lo, hi), names_to = \"stat\",\n                          values_to = \"value\")\n\nHaving done these steps, we can finally look at the prediction maps (mean, lower and upper bounds of the 50% confidence interval) with ggplot. The original data points are shown in red.\n\nggplot(pred_tall, aes(x = x, y = y)) +\n    geom_point(aes(color = value)) +\n    geom_point(data = gambia_agg, color = \"red\", size = 0) +\n    coord_fixed() +\n    facet_grid(stat~model) +\n    scale_color_viridis_c() +\n    theme_minimal()\n\n\n\n\nWhile both models agree that there is a higher prevalence near the eastern cluster of villages, the GAMM also estimates a higher prevalence at a few points (western edge and around the center) where there is no data. This is an artifact of the shape of the spline fit around the data points, since a spline is meant to fit a global, although nonlinear, trend. In contrast, the geostatistical model represents the spatial effect as local correlations and reverts to the overall mean prevalence when far from any data points, which is a safer assumption. This is one reason to choose a geostatistical / Gaussian process model in this case."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#bayesian-methods-for-glmms-with-gaussian-processes",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#bayesian-methods-for-glmms-with-gaussian-processes",
    "title": "Spatial Statistics in Ecology",
    "section": "Bayesian methods for GLMMs with Gaussian processes",
    "text": "Bayesian methods for GLMMs with Gaussian processes\nBayesian models provide a flexible framework to express models with complex dependence structure among the data, including spatial dependence. However, fitting a Gaussian process model with a fully Bayesian approach can be slow, due the need to compute a spatial covariance matrix between all point pairs at each iteration.\nThe INLA (integrated nested Laplace approximation) method performs an approximate calculation of the Bayesian posterior distribution, which makes it suitable for spatial regression problems. We do not cover it in this course, but I recommend the textbook by Paula Moraga (in the references section below) that provides worked examples of using INLA for various geostatistical and areal data models, in the context of epidemiology, including models with both space and time dependence. The book presents the same Gambia malaria data as an example of a geostatistical dataset, which inspired its use in this course."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#reference-1",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#reference-1",
    "title": "Spatial Statistics in Ecology",
    "section": "Reference",
    "text": "Reference\nMoraga, Paula (2019) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny. Chapman & Hall/CRC Biostatistics Series. Available online at https://www.paulamoraga.com/book-geospatial/."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#plan-du-cours",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#plan-du-cours",
    "title": "Spatial Statistics in Ecology",
    "section": "Plan du cours",
    "text": "Plan du cours\n\n\n\nJour\nSujets\n\n\n\n\n1\n• Introduction aux statistiques spatiales  • Analyse des patrons de points \n\n\n2\n• Corrélation spatiale d’une variable  • Modèles géostatistiques\n\n\n3\n• Données aréales  • Indice de Moran  • Modèles d’autorégression spatiale  • Analyse des données aréales dans R\n\n\n4\n• GLMM avec processus spatial gaussien  • GLMM avec autorégression spatiale"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#types-danalyses-spatiales",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#types-danalyses-spatiales",
    "title": "Spatial Statistics in Ecology",
    "section": "Types d’analyses spatiales",
    "text": "Types d’analyses spatiales\nDans le cadre de cette formation, nous discuterons de trois types d’analyses spatiales: l’analyse des patrons de points, les modèles géostatistiques et les modèles de données aréales.\nDans l’analyse des patrons de points, nous avons des données ponctuelles représentant la position d’individus ou d’événements dans une région d’étude et nous supposons que tous les individus ou événements ont été recensés dans cette région. Cette analyse s’intéresse à la distribution des positions des points eux-mêmes. Voici quelques questions typiques de l’analyse des patrons de points:\n\nLes points sont-ils disposés aléatoirement ou agglomérés?\nDeux types de points sont-ils disposés indépendamment?\n\nLes modèles géostatistiques visent à représenter la distribution spatiale de variables continues qui sont mesurés à certains points d’échantillonnage. Ils supposent que les mesures de ces variables à différents points sont corrélées en fonction de la distance entre ces points. Parmi les applications des modèles géostatistiques, notons le lissage des données spatiales (ex.: produire une carte d’une variable sur l’ensemble d’une région en fonction des mesures ponctuelles) et la prédiction de ces variables pour des points non-échantillonnés.\nLes données aréales sont des mesures prises non pas à des points, mais pour des régions de l’espace représentées par des polygones (ex.: divisions du territoire, cellules d’une grille). Les modèles représentant ces types de données définissent un réseau de voisinage reliant les régions et incluent une corrélation spatiale entre régions voisines."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#stationnarité-et-isotropie",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#stationnarité-et-isotropie",
    "title": "Spatial Statistics in Ecology",
    "section": "Stationnarité et isotropie",
    "text": "Stationnarité et isotropie\nPlusieurs analyses spatiales supposent que les variables sont stationnaires dans l’espace. Comme pour la stationnarité dans le domaine temporel, cette propriété signifie que les statistiques sommaires (moyenne, variance et corrélations entre mesures d’une variable) ne varient pas avec une translation dans l’espace. Par exemple, la corrélation spatiale entre deux points peut dépendre de la distance les séparant, mais pas de leur position absolue.\nEn particulier, il ne peut pas y avoir de tendance à grande échelle (souvent appelée gradient dans un contexte spatial), ou bien cette tendance doit être prise en compte afin de modéliser la corrélation spatiale des résidus.\nDans le cas de l’analyse des patrons de points, la stationnarité (aussi appelée homogénéité dans ce contexte) signifie que la densité des points ne suit pas de tendance à grande échelle.\nDans un modèle statistique isotropique, les corrélations spatiales entre les mesures à deux points dépendent seulement de la distance entre ces points, pas de la direction. Dans ce cas, les statistiques sommaires ne varient pas si on effectue une rotation dans l’espace."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#données-géoréférencées",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#données-géoréférencées",
    "title": "Spatial Statistics in Ecology",
    "section": "Données géoréférencées",
    "text": "Données géoréférencées\nLes études environnementales utilisent de plus en plus de données provenant de sources de données géospatiales, c’est-à-dire des variables mesurées sur une grande partie du globe (ex.: climat, télédétection). Le traitement de ces données requiert des concepts liés aux systèmes d’information géographique (SIG), qui ne sont pas couverts dans cet atelier, alors que nous nous concentrons sur les aspects statistiques de données variant dans l’espace.\nL’utilisation de données géospatiales ne signifie pas nécessairement qu’il faut avoir recours à des statistiques spatiales. Par exemple, il est courant d’extraire les valeurs de ces variables géographiques à des points d’étude pour expliquer une réponse biologique observée sur le terrain. Dans ce cas, l’utilisation de statistiques spatiales est seulement nécessaire en présence d’une corrélation spatiale dans les résidus, après avoir tenu compte de l’effet des prédicteurs."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#patron-de-points-et-processus-ponctuel",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#patron-de-points-et-processus-ponctuel",
    "title": "Spatial Statistics in Ecology",
    "section": "Patron de points et processus ponctuel",
    "text": "Patron de points et processus ponctuel\nUn patron de points (point pattern) décrit la position spatiale (le plus souvent en 2D) d’individus ou d’événements, représentés par des points, dans une aire d’étude donnée, souvent appelée la fenêtre d’observation.\nOn suppose que chaque point a une étendue spatiale négligeable par rapport aux distances entre les points. Des méthodes plus complexes existent pour traiter des patrons spatiaux d’objets qui ont une largeur non-néligeable, mais ce sujet dépasse la portée de cet atelier.\nUn processus ponctuel (point process) est un modèle statistique qui peut être utilisé pour simuler des patrons de points ou expliquer un patron de points observé."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#structure-spatiale-totalement-aléatoire",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#structure-spatiale-totalement-aléatoire",
    "title": "Spatial Statistics in Ecology",
    "section": "Structure spatiale totalement aléatoire",
    "text": "Structure spatiale totalement aléatoire\nUne structure spatiale totalement aléatoire (complete spatial randomness) est un des patrons les plus simples, qui sert de modèle nul pour évaluer les caractéristiques de patrons de points réels. Dans ce patron, la présence d’un point à une position donnée est indépendante de la présence de points dans un voisinage.\nLe processus créant ce patron est un processus de Poisson homogène. Selon ce modèle, le nombre de points dans toute région de superficie \\(A\\) suit une distribution de Poisson: \\(N(A) \\sim \\text{Pois}(\\lambda A)\\), où \\(\\lambda\\) est l’intensité du processus (i.e. la densité de points). \\(N\\) est indépendant entre deux régions disjointes, peu importe comment ces régions sont définies.\nDans le graphique ci-dessous, seul le patron à droite est totalement aléatoire. Le patron à gauche montre une agrégation des points (probabilité plus grande d’observer un point si on est à proximité d’un autre point), tandis que le patron du centre montre une répulsion (faible probabilité d’observer un point très près d’un autre)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#analyse-exploratoire-ou-inférentielle-pour-un-patron-de-points",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#analyse-exploratoire-ou-inférentielle-pour-un-patron-de-points",
    "title": "Spatial Statistics in Ecology",
    "section": "Analyse exploratoire ou inférentielle pour un patron de points",
    "text": "Analyse exploratoire ou inférentielle pour un patron de points\nPlusieurs statistiques sommaires sont utilisées pour décrire les caractéristiques un patron de points. La plus simple est l’intensité \\(\\lambda\\), qui comme mentionné plus haut représente la densité de points par unité de surface. Si le patron de points est hétérogène, l’intensité n’est pas constante, mais dépend de la position: \\(\\lambda(x, y)\\).\nPar rapport à l’intensité qui est une statistique dite de premier ordre, les statistiques de second ordre décrivent comment la probabilité de présence d’un point dans une région dépend de la présence d’autres points. L’indice \\(K\\) de Ripley présenté dans la prochaine section est un exemple de statistique sommaire de second ordre.\nLes inférences statistiques réalisées sur des patrons de points consistent habituellement à tester l’hypothèse que le patron de points correspond à un modèle nul donné, par exemple une structure spatiale totalement aléatoire, ou un modèle nul plus complexe. Même pour les modèles nuls les plus simples, nous connaissons rarement la distribution théorique pour une statistique sommaire du patron de points sous le modèle nul. Les tests d’hypothèses sur les patrons de points sont donc réalisés par simulation: on simule un grand nombre de patrons de points à partir du modèle nul et on compare la distribution des statistiques sommaires qui nous intéressent pour ces simulations à la valeur des statistiques pour le patron de points observé."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#indice-k-de-ripley",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#indice-k-de-ripley",
    "title": "Spatial Statistics in Ecology",
    "section": "Indice \\(K\\) de Ripley",
    "text": "Indice \\(K\\) de Ripley\nL’indice de Ripley \\(K(r)\\) est défini comme le nombre moyen de points se trouvant dans un cercle de rayon \\(r\\) donné autour d’un point du patron, normalisé par l’intensité \\(\\lambda\\).\nPour un patron totalement aléatoire, le nombre moyen de points dans un cercle de rayon \\(r\\) est \\(\\lambda \\pi r^2\\), donc en théorie \\(K(r) = \\pi r^2\\) pour ce modèle nul. Une valeur de \\(K(r)\\) supérieure signifie qu’il y a agrégation des points à l’échelle \\(r\\), tandis qu’une valeur inférieure signifie qu’il y a une répulsion.\nEn pratique, \\(K(r)\\) est estimé pour un patron de points donné par l’équation:\n\\[ K(r) = \\frac{A}{n(n-1)} \\sum_i \\sum_{j > i} I \\left( d_{ij} \\le r \\right) w_{ij}\\]\noù \\(A\\) est l’aire de la fenêtre d’observation et \\(n\\) est le nombre de points du patron, donc \\(n(n-1)\\) est le nombre de paires de points distinctes. On fait la somme pour toutes les paires de points de la fonction indicatrice \\(I\\), qui prend une valeur de 1 si la distance entre les points \\(i\\) et \\(j\\) est inférieure ou égale à \\(r\\). Finalement, le terme \\(w_{ij}\\) permet de donner un poids supplémentaire à certaines paires de points pour tenir compte des effets de bordure, tel que discuté dans la section suivante.\nPar exemple, les graphiques ci-dessous présentent la fonction estimée \\(K(r)\\) pour les patrons illustrés ci-dessus, pour des valeurs de \\(r\\) allant jusqu’à 1/4 de la largeur de la fenêtre. La courbe pointillée rouge indique la valeur théorique pour une structure spatiale totalement aléatoire et la zone grise est une “enveloppe” produite par 99 simulations de ce modèle nul. Le patron agrégé montre un excès de voisins jusqu’à \\(r = 0.25\\) et le patron avec répulsion montre un déficit significatif de voisins pour les petites valeurs de \\(r\\).\n\n\n\n\n\nOutre le \\(K\\), il existe d’autres statistiques pour décrire les propriétés de second ordre du patron, par exemple la distance moyenne entre un point et ses \\(N\\) plus proches voisins. Vous pouvez consulter le manuel de Wiegand et Moloney (2013) suggéré en référence pour en apprendre plus sur différentes statistiques sommaires des patrons de points."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effets-de-bordure",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effets-de-bordure",
    "title": "Spatial Statistics in Ecology",
    "section": "Effets de bordure",
    "text": "Effets de bordure\nDans le contexte de l’analyse de patrons de points, l’effet de bordure (“edge effect”) est dû au fait que nous avons une connaissance incomplète du voisinage des points près du bord de la fenêtre d’observation, ce qui peut induire un biais dans le calcul des statistiques comme le \\(K\\) de Ripley.\nDifférentes méthodes ont été développées pour corriger le biais dû aux effets de bordure. Selon la méthode de Ripley, la contribution d’un voisin \\(j\\) situé à une distance \\(r\\) d’un point \\(i\\) reçoit un poids \\(w_{ij} = 1/\\phi_i(r)\\), où \\(\\phi_i(r)\\) est la fraction du cercle de rayon \\(r\\) autour de \\(i\\) contenu dans la fenêtre d’observation. Par exemple, si 2/3 du cercle se trouve dans la fenêtre, ce voisin compte pour 3/2 voisins dans le calcul d’une statistique comme \\(K\\).\n\nLa méthode de Ripley est une des plus simples pour corriger les effets de bordure, mais n’est pas nécessairement la plus efficace; notamment, les poids plus grands donnés à certaines paires de points tend à accroître la variance du calcul de la statistique. D’autres méthodes de correction sont présentées dans les manuels spécialisés, comme celui de Wiegand et Moloney (2013) en référence."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exemple",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exemple",
    "title": "Spatial Statistics in Ecology",
    "section": "Exemple",
    "text": "Exemple\nPour cet exemple, nous utilisons le jeu de données semis_xy.csv, qui représente les coordonnées \\((x, y)\\) de semis de deux espèces (sp, B = bouleau et P = peuplier) dans une placette de 15 x 15 m.\n\nsemis <- read.csv(\"data/semis_xy.csv\")\nhead(semis)\n\n      x    y sp\n1 14.73 0.05  P\n2 14.72 1.71  P\n3 14.31 2.06  P\n4 14.16 2.64  P\n5 14.12 4.15  B\n6  9.88 4.08  B\n\n\nLe package spatstat permet d’effectuer des analyses de patrons de point dans R. La première étape consiste à transformer notre tableau de données en objet ppp (patron de points) avec la fonction du même nom. Dans cette fonction, nous spécifions quelles colonnes contiennent les coordonnées x et y ainsi que les marques (marks), qui seront ici les codes d’espèce. Il faut aussi spécifier une fenêtre d’observation (window) à l’aide de la fonction owin, à laquelle nous indiquons les limites de la placette en x et y.\n\nlibrary(spatstat)\n\nsemis <- ppp(x = semis$x, y = semis$y, marks = as.factor(semis$sp),\n             window = owin(xrange = c(0, 15), yrange = c(0, 15)))\nsemis\n\nMarked planar point pattern: 281 points\nMultitype, with levels = B, P \nwindow: rectangle = [0, 15] x [0, 15] units\n\n\nLes marques peuvent être numériques ou catégorielles. Notez que pour des marques catégorielles comme c’est le cas ici, il faut convertir explicitement la variable en facteur.\nLa fonction plot appliquée à un patron de points montre un diagramme du patron.\n\nplot(semis)\n\n\n\n\nLa fonction intensity calcule la densité des points de chaque espèce par unité de surface, ici en \\(m^2\\).\n\nintensity(semis)\n\n        B         P \n0.6666667 0.5822222 \n\n\nPour analyser d’abord séparément la distribution de chaque espèce, nous séparons le patron avec split. Puisque le patron contient des marques catégorielles, la séparation se fait automatiquement en fonction de la valeur des marques. Le résultat est une liste de deux patrons de points.\n\nsemis_split <- split(semis)\nplot(semis_split)\n\n\n\n\nLa fonction Kest calcule le \\(K\\) de Ripley pour une série de distances allant (par défaut) jusqu’à 1/4 de la largeur de la fenêtre. Ici, nous l’appliquons au premier patron (bouleau) en choisissant semis_split[[1]]. Notez que les doubles crochets sont nécessaires pour choisir un élément d’une liste dans R.\nL’argument correction = \"iso\" indique d’appliquer la méthode de Ripley pour corriger les effets de bordure.\n\nk <- Kest(semis_split[[1]], correction = \"iso\")\nplot(k)\n\n\n\n\nSelon ce graphique, il semble y avoir une excès de voisins à partir d’un rayon de 1 m. Pour vérifier s’il s’agit d’un écart significatif, nous produisons une enveloppe de simulation avec la fonction envelope. Le permier argument d’envelope est un patron de point auquel les simulations seront comparées, le deuxième une fonction à calculer (ici, Kest) pour chaque patron simulé, puis on y ajoute les arguments de la fonction Kest (ici, seulement correction).\n\nplot(envelope(semis_split[[1]], Kest, correction = \"iso\"))\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\n\n\nTel qu’indiqué par le message, cette fonction effectue par défaut 99 simulations de l’hypothèse nulle correspondant à une structure spatiale totalement aléatoire (CSR, pour complete spatial randomness).\nLa courbe observée sort de l’enveloppe des 99 simulations près de \\(r = 2\\). Il faut être prudent de ne pas interpréter trop rapidement un résultat sortant de l’enveloppe. Même s’il y a environ une probabilité de 1% d’obtenir un résultat plus extrême selon l’hypothèse nulle à une distance donnée, l’enveloppe est calculée pour un grand nombre de valeurs de la distance et nous n’effectuons pas de correction pour les comparaisons multiples. Ainsi, un écart significatif pour une très petite plage de valeurs de \\(r\\) peut être simplement dû au hasard.\n\nExercice 1\nEn regardant le graphique du deuxième patron de points (semis de peuplier), pouvez-vous prédire où se situera le \\(K\\) de Ripley par rapport à l’hypothèse nulle d’une structure spatiale totalement aléatoire? Vérifiez votre prédiction en calculant le \\(K\\) de Ripley pour ce patron de points dans R."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effet-de-lhétérogénéité",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#effet-de-lhétérogénéité",
    "title": "Spatial Statistics in Ecology",
    "section": "Effet de l’hétérogénéité",
    "text": "Effet de l’hétérogénéité\nLe graphique ci-dessous illustre un patron de points hétérogène, c’est-à-dire qu’il présente un gradient d’intensité (plus de points à gauche qu’à droite).\n\n\n\n\n\nUn gradient de densité peut être confondu avec une agrégation des points, comme on peut voir sur le graphique du \\(K\\) de Ripley correspondant. En théorie, il s’agit de deux processus différents:\n\nHétérogénéité: La densité de points varie dans la région d’étude, par exemple dû au fait que certaines conditions locales sont plus propices à la présence de l’espèce étudiée.\nAgrégation: La densité moyenne des points est homogène, mais la présence d’un point augmente la présence d’autre points dans son voisinage, par exemple en raison d’interactions positives entre les individus.\n\nCependant, il peut être difficile de différencier les deux en pratique, surtout que certains patrons peuvent être à la fois hétérogènes et agrégés.\nPrenons l’exemple des semis de peuplier de l’exercice précédent. La fonction density appliquée à un patron de points effectue une estimation par noyau (kernel density estimation) de la densité des semis à travers la placette. Par défaut, cette fonction utilise un noyau gaussien avec un écart-type sigma spécifié dans la fonction, qui détermine l’échelle à laquelle les fluctuations de densité sont “lissées”. Ici, nous utilisons une valeur de 2 m pour sigma et nous représentons d’abord la densité estimée avec plot, avant d’y superposer les points (add = TRUE signifie que les points sont ajoutés au graphique existant plutôt que de créer un nouveau graphique).\n\ndens_p <- density(semis_split[[2]], sigma = 2)\nplot(dens_p)\nplot(semis_split[[2]], add = TRUE)\n\n\n\n\nPour mesurer l’agrégation ou la répulsion des points d’un patron hétérogène, nous devons utilisé la version non-homogène de la statistique \\(K\\) (Kinhom dans spatstat). Cette statistique est toujours égale au nombre moyen de voisins dans un rayon \\(r\\) d’un point du patron, mais plutôt que de normaliser ce nombre par l’intensité globale du patron, il est normalisé par l’estimation locale de la densité de points. Comme ci-dessus, nous spécifions sigma = 2 pour contrôler le niveau de lissage de l’estimation de la densité variable.\n\nplot(Kinhom(semis_split[[2]], sigma = 2, correction = \"iso\"))\n\n\n\n\nEn tenant compte de l’hétérogénéité du patron à une échelle sigma de 2 m, il semble donc y avoir un déficit de voisins à partir d’environ 1.5 m des points du patron. Il reste à voir si cette déviation est significative.\nComme précédemment, nous utilisons envelope pour simuler la statistique Kinhom sous le modèle nul. Cependant, ici le modèle nul n’est pas un processus de Poisson homogène (structure spatiale totalement aléatoire). Il s’agit plutôt d’un processus de Poisson hétérogène simulé par la fonction rpoispp(dens_p), c’est-à-dire que les points sont indépendants les uns des autres, mais leur densité est hétérogène et donnée par dens_p. L’argument simulate de la fonction envelope permet de spécifier une fonction utilisée pour les simulations sous le modèle nul; cette fonction doit avoir un argument, ici x, même s’il n’est pas utilisé.\nFinalement, en plus des arguments nécessaires pour Kinhom, soit sigma et correction, nous spécifions aussi nsim = 199 pour réaliser 199 simulations et nrank = 5 pour éliminer les 5 résultats les plus extrêmes de chaque côté de l’enveloppe, donc les 10 plus extrêmes sur 199, pour réaliser un intervalle contenant environ 95% de la probabilité sous l’hypothèse nulle.\n\nkhet_p <- envelope(semis_split[[2]], Kinhom, sigma = 2, correction = \"iso\",\n                   nsim = 199, nrank = 5, simulate = function(x) rpoispp(dens_p))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(khet_p)\n\n\n\n\nNote: Pour un test d’hypothèse basé sur des simulations d’une hypothèse nulle, la valeur \\(p\\) est estimée par \\((m + 1)/(n + 1)\\), où \\(n\\) est le nombre de simulations et \\(m\\) est le nombre de simulations où la valeur de la statistique est plus extrême que celle des données observées. C’est pour cette raison qu’on choisit un nombre de simulations comme 99, 199, etc.\n\nExercice 2\nRépétez l’estimation de la densité hétérogène et le calcul de Kinhom avec un écart-type sigma de 5 plutôt que 2. Comment le niveau de lissage pour la densité influence-t-il les conclusions?\nPour différencier une variation de densité des points et d’une interaction (agrégation ou répulsion) entre ces points avec ce type d’analyse, il faut généralement supposer que les deux processus opèrent à différentes échelles. Typiquement, nous pouvons tester si les points sont agrégés à petite échelle après avoir tenu compte d’une variation de la densité à une échelle plus grande."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#relation-entre-deux-patrons-de-points",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#relation-entre-deux-patrons-de-points",
    "title": "Spatial Statistics in Ecology",
    "section": "Relation entre deux patrons de points",
    "text": "Relation entre deux patrons de points\nConsidérons un cas où nous avons deux patrons de points, par exemple la position des arbres de deux espèces dans une parcelle (points oranges et verts dans le graphique ci-dessous). Chacun des deux patrons peut présenter ou non des agrégations de points.\n\n\n\n\n\nSans égard à cette agrégation au niveau de l’espèce, nous voulons déterminer si les deux espèces sont disposées indépendamment. Autrement dit, la probabilité d’observer un arbre d’une espèce dépend-elle de la présence d’un arbre de l’autre espèce à une distance donnée?\nLa version bivariée du \\(K\\) de Ripley permet de répondre à cette question. Pour deux patrons désignés 1 et 2, l’indice \\(K_{12}(r)\\) calcule le nombre moyen de points du patron 2 dans un rayon \\(r\\) autour d’un point du patron 1, normalisé par la densité du patron 2.\nEn théorie, cet indice est symétrique, donc \\(K_{12}(r) = K_{21}(r)\\) et le résultat serait le même si on choisit les points du patron 1 ou 2 comme points “focaux” pour l’analyse. Cependant, l’estimation des deux quantités pour un patron observé peut différer, notamment en raison des effets de bord. La variabilité peut aussi être différente pour \\(K_{12}\\) et \\(K_{21}\\) entre les simulations d’un modèle nul, donc le test de l’hypothèse nulle peut avoir une puissance différente selon le choix de l’espèce focale.\nLe choix d’un modèle nul approprié est important ici. Afin de déterminer s’il existe une attraction ou une répulsion significative entre les deux patrons, il faut déplacer aléatoirement la position d’un des patrons relative à celle de l’autre patron, tout en conservant la structure spatiale de chaque patron pris isolément.\nUne des façons d’effectuer cette randomisation consiste à décaler l’un des deux patrons horizontalement et/ou verticalement d’une distance aléatoire. La partie du patron qui “sort” d’un côté de la fenêtre est rattachée de l’autre côté. Cette méthode s’appelle une translation toroïdale (toroidal shift), car en connectant le haut et le bas ainsi que la gauche et la droite d’une surface rectangulaire, on obtient la forme d’un tore (un “beigne” en trois dimensions).\n\n\n\n\n\nLe graphique ci-dessus illustre une translation du patron vert vers la droite, tandis que le patron orange reste au même endroit. Les points verts dans la zone ombragée sont ramenés de l’autre côté. Notez que si cette méthode préserve de façon générale la structure de chaque patron tout en randomisant leur position relative, elle peut comporter certains inconvénients, comme de diviser des amas de points qui se trouvent près du point de coupure.\nVérifions maintenant s’il y a une dépendance entre la position des deux espèces (bouleau et peuplier) dans notre placette. La fonction Kcross calcule l’indice bivarié \\(K_{ij}\\), il faut spécifier quel type de point est considéré comme l’espèce focale \\(i\\) et l’espèce voisine \\(j\\).\n\nplot(Kcross(semis, i = \"P\", j = \"B\", correction = \"iso\"))\n\n\n\n\nIci, le \\(K\\) observé est inférieur à la valeur théorique, indiquant une répulsion possible des deux patrons.\nPour déterminer l’enveloppe du \\(K\\) selon l’hypothèse nulle d’indépendance des deux patrons, nous devons spécifier que les simulations doivent être basées sur une translation des patrons. Nous indiquons que les simulations doivent utiliser la fonction rshift (translation aléatoire) avec l’argument simulate = function(x) rshift(x, which = \"B\"); ici, l’argument x de simulate correspond au patron de points original et l’argument which indique quel type de points subit la translation. Comme pour le cas précédent, il faut répéter dans la fonction envelope les arguments nécessaires pour Kcross, soit i, j et correction.\n\nplot(envelope(semis, Kcross, i = \"P\", j = \"B\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = function(x) rshift(x, which = \"B\")))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nIci, la courbe observée se situe totalement dans l’enveloppe, donc nous ne rejetons pas l’hypothèse nulle d’indépendance des deux patrons.\n\nQuestions\n\nQuelle raison pourrait justifier ici notre choix d’effectuer la translation des points du bouleau plutôt que du peuplier?\nEst-ce que les simulations générées par translation aléatoire constitueraient un bon modèle nul si les deux patrons étaient hétérogènes?"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#patrons-de-points-marqués",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#patrons-de-points-marqués",
    "title": "Spatial Statistics in Ecology",
    "section": "Patrons de points marqués",
    "text": "Patrons de points marqués\nLe jeu de données fir.csv contient les coordonnées \\((x, y)\\) de 822 sapins dans une placette d’un hectare et leur statut (A = vivant, D = mort) suivant une épidémie de tordeuse des bourgeons de l’épinette.\n\nfir <- read.csv(\"data/fir.csv\")\nhead(fir)\n\n      x     y status\n1 31.50  1.00      A\n2 85.25 30.75      D\n3 83.50 38.50      A\n4 84.00 37.75      A\n5 83.00 33.25      A\n6 33.25  0.25      A\n\n\n\nfir <- ppp(x = fir$x, y = fir$y, marks = as.factor(fir$status),\n           window = owin(xrange = c(0, 100), yrange = c(0, 100)))\nplot(fir)\n\n\n\n\nSupposons que nous voulons vérifier si la mortalité des sapins est indépendante ou corrélée entre arbres rapprochés. En quoi cette question diffère-t-elle de l’exemple précédent où nous voulions savoir si la position des points de deux espèces était indépendante?\nDans l’exemple précédent, l’indépendance ou l’interaction entre les espèces référait à la formation du patron lui-même (que des semis d’une espèce s’établissent ou non à proximité de ceux de l’autre espèce). Ici, la caractéristique qui nous intéresse (survie des sapins) est postérieure à l’établissement du patron, en supposant que tous ces arbres étaient vivants d’abord et que certains sont morts suite à l’épidémie. Donc nous prenons la position des arbres comme fixe et nous voulons savoir si la distribution des statuts (mort, vivant) entre ces arbres est aléatoire ou présente un patron spatial.\nDans le manuel de Wiegand et Moloney, la première situation (établissement de semis de deux espèces) est appelé patron bivarié, donc il s’agit vraiment de deux patrons qui interagissent, tandis que la deuxième est un seul patron avec une marque qualitative. Le package spatstat dans R ne fait pas de différences entre les deux au niveau de la définition du patron (les types de points sont toujours représentés par l’argument marks), mais les méthodes d’analyse appliquées aux deux questions diffèrent.\nDans le cas d’un patron avec une marque qualitative, nous pouvons définir une fonction de connexion de marques (mark connection function) \\(p_{ij}(r)\\). Pour deux points séparés par une distance \\(r\\), cette fonction donne la probabilité que le premier point porte la marque \\(i\\) et le deuxième la marque \\(j\\). Selon l’hypothèse nulle où les marques sont indépendantes, cette probabilité est égale au produit des proportions de chaque marque dans le patron entier, \\(p_{ij}(r) = p_i p_j\\) indépendamment de \\(r\\).\nDans spatstat, la fonction de connexion de marques est calculée avec la fonction markconnect, où il faut spécifier les marques \\(i\\) et \\(j\\) ainsi que le type de correction des effets de bord. Dans notre exemple, nous voyons que deux points rapprochés ont moins de chance d’avoir une statut différent (A et D) que prévu selon l’hypothèse de distribution aléatoire et indépendante des marques (ligne rouge pointillée).\n\nplot(markconnect(fir, i = \"A\", j = \"D\", correction = \"iso\"))\n\n\n\n\nDans ce graphique, les ondulations dans la fonction sont dues à l’erreur d’estimation d’une fonction continue de \\(r\\) à partir d’un nombre limité de paires de points discrètes.\nPour simuler le modèle nul dans ce cas-ci, nous utilisons la fonction rlabel qui réassigne aléatoirement les marques parmi les points du patron, en maintenant la position des points.\n\nplot(envelope(fir, markconnect, i = \"A\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nNotez que puisque la fonction rlabel a un seul argument obligatoire correspondant au patron de points original, il n’était pas nécessaire de spécifier au long: simulate = function(x) rlabel(x).\nVoici les résultats pour les paires d’arbres du même statut A ou D:\n\npar(mfrow = c(1, 2))\nplot(envelope(fir, markconnect, i = \"A\", j = \"A\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(envelope(fir, markconnect, i = \"D\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36.38.40\n.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74.76.78.80\n.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114.116.118.120\n.122.124.126.128.130.132.134.136.138.140.142.144.146.148.150.152.154.156.158.160\n.162.164.166.168.170.172.174.176.178.180.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\n\n\n\nIl semble donc que la mortalité des sapins due à cette épidémie est agrégée spatialement, puisque les arbres situés à proximité l’un de l’autre ont une plus grande probabilité de partager le même statut que prévu par l’hypothèse nulle."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#références",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#références",
    "title": "Spatial Statistics in Ecology",
    "section": "Références",
    "text": "Références\nFortin, M.-J. et Dale, M.R.T. (2005) Spatial Analysis: A Guide for Ecologists. Cambridge University Press: Cambridge, UK.\nWiegand, T. et Moloney, K.A. (2013) Handbook of Spatial Point-Pattern Analysis in Ecology, CRC Press.\nLe jeu de données du dernier exemple est tiré des données de la Forêt d’enseignement et de recherche du Lac Duparquet (FERLD), disponibles sur Dryad en suivant ce lien."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#dépendance-intrinsèque-ou-induite",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#dépendance-intrinsèque-ou-induite",
    "title": "Spatial Statistics in Ecology",
    "section": "Dépendance intrinsèque ou induite",
    "text": "Dépendance intrinsèque ou induite\nIl existe deux types fondamentaux de dépendance spatiale sur une variable mesurée \\(y\\): une dépendance intrinsèque à \\(y\\), ou une dépendance induite par des variables externes influençant \\(y\\), qui sont elles-mêmes corrélées dans l’espace.\nPar exemple, supposons que l’abondance d’une espèce soit corrélée entre deux sites rapprochés:\n\ncette dépendance spatiale peut être induite si elle est due à une corrélation spatiale des facteurs d’habitat qui favorisent ou défavorisent l’espèce;\nou elle peut être intrinsèque si elle est due à la dispersion d’individus entre sites rapprochés.\n\nDans plusieurs cas, les deux types de dépendance affectent une variable donnée.\nSi la dépendance est simplement induite et que les variables externes qui en sont la cause sont incluses dans le modèle expliquant \\(y\\), alors les résidus du modèle seront indépendants et nous pouvons utiliser toutes les méthodes déjà vues qui ignorent la dépendance spatiale.\nCependant, si la dépendance est intrinsèque ou due à des influences externes non-mesurées, alors il faudra tenir compte de la dépendance spatiale des résidus dans le modèle."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#différentes-façons-de-modéliser-les-effets-spatiaux",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#différentes-façons-de-modéliser-les-effets-spatiaux",
    "title": "Spatial Statistics in Ecology",
    "section": "Différentes façons de modéliser les effets spatiaux",
    "text": "Différentes façons de modéliser les effets spatiaux\nDans cette formation, nous modéliserons directement les corrélations spatiales de nos données. Il est utile de comparer cette approche à d’autres façons d’inclure des aspects spatiaux dans un modèle statistique.\nD’abord, nous pourrions inclure des prédicteurs dans le modèle qui représentent la position (ex.: longitude, latitude). De tels prédicteurs peuvent être utiles pour détecter une tendance ou un gradient systématique à grande échelle, que cette tendance soit linéaire ou non (par exemple, avec un modèle additif généralisé).\nEn contraste à cette approche, les modèles que nous verrons maintenant servent à modéliser une corrélation spatiale dans les fluctuations aléatoires d’une variable (i.e., dans les résidus après avoir enlevé tout effet systématique).\nLes modèles mixtes utilisent des effets aléatoires pour représenter la non-indépendance de données sur la base de leur groupement, c’est-à-dire qu’après avoir tenu compte des effets fixes systématiques, les données d’un même groupe sont plus semblables (leur variation résiduelle est corrélée) par rapport aux données de groupes différents. Ces groupes étaient parfois définis selon des critères spatiaux (observations regroupées en sites).\nCependant, dans un contexte d’effet aléatoire de groupe, tous les groupes sont aussi différents les uns des autres, ex.: deux sites à 100 km l’un de l’autre ne sont pas plus ou moins semblables que deux sites distants de 2 km.\nLes méthodes que nous verrons ici et dans les prochains parties de la formation nous permettent donc ce modéliser la non-indépendance sur une échelle continue (plus proche = plus corrélé) plutôt que seulement discrète (hiérarchie de groupements)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogramme",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogramme",
    "title": "Spatial Statistics in Ecology",
    "section": "Variogramme",
    "text": "Variogramme\nUn aspect central de la géostatistique est l’estimation du variogramme \\(\\gamma_z\\) de la variable \\(z\\). Le variogramme est égal à la moitié de l’écart carré moyen entre les valeurs de \\(z\\) pour deux points \\((x_i, y_i)\\) et \\((x_j, y_j)\\) séparés par une distance \\(h\\).\n\\[\\gamma_z(h) = \\frac{1}{2} \\text{E} \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h}\\]\nDans cette équation, la fonction \\(\\text{E}\\) avec l’indice \\(d_{ij}=h\\) désigne l’espérance statistique (autrement dit, la moyenne) de l’écart au carré entre les valeurs de \\(z\\) pour les points séparés par une distance \\(h\\).\nSi on préfère exprimer l’autocorrélation \\(\\rho_z(h)\\) entre mesures de \\(z\\) séparées par une distance \\(h\\), celle-ci est reliée au variogramme par l’équation:\n\\[\\gamma_z = \\sigma_z^2(1 - \\rho_z)\\] ,\noù \\(\\sigma_z^2\\) est la variance globale de \\(z\\).\nNotez que \\(\\gamma_z = \\sigma_z^2\\) si nous sommes à une distance où les mesures de \\(z\\) sont indépendantes, donc \\(\\rho_z = 0\\). Dans ce cas, on voit bien que \\(\\gamma_z\\) s’apparente à une variance, même s’il est parfois appelé “semivariogramme” ou “semivariance” en raison du facteur 1/2 dans l’équation ci-dessus."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèles-théoriques-du-variogramme",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèles-théoriques-du-variogramme",
    "title": "Spatial Statistics in Ecology",
    "section": "Modèles théoriques du variogramme",
    "text": "Modèles théoriques du variogramme\nPlusieurs modèles paramétriques ont été proposés pour représenter la corrélation spatiale en fonction de la distance entre points d’échantillonnage. Considérons d’abord une corrélation qui diminue de façon exponentielle:\n\\[\\rho_z(h) = e^{-h/r}\\]\nIci, \\(\\rho_z = 1\\) pour \\(h = 0\\) et la corréaltion est multipliée par \\(1/e \\approx 0.37\\) pour chaque augmentation de \\(r\\) de la distance. Dans ce contexte, \\(r\\) se nomme la portée (range) de la corrélation.\nÀ partir de l’équation ci-dessus, nous pouvons calculer le variogramme correspondant.\n\\[\\gamma_z(h) = \\sigma_z^2 (1 - e^{-h/r})\\]\nVoici une représentation graphique de ce variogramme.\n\n\n\n\n\nEn raison de la fonction exponentielle, la valeur de \\(\\gamma\\) à des grandes distances s’approche de la variance globale \\(\\sigma_z^2\\) sans exactement l’atteindre. Cette asymptote est appelée palier (sill) dans le contexte géostatistique et représentée par le symbole \\(s\\).\nFinalement, il n’est parfois pas réaliste de supposer une corrélation parfaite lorsque la distance tend vers 0, en raison d’une variation possible de \\(z\\) à très petite échelle. On peut ajouter au modèle un effet de pépite (nugget), noté \\(n\\), pour que \\(\\gamma\\) s’approche de \\(n\\) (plutôt que 0) si \\(h\\) tend vers 0. Le terme pépite provient de l’origine minière de ces techniques, où une pépite d’un minerai pourrait être la source d’une variation abrupte de la concentration à petite échelle.\nEn ajoutant l’effet de pépite, le reste du variogramme est “compressé” pour conserver le même palier, ce qui résulte en l’équation suivante.\n\\[\\gamma_z(h) = n + (s - n) (1 - e^{-h/r})\\]\nDans le package gstat que nous utiliserons ci-dessous, le terme \\((s - n)\\) est le palier partiel (partial sill, ou psill) pour la partie exponentielle.\n\n\n\n\n\nEn plus du modèle exponentiel, deux autres modèles théoriques courants pour le variogramme sont le modèle gaussien (où la corrélation suit une courbe demi-normale), ainsi que le modèle sphérique (où le variogramme augmente de façon linéaire au départ pour ensuite courber et atteindre le palier à une distance égale à sa portée \\(r\\)). Le modèle sphérique permet donc à la corrélation d’être exactement 0 à grande distance, plutôt que de s’approcher graduellement de zéro dans le cas des autres modèles.\n\n\n\n\n\n\n\n\nModèle\n\\(\\rho(h)\\)\n\\(\\gamma(h)\\)\n\n\n\n\nExponentiel\n\\(\\exp\\left(-\\frac{h}{r}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h}{r}\\right)\\right)\\)\n\n\nGaussien\n\\(\\exp\\left(-\\frac{h^2}{r^2}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h^2}{r^2}\\right)\\right)\\)\n\n\nSphérique \\((h < r)\\) *\n\\(1 - \\frac{3}{2}\\frac{h}{r} + \\frac{1}{2}\\frac{h^3}{r^3}\\)\n\\(s \\left(\\frac{3}{2}\\frac{h}{r} - \\frac{1}{2}\\frac{h^3}{r^3} \\right)\\)\n\n\n\n* Pour le modèle sphérique, \\(\\rho = 0\\) et \\(\\gamma = s\\) si \\(h \\ge r\\)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogramme-empirique",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#variogramme-empirique",
    "title": "Spatial Statistics in Ecology",
    "section": "Variogramme empirique",
    "text": "Variogramme empirique\nPour estimer \\(\\gamma_z(h)\\) à partir de données empiriques, nous devons définir des classes de distance, donc grouper différentes distances dans une marge \\(\\pm \\delta\\) autour d’une distance \\(h\\), puis calculer l’écart-carré moyen pour les paires de points dans cette classe de distance.\n\\[\\hat{\\gamma_z}(h) = \\frac{1}{2 N_{\\text{paires}}} \\sum \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h \\pm \\delta}\\]\nNous verrons dans la partie suivante comment estimer un variogramme dans R."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèle-de-régression-avec-corrélation-spatiale",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèle-de-régression-avec-corrélation-spatiale",
    "title": "Spatial Statistics in Ecology",
    "section": "Modèle de régression avec corrélation spatiale",
    "text": "Modèle de régression avec corrélation spatiale\nL’équation suivante représente une régression linéaire multiple incluant une corrélation spatiale résiduelle:\n\\[v = \\beta_0 + \\sum_i \\beta_i u_i + z + \\epsilon\\]\nIci, \\(v\\) désigne la variable réponse et \\(u\\) les prédicteurs, pour ne pas confondre avec les coordonnées spatiales \\(x\\) et \\(y\\).\nEn plus du résidu \\(\\epsilon\\) qui est indépendant entre les observations, le modèle inclut un terme \\(z\\) qui représente la portion spatialement corrélée de la variance résiduelle.\nVoici une suggestions d’étapes à suivre pour appliquer ce type de modèle:\n\nAjuster le modèle de régression sans corrélation spatiale.\nVérifier la présence de corrélation spatiale à partir du variogramme empirique des résidus.\nAjuster un ou plusieurs modèles de régression avec corrélation spatiale et choisir celui qui montre le meilleur ajustement aux données."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#régression-avec-corrélation-spatiale",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#régression-avec-corrélation-spatiale",
    "title": "Spatial Statistics in Ecology",
    "section": "Régression avec corrélation spatiale",
    "text": "Régression avec corrélation spatiale\nNous avons vu ci-dessus que le package gstat permet d’estimer le variogramme des résidus d’un modèle linéaire. Dans notre exemple, la concentration de magnésium était modélisée en fonction du pH, avec des résidus spatialement corrélés.\nUn autre outil pour ajuster ce même type de modèle est la fonction gls du package nlme, qui est inclus avec l’installation de R.\nCette fonction applique la méthode des moindres carrés généralisés (generalized least squares) pour ajuster des modèles de régression linéaire lorsque les résidus ne sont pas indépendants ou lorsque la variance résiduelle n’est pas la même pour toutes les observations. Comme les estimés des coefficients dépendent de l’estimé des corrélations entre les résidus et que ces derniers dépendent eux-mêmes des coefficients, le modèle est ajusté par un algorithme itératif:\n\nOn ajuste un modèle de régression linéaire classique (sans corrélation) pour obtenir des résidus.\nOn ajuste le modèle de corrélation spatiale (variogramme) avec ses résidus.\nOn ré-estime les coefficients de la régression en tenant compte maintenant des corrélations.\n\nLes étapes 2 et 3 sont répétées jusqu’à ce que les estimés soient stables à une précision voulue.\nVoici l’application de cette méthode au même modèle pour la concentration de magnésium dans le jeu de données oxford. Dans l’argument correlation de gls, nous spécifions un modèle de corrélation exponentielle en fonction de nos coordonnées spatiales et indiquons que nous voulons aussi estimer un effet de pépite.\nEn plus de la corrélation exponentielle corExp, la fonction gls peut aussi estimer un modèle gaussien (corGaus) ou sphérique (corSpher).\n\nlibrary(nlme)\ngls_mg <- gls(MG1 ~ PH1, oxford, \n              correlation = corExp(form = ~ XCOORD + YCOORD, nugget = TRUE))\nsummary(gls_mg)\n\nGeneralized least squares fit by REML\n  Model: MG1 ~ PH1 \n  Data: oxford \n      AIC      BIC   logLik\n  1278.65 1292.751 -634.325\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~XCOORD + YCOORD \n Parameter estimate(s):\n      range      nugget \n478.0322964   0.2944753 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 391.1387  50.42343  7.757084       0\nPH1         -41.0836   6.15662 -6.673079       0\n\n Correlation: \n    (Intr)\nPH1 -0.891\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.1846957 -0.6684520 -0.3687813  0.4627580  3.1918604 \n\nResidual standard error: 53.8233 \nDegrees of freedom: 126 total; 124 residual\n\n\nPour comparer ce résultat au variogramme ajusté ci-dessus, il faut transformer les paramètres donnés par gls. La portée (range) a le même sens dans les deux cas et correspond à 478 m pour le résultat de gls. La variance globale des résidus est le carré de Residual standard error. L’effet de pépite ici (0.294) est exprimé comme fraction de cette variance. Finalement, pour obtenir le palier partiel de la partie exponentielle, il faut soustraire l’effet de pépite de la variance totale.\nAprès avoir réalisé ces calculs, nous pouvons donner ces paramètres à la fonction vgm de gstat pour superposer ce variogramme estimé par gls à notre variogramme des résidus du modèle linéaire classique.\n\ngls_range <- 478\ngls_var <- 53.823^2\ngls_nugget <- 0.294 * gls_var\ngls_psill <- gls_var - gls_nugget\n\ngls_vgm <- vgm(\"Exp\", psill = gls_psill, range = gls_range, nugget = gls_nugget)\n\nplot(var_mg, gls_vgm, col = \"black\", ylim = c(0, 4000))\n\n\n\n\nEst-ce que le modèle est moins bien ajusté aux données ici? En fait, ce variogramme empirique représenté par les points avait été obtenu à partir des résidus du modèle linéaire ignorant la corrélation spatiale, donc c’est un estimé biaisé des corrélations spatiales réelles. La méthode est quand même adéquate pour vérifier rapidement s’il y a présence de corrélations spatiales. Toutefois, pour ajuster simultanément les coefficients de la régression et les paramètres de corrélation spatiale, l’approche des moindres carrés généralisés (GLS) est préférable et produira des estimés plus justes.\nFinalement, notez que le résultat du modèle gls donne aussi l’AIC, que nous pouvons utiliser pour comparer l’ajustement de différents modèles (avec différents prédicteurs ou différentes formes de corrélation spatiale)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercice",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercice",
    "title": "Spatial Statistics in Ecology",
    "section": "Exercice",
    "text": "Exercice\nLe fichier bryo_belg.csv est adapté des données de l’étude:\n\nNeyens, T., Diggle, P.J., Faes, C., Beenaerts, N., Artois, T. et Giorgi, E. (2019) Mapping species richness using opportunistic samples: a case study on ground-floor bryophyte species richness in the Belgian province of Limburg. Scientific Reports 9, 19122. https://doi.org/10.1038/s41598-019-55593-x\n\nCe tableau de données indique la richesse spécifique des bryophytes au sol (richness) pour différents points d’échantillonnage de la province belge de Limbourg, avec leur position (x, y) en km, en plus de l’information sur la proportion de forêts (forest) et de milieux humides (wetland) dans une cellule de 1 km\\(^2\\) contenant le point d’échantillonnage.\n\nbryo_belg <- read.csv(\"data/bryo_belg.csv\")\nhead(bryo_belg)\n\n  richness    forest   wetland        x        y\n1        9 0.2556721 0.5036614 228.9516 220.8869\n2        6 0.6449114 0.1172068 227.6714 219.8613\n3        5 0.5039905 0.6327003 228.8252 220.1073\n4        3 0.5987329 0.2432942 229.2775 218.9035\n5        2 0.7600775 0.1163538 209.2435 215.2414\n6       10 0.6865434 0.0000000 210.4142 216.5579\n\n\nPour cet exercice, nous utiliserons la racine carrée de la richesse spécifique comme variable réponse. La transformation racine carrée permet souvent d’homogénéiser la variance des données de comptage afin d’y appliquer une régression linéaire.\n\nAjustez un modèle linéaire de la richesse spécifique transformée en fonction de la fraction de forêt et de milieux humides, sans tenir compte des corrélations spatiales. Quel est l’effet des deux prédicteurs selon ce modèle?\nCalculez le variogramme empirique des résidus du modèle en (a). Semble-t-il y avoir une corrélation spatiale entre les points?\n\nNote: L’argument cutoff de la fonction variogram spécifie la distance maximale à laquelle le variogramme est calculé. Vous pouvez ajuster manuellement cette valeur pour bien voir le palier.\n\nRé-ajustez le modèle linéaire en (a) avec la fonction gls du package nlme, en essayant différents types de corrélations spatiales (exponentielle, gaussienne, sphérique). Comparez les modèles (incluant celui sans corrélation spatiale) avec l’AIC.\nQuel est l’effet de la fraction de forêts et de milieux humides selon le modèle en (c)? Expliquez les différences entre les conclusions de ce modèle et du modèle en (a)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#autorégression-conditionnelle-car",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#autorégression-conditionnelle-car",
    "title": "Spatial Statistics in Ecology",
    "section": "Autorégression conditionnelle (CAR)",
    "text": "Autorégression conditionnelle (CAR)\nDans le modèle d’autorégression conditionnelle, la valeur de \\(z_i\\) pour la région \\(i\\) suit une distribution normale: sa moyenne dépend de la valeur \\(z_j\\) des régions voisines, multipliée par le poids \\(w_{ij}\\) et un coefficient de corrélation \\(\\rho\\); son écart-type \\(\\sigma_{z_i}\\) peut varier d’une région à l’autre.\n\\[z_i \\sim \\text{N}\\left(\\sum_j \\rho w_{ij} z_j,\\sigma_{z_i} \\right)\\]\nDans ce modèle, si \\(w_{ij}\\) est une matrice binaire (0 pour les non-voisins, 1 pour les voisins), alors \\(\\rho\\) est le coefficient de corrélation partielle entre régions voisines. Cela est semblable à un modèle autorégressif d’ordre 1 dans le contexte de séries temporelles, où le coefficient d’autorégression indique la corrélation partielle."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#autorégression-simultanée-sar",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#autorégression-simultanée-sar",
    "title": "Spatial Statistics in Ecology",
    "section": "Autorégression simultanée (SAR)",
    "text": "Autorégression simultanée (SAR)\nDans le modèle d’autorégression simultanée, la valeur de \\(z_i\\) est donnée directement par la somme de contributions des valeurs voisines \\(z_j\\), multipliées par \\(\\rho w_{ij}\\), avec un résidu indépendant \\(\\nu_i\\) d’écart-type \\(\\sigma_z\\).\n\\[z_i = \\sum_j \\rho w_{ij} z_j + \\nu_i\\]\nÀ première vue, cela ressemble à un modèle autorégressif temporel. Il existe cependant une différence conceptuelle importante. Pour les modèles temporels, l’influence causale est dirigée dans une seule direction: \\(v(t-2)\\) affecte \\(v(t-1)\\) qui affecte ensuite \\(v(t)\\). Pour un modèle spatial, chaque \\(z_j\\) qui affecte \\(z_i\\) dépend à son tour de \\(z_i\\). Ainsi, pour déterminer la distribution conjointe des \\(z\\), il faut résoudre simultanément (d’où le nom du modèle) un système d’équations.\nPour cette raison, même si ce modèle ressemble à la formule du modèle conditionnel (CAR), les solutions des deux modèles diffèrent et dans le cas du SAR, le coefficient \\(\\rho\\) n’est pas directement égal à la corrélation partielle due à chaque région voisine.\nPour plus de détails sur les aspects mathématiques de ces modèles, vous pouvez consulter l’article de Ver Hoef et al. (2018) suggéré en référence.\nPour l’instant, nous considérerons les SAR et les CAR comme deux types de modèles possibles pour représenter une corrélation spatiale sur un réseau. Nous pouvons toujours ajuster plusieurs modèles et les comparer avec l’AIC pour choisir la meilleure forme de la corrélation ou la meilleure matrice de poids.\nLes modèles CAR et SAR partagent un avantage sur les modèles géostatistiques au niveau de l’efficacité. Dans un modèle géostatistique, les corrélations spatiales sont définies entre chaque paire de points, même si elles deviennent négligeables lorsque la distance augmente. Pour un modèle CAR ou SAR, seules les régions voisines contribuent et la plupart des poids sont égaux à 0, ce qui rend ces modèles plus rapides à ajuster qu’un modèle géostatistique lorsque les données sont massives."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#définition-du-réseau-de-voisinage",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#définition-du-réseau-de-voisinage",
    "title": "Spatial Statistics in Ecology",
    "section": "Définition du réseau de voisinage",
    "text": "Définition du réseau de voisinage\nLa fonction poly2nb du package spdep définit un réseau de voisinage à partir de polygones. Le résultat vois est une liste de 125 éléments où chaque élément contient les indices des polygones voisins (limitrophes) d’un polygone donné.\n\nvois <- poly2nb(elect2018)\nvois[[1]]\n\n[1]   2  37  63  88 101 117\n\n\nAinsi, la première circonscription (Abitibi-Est) a 6 circonscriptions voisines, dont on peut trouver les noms ainsi:\n\nelect2018$circ[vois[[1]]]\n\n[1] \"Abitibi-Ouest\"               \"Gatineau\"                   \n[3] \"Laviolette-Saint-Maurice\"    \"Pontiac\"                    \n[5] \"Rouyn-Noranda-Témiscamingue\" \"Ungava\"                     \n\n\nNous pouvons illustrer ce réseau en faisant l’extraction des coordonnées du centre de chaque circonscription, en créant une carte muette avec plot(elect2018[\"geometry\"]), puis en ajoutant le réseau comme couche additionnelle avec plot(vois, add = TRUE, coords = coords).\n\ncoords <- st_centroid(elect2018) %>%\n    st_coordinates()\nplot(elect2018[\"geometry\"])\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nOn peut faire un “zoom” sur le sud du Québec en choisissant les limites xlim et ylim appropriées.\n\nplot(elect2018[\"geometry\"], \n     xlim = c(400000, 800000), ylim = c(100000, 500000))\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nIl nous reste à ajouter des poids à chaque lien du réseau avec la fonction nb2listw. Le style de poids “B” correspond aux poids binaires, soit 1 pour la présence de lien et 0 pour l’absence de lien entre deux circonscriptions.\nUne fois ces poids définis, nous pouvons vérifier avec le test de Moran s’il y a une autocorrélation significative des votes obtenus par la CAQ entre circonscriptions voisines.\n\npoids <- nb2listw(vois, style = \"B\")\n\nmoran.test(elect2018$propCAQ, poids)\n\n\n    Moran I test under randomisation\n\ndata:  elect2018$propCAQ  \nweights: poids    \n\nMoran I statistic standard deviate = 13.148, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.680607768      -0.008064516       0.002743472 \n\n\nLa valeur de \\(I = 0.68\\) est très significative à en juger par la valeur \\(p\\) du test.\nVérifions si la corrélation spatiale persiste après avoir tenu compte des quatre caractéristiques de la population, donc en inspectant les résidus d’un modèle linéaire incluant ces quatre prédicteurs.\n\nelect_lm <- lm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, data = elect2018)\nsummary(elect_lm)\n\n\nCall:\nlm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.9890  -4.4878   0.0562   6.2653  25.8146 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.354e+01  1.836e+01   0.737    0.463    \nage_moy     -9.170e-01  3.855e-01  -2.378    0.019 *  \npct_frn      4.588e+01  5.202e+00   8.820 1.09e-14 ***\npct_prp      3.582e+01  6.527e+00   5.488 2.31e-07 ***\nrev_med     -2.624e-05  2.465e-04  -0.106    0.915    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.409 on 120 degrees of freedom\nMultiple R-squared:  0.6096,    Adjusted R-squared:  0.5965 \nF-statistic: 46.84 on 4 and 120 DF,  p-value: < 2.2e-16\n\nmoran.test(residuals(elect_lm), poids)\n\n\n    Moran I test under randomisation\n\ndata:  residuals(elect_lm)  \nweights: poids    \n\nMoran I statistic standard deviate = 6.7047, p-value = 1.009e-11\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.340083290      -0.008064516       0.002696300 \n\n\nL’indice de Moran a diminué mais demeure significatif, donc une partie de la corrélation précédente était induite par ces prédicteurs, mais il reste une corrélation spatiale due à d’autres facteurs."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèles-dautorégression-spatiale",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#modèles-dautorégression-spatiale",
    "title": "Spatial Statistics in Ecology",
    "section": "Modèles d’autorégression spatiale",
    "text": "Modèles d’autorégression spatiale\nFinalement, nous ajustons des modèles SAR et CAR à ces données avec la fonction spautolm (spatial autoregressive linear model) de spatialreg. Voici le code pour un modèle SAR incluant l’effet des même quatre prédicteurs.\n\nelect_sar <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids)\nsummary(elect_sar)\n\n\nCall: spautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-23.08342  -4.10573   0.24274   4.29941  23.08245 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 15.09421119 16.52357745  0.9135   0.36098\nage_moy     -0.70481703  0.32204139 -2.1886   0.02863\npct_frn     39.09375061  5.43653962  7.1909 6.435e-13\npct_prp     14.32329345  6.96492611  2.0565   0.03974\nrev_med      0.00016730  0.00023209  0.7208   0.47101\n\nLambda: 0.12887 LR test value: 42.274 p-value: 7.9339e-11 \nNumerical Hessian standard error of lambda: 0.012069 \n\nLog likelihood: -433.8862 \nML residual variance (sigma squared): 53.028, (sigma: 7.282)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 881.77\n\n\nLa valeur donnée par Lambda dans le sommaire correspond au coefficient \\(\\rho\\) dans notre description du modèle. Le test du rapport de vraisemblance (LR test) confirme que cette corrélation spatiale résiduelle (après avoir tenu compte de l’effet des prédicteurs) est significative.\nLes effets estimés pour les prédicteurs sont semblables à ceux du modèle linéaire sans corrélation spatiale. Les effets de l’âge moyen, de la fraction de francophones et la fraction de propriétaires demeurent significatifs, bien que leur magnitude ait un peu diminué.\nPour évaluer un modèle CAR plutôt que SAR, nous devons spécifier family = \"CAR\".\n\nelect_car <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids, family = \"CAR\")\nsummary(elect_car)\n\n\nCall: spautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids, family = \"CAR\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-21.73315  -4.24623  -0.24369   3.44228  23.43749 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 16.57164696 16.84155327  0.9840  0.325128\nage_moy     -0.79072151  0.32972225 -2.3981  0.016478\npct_frn     38.99116707  5.43667482  7.1719 7.399e-13\npct_prp     17.98557474  6.80333470  2.6436  0.008202\nrev_med      0.00012639  0.00023106  0.5470  0.584364\n\nLambda: 0.15517 LR test value: 40.532 p-value: 1.9344e-10 \nNumerical Hessian standard error of lambda: 0.0026868 \n\nLog likelihood: -434.7573 \nML residual variance (sigma squared): 53.9, (sigma: 7.3416)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 883.51\n\n\nPour un modèle CAR avec des poids binaires, la valeur de Lambda (que nous avions appelé \\(\\rho\\)) donne directement le coefficient de corrélation partielle entre circonscriptions voisines. Notez que l’AIC ici est légèrement supérieur au modèle SAR, donc ce dernier donnait un meilleur ajustement."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercice-3",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#exercice-3",
    "title": "Spatial Statistics in Ecology",
    "section": "Exercice",
    "text": "Exercice\nLe jeu de données rls_covid, en format shapefile, contient des données sur les cas de COVID-19 détectés, le nombre de cas par 1000 personnes (taux_1k) et la densité de population (dens_pop) dans chacun des réseaux locaux de service de santé (RLS) du Québec. (Source: Données téléchargées de l’Institut national de santé publique du Québec en date du 17 janvier 2021.)\n\nrls_covid <- read_sf(\"data/rls_covid.shp\")\nhead(rls_covid)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 785111.2 ymin: 341057.8 xmax: 979941.5 ymax: 541112.7\nProjected CRS: Conique_conforme_de_Lambert_du_MTQ_utilis_e_pour_Adresse_Qu_be\n# A tibble: 6 × 6\n  RLS_code RLS_nom                 cas taux_1k dens_…¹                  geometry\n  <chr>    <chr>                 <dbl>   <dbl>   <dbl>        <MULTIPOLYGON [m]>\n1 0111     RLS de Kamouraska       152    7.34    6.76 (((827028.3 412772.4, 82…\n2 0112     RLS de Rivière-du-Lo…   256    7.34   19.6  (((855905 452116.9, 8557…\n3 0113     RLS de Témiscouata       81    4.26    4.69 (((911829.4 441311.2, 91…\n4 0114     RLS des Basques          28    3.3     5.35 (((879249.6 471975.6, 87…\n5 0115     RLS de Rimouski         576    9.96   15.5  (((917748.1 503148.7, 91…\n6 0116     RLS de La Mitis          76    4.24    5.53 (((951316 523499.3, 9525…\n# … with abbreviated variable name ¹​dens_pop\n\n\nAjustez un modèle linéaire du nombre de cas par 1000 en fonction de la densité de population (il est suggéré d’appliquer une transformation logarithmique à cette dernière). Vérifiez si les résidus du modèle sont corrélés entre RLS limitrophes avec un test de Moran, puis modélisez les mêmes données avec un modèle autorégressif conditionnel."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#référence",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#référence",
    "title": "Spatial Statistics in Ecology",
    "section": "Référence",
    "text": "Référence\nVer Hoef, J.M., Peterson, E.E., Hooten, M.B., Hanks, E.M. et Fortin, M.-J. (2018) Spatial autoregressive models for statistical inference from ecological data. Ecological Monographs 88: 36-59."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#données",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#données",
    "title": "Spatial Statistics in Ecology",
    "section": "Données",
    "text": "Données\nLe jeu de données gambia inclus avec le package geoR présente les résultats d’une étude sur la prévalence du paludisme chez les enfants de 65 villages en Gambie. Nous utiliserons une version légèrement transformée des données contenues dans le fichier gambia.csv.\n\nlibrary(geoR)\n\ngambia <- read.csv(\"data/gambia.csv\")\nhead(gambia)\n\n  id_village        x        y pos  age netuse treated green phc\n1          1 349.6313 1458.055   1 1783      0       0 40.85   1\n2          1 349.6313 1458.055   0  404      1       0 40.85   1\n3          1 349.6313 1458.055   0  452      1       0 40.85   1\n4          1 349.6313 1458.055   1  566      1       0 40.85   1\n5          1 349.6313 1458.055   0  598      1       0 40.85   1\n6          1 349.6313 1458.055   1  590      1       0 40.85   1\n\n\nVoici les champs de ce jeu de données:\n\nid_village: Identifiant du village.\nx and y: Coordonnées spatiales du village (en km, basé sur les coordonnées UTM).\npos: Réponse binaire, si l’enfant a eu un test positif du paludisme.\nage: Âge de l’enfant en jours.\nnetuse: Si l’enfant dort sous un moustiquaire ou non.\ntreated: Si le moustiquaire est traité ou non.\ngreen: Mesure de la végétation basée sur les données de télédétection (disponible à l’échelle du village).\nphc: Présence ou absence d’un centre de santé publique pour le village.\n\nNous pouvons compter le nombre de cas positifs et le nombre total d’enfants testés par village pour cartographier la fraction des cas positifs (ou prévalence, prev).\n\n# Jeu de données à l'échelle du village\ngambia_agg <- group_by(gambia, id_village, x, y, green, phc) %>%\n    summarize(pos = sum(pos), total = n()) %>%\n    mutate(prev = pos / total) %>%\n    ungroup()\n\n`summarise()` has grouped output by 'id_village', 'x', 'y', 'green'. You can\noverride using the `.groups` argument.\n\nhead(gambia_agg)\n\n# A tibble: 6 × 8\n  id_village     x     y green   phc   pos total  prev\n       <int> <dbl> <dbl> <dbl> <int> <int> <int> <dbl>\n1          1  350. 1458.  40.8     1    17    33 0.515\n2          2  359. 1460.  40.8     1    19    63 0.302\n3          3  360. 1460.  40.1     0     7    17 0.412\n4          4  364. 1497.  40.8     0     8    24 0.333\n5          5  366. 1460.  40.8     0    10    26 0.385\n6          6  367. 1463.  40.8     0     7    18 0.389\n\n\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nNous utilisons le jeu de données gambia.borders du package geoR pour tracer les frontières des pays avec geom_path. Comme ces frontières sont en mètres, nous les divisons par 1000 pour obtenir la même échelle que nos points. Nous utilisons également coord_fixed pour assurer un rapport d’aspect de 1:1 entre les axes et utilisons la palette de couleur viridis, qui permet de visualiser plus facilement une variable continue par rapport à la palette par défaut dans ggplot2.\nSur la base de cette carte, il semble y avoir une corrélation spatiale dans la prévalence du paludisme, le groupe de villages de l’est montrant des valeurs de prévalence plus élevées (jaune-vert) et le groupe du milieu montrant des valeurs de prévalence plus faibles (violet)."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#glmm-non-spatial",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#glmm-non-spatial",
    "title": "Spatial Statistics in Ecology",
    "section": "GLMM non spatial",
    "text": "GLMM non spatial\nPour ce premier exemple, nous allons ignorer l’aspect spatial des données et modéliser la présence du paludisme (pos) en fonction de l’utilisation d’une moustiquaire (netuse) et de la présence d’un centre de santé publique (phc). Comme nous avons une réponse binaire, nous devons utiliser un modèle de régression logistique (un GLM). Comme nous avons des prédicteurs au niveau individuel et au niveau du village et que nous nous attendons à ce que les enfants d’un même village aient une probabilité plus similaire d’avoir le paludisme même après avoir pris en compte ces prédicteurs, nous devons ajouter un effet aléatoire du village. Le résultat est un GLMM que nous ajustons en utilisant la fonction glmer du package lme4.\n\nlibrary(lme4)\n\nmod_glmm <- glmer(pos ~ netuse + phc + (1 | id_village), \n                  data = gambia, family = binomial)\nsummary(mod_glmm)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: pos ~ netuse + phc + (1 | id_village)\n   Data: gambia\n\n     AIC      BIC   logLik deviance df.resid \n  2428.0   2450.5  -1210.0   2420.0     2031 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1286 -0.7120 -0.4142  0.8474  3.3434 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n id_village (Intercept) 0.8149   0.9027  \nNumber of obs: 2035, groups:  id_village, 65\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.1491     0.2297   0.649   0.5164    \nnetuse       -0.6044     0.1442  -4.190 2.79e-05 ***\nphc          -0.4985     0.2604  -1.914   0.0556 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) netuse\nnetuse -0.422       \nphc    -0.715 -0.025\n\n\nD’après ces résultats, les variables netuse et phc sont toutes deux associées à une diminution de la prévalence du paludisme, bien que l’effet de phc ne soit pas significatif à un seuil \\(\\alpha = 0.05\\). L’ordonnée à l’origine (0.149) est le logit de la probabilité de présence du paludisme pour un enfant sans moustiquaire et sans centre de santé publique, mais c’est l’ordonnée à l’origine moyenne pour tous les villages. Il y a beaucoup de variation entre les villages selon l’écart-type de l’effet aléatoire (0.90). Nous pouvons obtenir l’ordonnée à l’origine estimée pour chaque village avec la fonction coef:\n\nhead(coef(mod_glmm)$id_village)\n\n  (Intercept)     netuse        phc\n1  0.93727515 -0.6043602 -0.4984835\n2  0.09204843 -0.6043602 -0.4984835\n3  0.22500620 -0.6043602 -0.4984835\n4 -0.46271089 -0.6043602 -0.4984835\n5  0.13680037 -0.6043602 -0.4984835\n6 -0.03723346 -0.6043602 -0.4984835\n\n\nPar exemple, l’ordonnée à l’origine pour le village 1 est environ 0.94, équivalente à une probabilité de 72%:\n\nplogis(0.937)\n\n[1] 0.7184933\n\n\ntandis que celle pour le village 2 est équivalente à une probabilité de 52%:\n\nplogis(0.092)\n\n[1] 0.5229838\n\n\nLe package DHARMa fournit une méthode générale pour vérifier si les résidus d’un GLMM sont distribués selon le modèle spécifié et s’il existe une tendance résiduelle. Il simule des réplicats de chaque observation selon le modèle ajusté et détermine ensuite un “résidu standardisé”, qui est la position relative de la valeur observée par rapport aux valeurs simulées, par exemple 0 si l’observation est plus petite que toutes les simulations, 0.5 si elle se trouve au milieu, etc. Si le modèle représente bien les données, chaque valeur du résidu standardisé entre 0 et 1 doit avoir la même probabilité, de sorte que les résidus standardisés doivent produire une distribution uniforme entre 0 et 1.\nLa fonction simulateResiduals effectue le calcul des résidus standardisés, puis la fonction plot trace les graphiques de diagnostic avec les résultats de certains tests.\n\nlibrary(DHARMa)\nres_glmm <- simulateResiduals(mod_glmm)\nplot(res_glmm)\n\n\n\n\nLe graphique de gauche est un graphique quantile-quantile des résidus standardisés. Les résultats de trois tests statistiques sont également présentés: un test de Kolmogorov-Smirnov (KS) qui vérifie s’il y a un écart par rapport à la distribution théorique, un test de dispersion qui vérifie s’il y a une sous-dispersion ou une surdispersion et un test de valeurs aberrantes (outlier) basé sur le nombre de résidus qui sont plus extrêmes que toutes les simulations. Ici, nous obtenons un résultat significatif pour les valeurs aberrantes, bien que le message indique que ce résultat pourrait avoir un taux d’erreur de type I plus grand que prévu dans ce cas.\nÀ droite, nous obtenons généralement un graphique des résidus standardisés (en y) en fonction du rang des valeurs prédites, afin de vérifier l’absence de tendance résiduelle. Ici, les prédictions sont regroupées par quartile, il serait donc préférable d’agréger les prédictions et les résidus par village, ce que nous pouvons faire avec la fonction recalculateResiduals.\n\nplot(recalculateResiduals(res_glmm, group = gambia$id_village))\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nLe graphique de droite montre les points individuels, ainsi qu’une régression quantile pour le 1er quartile, la médiane et le 3e quartile. En théorie, ces trois courbes devraient être des lignes droites horizontales (pas de tendance des résidus par rapport aux prévisions). La courbe pour le 3e quartile (en rouge) est significativement différente d’une ligne horizontale, ce qui pourrait indiquer un effet systématique manquant dans le modèle."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#glmm-spatial-avec-spamm",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#glmm-spatial-avec-spamm",
    "title": "Spatial Statistics in Ecology",
    "section": "GLMM spatial avec spaMM",
    "text": "GLMM spatial avec spaMM\nLe package spaMM (modèles mixtes spatiaux) est un package R relativement récent qui permet d’effectuer une estimation approximative du maximum de vraisemblance des paramètres pour les GLM avec dépendance spatiale, modélisés soit comme un processus gaussien, soit avec un CAR (nous verrons ce dernier dans la dernière section). Le package implémente différents algorithmes, mais il existe une fonction unique fitme qui choisit l’algorithme approprié pour chaque type de modèle. Par exemple, voici le même modèle (non spatial) que nous avons vu ci-dessus, ajusté avec spaMM.\n\nlibrary(spaMM)\n\nmod_spamm_glmm <- fitme(pos ~ netuse + phc + (1 | id_village),\n                        data = gambia, family = binomial)\nsummary(mod_spamm_glmm)\n\nformula: pos ~ netuse + phc + (1 | id_village)\nEstimation of lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)   0.1491   0.2287  0.6519\nnetuse       -0.6045   0.1420 -4.2567\nphc          -0.4986   0.2593 -1.9231\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   id_village  :  0.8151  \n             --- Coefficients for log(lambda):\n      Group        Term Estimate Cond.SE\n id_village (Intercept)  -0.2045  0.2008\n# of obs: 2035; # of groups: id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1210.016\n\n\nNotez que les estimés des effets fixes ainsi que la variance des effets aléatoires sont presque identiques à ceeux obtenues par glmer ci-dessus.\nNous pouvons maintenant utiliser spaMM pour ajuster le même modèle avec l’ajout de corrélations spatiales entre les villages. Dans la formule du modèle, ceci est représenté comme un effet aléatoire Matern(1 | x + y), ce qui signifie que les ordonnées à l’origine sont spatialement corrélées entre les villages suivant une fonction de corrélation de Matérn des coordonnées (x, y). La fonction de Matérn est une fonction flexible de corrélation spatiale qui comprend un paramètre de forme \\(\\nu\\) (nu), de sorte que lorsque \\(\\nu = 0,5\\), elle est équivalente à la corrélation exponentielle, mais quand \\(\\nu\\) prend de grandes valeurs, elle se rapproche d’une corrélation gaussienne. Nous pourrions laisser la fonction estimer \\(\\nu\\), mais ici nous le fixons à 0.5 avec l’argument fixed de fitme.\n\nmod_spamm <- fitme(pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village),\n                   data = gambia, family = binomial, fixed = list(nu = 0.5))\n\nIncrease spaMM.options(separation_max=<.>) to at least 21 if you want to check separation (see 'help(separation)').\n\nsummary(mod_spamm)\n\nformula: pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village)\nEstimation of corrPars and lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nEstimation of lambda by 'outer' ML, maximizing logL.\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3352  0.2047\nnetuse      -0.51719   0.1407 -3.6757\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu      1.rho \n0.50000000 0.05128692 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.6421 \n   id_village  :  0.1978  \n# of obs: 2035; # of groups: x + y, 65; id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1197.968\n\n\nCommençons par vérifier les effets aléatoires du modèle. La fonction de corrélation spatiale a un paramètre rho égal à 0.0513. Ce paramètre dans spaMM est l’inverse de la portée, donc ici la portée de la corrélation exponentielle est de 1/0.0513 ou environ 19.5 km. Il y a maintenant deux pramètres de variance, celui identifié comme x + y est la variance à longue distance (i.e. le palier) pour le modèle de corrélation exponentielle alors que celui identifié comme id_village montre la portion non corrélée de la variation entre les villages.\nSi nous avions ici laissé les effets aléatoires (1 | id_village) dans la formule pour représenter la partie non spatiale de la variation entre les villages, nous pourrions également représenter ceci avec un effet de pépite dans le modèle géostatistique. Dans les deux cas, cela représenterait l’idée que même deux villages très proches l’un de l’autre auraient des prévalences de base différentes dans le modèle.\nPar défaut, la fonction Matern n’a pas d’effet de pépite, mais nous pouvons en ajouter un en spécifiant une pépite non nulle dans la liste initiale des paramètres init.\n\nmod_spamm2 <- fitme(pos ~ netuse + phc + Matern(1 | x + y),\n                    data = gambia, family = binomial, fixed = list(nu = 0.5),\n                    init = list(Nugget = 0.1))\n\nIncrease spaMM.options(separation_max=<.>) to at least 21 if you want to check separation (see 'help(separation)').\n\nsummary(mod_spamm2)\n\nformula: pos ~ netuse + phc + Matern(1 | x + y)\nEstimation of corrPars and lambda by ML (p_v approximation of logL).\nEstimation of fixed effects by ML (p_v approximation of logL).\nEstimation of lambda by 'outer' ML, maximizing logL.\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3352  0.2047\nnetuse      -0.51719   0.1407 -3.6757\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu   1.Nugget      1.rho \n0.50000000 0.23551027 0.05128692 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.8399  \n# of obs: 2035; # of groups: x + y, 65 \n ------------- Likelihood values  -------------\n                        logLik\nlogL       (p_v(h)): -1197.968\n\n\nComme vous pouvez le voir, toutes les estimations sont les mêmes, sauf que la variance de la portion spatiale (palier) est maintenant de 0.84 et que la pépite est égale à une fraction 0.235 de ce palier, soit une variance de 0.197, ce qui est identique à l’effet aléatoire id_village dans la version ci-dessus. Les deux formulations sont donc équivalentes.\nMaintenant, rappelons les coefficients que nous avions obtenus pour le GLMM non spatial :\n\nsummary(mod_glmm)$coefficients\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept)  0.1490596  0.2296971  0.6489399 5.163772e-01\nnetuse      -0.6043602  0.1442448 -4.1898243 2.791706e-05\nphc         -0.4984835  0.2604083 -1.9142382 5.558973e-02\n\n\nDans la version spatiale, les deux effets fixes se sont légèrement rapprochés de zéro, mais l’erreur-type de l’effet de phc a diminué. Il est intéressant de noter que l’inclusion de la dépendance spatiale nous a permis d’estimer plus précisément l’effet de la présence d’un centre de santé publique dans le village. Ce ne serait pas toujours le cas: pour un prédicteur qui est également fortement corrélé dans l’espace, la corrélation spatiale dans la réponse rend plus difficile l’estimation de l’effet de ce prédicteur, puisqu’il est confondu avec l’effet spatial. Cependant, pour un prédicteur qui n’est pas corrélé dans l’espace, l’inclusion de l’effet spatial réduit la variance résiduelle (non spatiale) et peut donc augmenter la précision de l’effet du prédicteur.\nLe package spaMM est également compatible avec DHARMa pour les diagnostics résiduels. (Vous pouvez ignorer l’avertissement selon lequel il ne fait pas partie de la classe des modèles pris en charge, cela est dû à l’utilisation de la fonction fitme plutôt que d’une fonction d’algorithme spécifique dans spaMM).\n\nres_spamm <- simulateResiduals(mod_spamm2)\nplot(res_spamm)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\nplot(recalculateResiduals(res_spamm, group = gambia$id_village))\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nEnfin, bien que nous allons montrer comment calculer et visualiser des prédictions spatiales ci-dessous, nous pouvons produire une carte rapide des effets spatiaux estimés dans un modèle spaMM avec la fonction filled.mapMM.\n\nfilled.mapMM(mod_spamm2)"
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#processus-gaussiens-vs.-splines-de-lissage",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#processus-gaussiens-vs.-splines-de-lissage",
    "title": "Spatial Statistics in Ecology",
    "section": "Processus gaussiens vs. splines de lissage",
    "text": "Processus gaussiens vs. splines de lissage\nSi vous connaissez bien les modèles additifs généralisés (GAM), vous avez peut-être pensé à représenter la variation spatiale de la prévalence du paludisme (comme le montre la carte ci-dessus) par une spline de lissage en 2D (en fonction de \\(x\\) et \\(y\\)) dans un GAM.\nLe code ci-dessous correspond à l’équivalent GAM de notre GLMM avec processus gaussien ci-dessus, ajusté avec la fonction gam du package mgcv. L’effet spatial est représenté par la spline 2D s(x, y) alors que l’effet aléatoire non spatial de village est représenté par s(id_village, bs = \"re\"), qui est équivalent à (1 | id_village) dans les modèles précédents. Notez que pour la fonction gam, les variables catégorielles doivent être explicitement converties en facteurs.\n\nlibrary(mgcv)\ngambia$id_village <- as.factor(gambia$id_village)\nmod_gam <- gam(pos ~ netuse + phc + s(id_village, bs = \"re\") + s(x, y), \n               data = gambia, family = binomial)\n\nPour visualiser la spline en 2D, nous utiliserons le package gratia.\n\nlibrary(gratia)\ndraw(mod_gam)\n\n\n\n\nNotez que le graphique de la spline s(x, y) (en haut à droite) ne s’étend pas trop loin des emplacements des données (les autres zones sont vides). Dans ce graphique, on peut également voir que les effets aléatoires des villages suivent la distribution gaussienne attendue (en haut à gauche).\nEnsuite, nous utiliserons à la fois le GLMM spatial de la section précédente et ce GAMM pour prédire la prévalence moyenne sur une grille spatiale de points contenue dans le fichier gambia_pred.csv. Le graphique ci-dessous ajoute ces points de prédiction (en noir) sur la carte précédente des points de données.\n\ngambia_pred <- read.csv(\"data/gambia_pred.csv\")\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(data = gambia_pred) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nPour faire des prédictions à partir du modèle GAMM à ces endroits, le code ci-dessous effectue les étapes suivantes:\n\nTous les prédicteurs du modèle doivent se trouver dans le tableau de données de prédiction, nous ajoutons donc des valeurs constantes de netuse et phc (toutes deux égales à 1) pour tous les points. Ainsi, nous ferons des prédictions sur la prévalence du paludisme dans le cas où un moustiquaire est utilisée et où un centre de santé publique est présent. Nous ajoutons également un id_village constant, bien qu’il ne soit pas utilisé dans les prédictions (voir ci-dessous).\nNous appelons la fonction predict à la sortie de gam pour produire des prédictions aux nouveaux points de données (argument newdata), en incluant les erreurs-types (se.fit = TRUE) et en excluant les effets aléatoires du village, donc la prédiction est faite pour un “village moyen”. L’objet résultant gam_pred aura des colonnes fit (prédiction moyenne) et se.fit (erreur-type). Ces prédictions et erreurs-types sont sur l’échelle du lien (logit).\nNous rattachons le jeu de données de prédiction original à gam_pred avec cbind.\nNous ajoutons des colonnes pour la prédiction moyenne et les limites de l’intervalle de confiance à 50% (moyenne \\(\\pm\\) 0.674 erreur-type), converties de l’échelle logit à l’échelle de probabilité avec plogis. Nous choisissons un intervalle de 50% car un intervalle de 95% peut être trop large ici pour contraster les différentes prédictions sur la carte à la fin de cette section.\n\n\ngambia_pred <- mutate(gambia_pred, netuse = 1, phc = 1, id_village = 1)\n\ngam_pred <- predict(mod_gam, newdata = gambia_pred, se.fit = TRUE, \n                    exclude = \"s(id_village)\")\ngam_pred <- cbind(gambia_pred, as.data.frame(gam_pred))\ngam_pred <- mutate(gam_pred, pred = plogis(fit), \n                   lo = plogis(fit - 0.674 * se.fit), # 50% CI\n                   hi = plogis(fit + 0.674 * se.fit))\n\nNote : La raison pour laquelle nous ne faisons pas de prédictions directement sur l’échelle de probabilité (réponse) est que la formule normale des intervalles de confiance s’applique plus précisément sur l’échelle logit. L’ajout d’un certain nombre d’erreurs-types autour de la moyenne sur l’échelle de probabilité conduirait à des intervalles moins précis et peut-être même à des intervalles de confiance en dehors de la plage de valeurs possible (0, 1) pour une probabilité.\nNous appliquons la même stratégie pour faire des prédictions à partir du GLMM spatial avec spaMM. Il y a quelques différences dans la méthode predict par rapport au cas du GAMM.\n\nL’argument binding = \"fit\" signifie que les prédictions moyennes (colonne fit) seront attachées à l’ensemble de données de prédiction et retournées sous forme de tableau de données spamm_pred.\nL’argument variances = list(linPred = TRUE) indique à predict de calculer la variance du prédicteur linéaire (donc le carré de l’erreur-type). Cependant, il apparaît comme un attribut predVar dans le tableau de données de sortie plutôt que dans une colonne se.fit, donc nous le déplaçons vers une colonne sur la ligne suivante.\n\n\nspamm_pred <- predict(mod_spamm, newdata = gambia_pred, type = \"link\",\n                      binding = \"fit\", variances = list(linPred = TRUE))\nspamm_pred$se.fit <- sqrt(attr(spamm_pred, \"predVar\"))\nspamm_pred <- mutate(spamm_pred, pred = plogis(fit), \n                     lo = plogis(fit - 0.674 * se.fit),\n                     hi = plogis(fit + 0.674 * se.fit))\n\nEnfin, nous combinons les deux ensembles de prédictions sous la forme de différentes rangées d’un tableau de données pred_all avec bind_rows. Le nom du tableau de données d’où provient chaque prédiction (gam ou spamm) apparaîtra dans la colonne “model” (argument .id). Pour simplifier la production du prochain graphique, nous utilisons ensuite pivot_longer dans le package tidyr pour changer les trois colonnes “pred”, “lo” et “hi” en deux colonnes, “stat” et “value” (pred_tall a donc trois rangées pour chaque rangée dans pred_all).\n\npred_all <- bind_rows(gam = gam_pred, spamm = spamm_pred, .id = \"model\")\n\nlibrary(tidyr)\npred_tall <- pivot_longer(pred_all, c(pred, lo, hi), names_to = \"stat\",\n                          values_to = \"value\")\n\nUne fois ces étapes franchies, nous pouvons enfin examiner les cartes de prédiction (moyenne, limites inférieure et supérieure de l’intervalle de confiance à 50 %) à l’aide d’un graphique ggplot. Les points de données originaux sont indiqués en rouge.\n\nggplot(pred_tall, aes(x = x, y = y)) +\n    geom_point(aes(color = value)) +\n    geom_point(data = gambia_agg, color = \"red\", size = 0) +\n    coord_fixed() +\n    facet_grid(stat~model) +\n    scale_color_viridis_c() +\n    theme_minimal()\n\n\n\n\nBien que les deux modèles s’accordent à dire que la prévalence est plus élevée près du groupe de villages de l’est, le GAMM estime également une prévalence plus élevée en quelques points (bord ouest et autour du centre) où il n’y a pas de données. Il s’agit d’un artefact de la forme de la spline autour des points de données, puisqu’une spline est censée correspondre à une tendance globale, bien que non linéaire. En revanche, le modèle géostatistique représente l’effet spatial sous forme de corrélations locales et revient à la prévalence moyenne globale lorsqu’il est éloigné de tout point de données, ce qui est une supposition plus sûre. C’est l’une des raisons pour lesquelles il est préférable de choisir un modèle géostatistique / processus gaussien dans ce cas."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#méthodes-bayésiennes-pour-les-glmm-avec-processus-gaussiens",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#méthodes-bayésiennes-pour-les-glmm-avec-processus-gaussiens",
    "title": "Spatial Statistics in Ecology",
    "section": "Méthodes bayésiennes pour les GLMM avec processus gaussiens",
    "text": "Méthodes bayésiennes pour les GLMM avec processus gaussiens\nLes modèles bayésiens fournissent un cadre flexible pour exprimer des modèles avec une structure de dépendance complexe entre les données, y compris la dépendance spatiale. Cependant, l’ajustement d’un modèle de processus gaussien avec une approche entièrement bayésienne peut être lent, en raison de la nécessité de calculer une matrice de covariance spatiale entre toutes les paires de points à chaque itération.\nLa méthode INLA (pour integrated nested Laplace approximation) effectue un calcul approximatif de la distribution postérieure bayésienne, ce qui la rend adaptée aux problèmes de régression spatiale. Nous ne l’abordons pas dans ce cours, mais je recommande le manuel de Paula Moraga (dans la section des références ci-dessous) qui fournit des exemples concrets d’utilisation de la méthode INLA pour divers modèles de données géostatistiques et aréales, dans le contexte de l’épidémiologie, y compris des modèles avec une dépendance à la fois spatiale et temporelle. Le livre présente les mêmes données sur le paludisme en Gambie comme exemple d’un ensemble de données géostatistiques, ce qui a inspiré son utilisation dans ce cours."
  },
  {
    "objectID": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#référence-1",
    "href": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/index.html#référence-1",
    "title": "Spatial Statistics in Ecology",
    "section": "Référence",
    "text": "Référence\nMoraga, Paula (2019) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny. Chapman & Hall/CRC Biostatistics Series. Disponible en ligne: https://www.paulamoraga.com/book-geospatial/."
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html",
    "title": "Introduction to Microbiome Analysis",
    "section": "",
    "text": "Dr. Steve Kembel, Zihui Wang, and Salix Dubois\nUniversité du Québec à Montréal"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#day-1---microbiome-quantification-using-amplicon-sequencing-approaches",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#day-1---microbiome-quantification-using-amplicon-sequencing-approaches",
    "title": "Introduction to Microbiome Analysis",
    "section": "Day 1 - Microbiome quantification using amplicon sequencing approaches",
    "text": "Day 1 - Microbiome quantification using amplicon sequencing approaches\n\nLecture - Microbiome quantification using amplicon sequencing approaches (PDF)\nR practical - Microbiome sequence analysis workshop\nDownloads\n\nSequence data and SILVA database files (~350MB download from figshare)\nR workspace Microbiome-sequence-analysis-workspace.RData\nR Markdown file used to generate the R practical document"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#day-2---data-normalization-and-ecological-analysis-of-microbiome-data",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#day-2---data-normalization-and-ecological-analysis-of-microbiome-data",
    "title": "Introduction to Microbiome Analysis",
    "section": "Day 2 - Data normalization and ecological analysis of microbiome data",
    "text": "Day 2 - Data normalization and ecological analysis of microbiome data\n\nLecture - Data normalization and ecological analysis of microbiome data (PDF)\nR practical - Microbiome ecological analysis workshop\nDownloads\n\nSample metadata metadata-Qleaf_BACT.csv\nDADA2 ASV sequence table seqtab.nochim.rds\nDADA2 ASV taxonomic annotations taxa.sp.rds\n\nR workspace Microbiome-ecological-analysis-workspace.RData\nR Markdown file used to generate the R practical document"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#about-the-data-set",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#about-the-data-set",
    "title": "Introduction to Microbiome Analysis",
    "section": "About the data set",
    "text": "About the data set\nIn this workshop, we will be analyzing the bacterial communities found on leaves of sugar maple seedlings of different provenances planted at sites across eastern North America.\nThese data are taken from the article:\nDe Bellis, Laforest-Lapointe, Solarik, Gravel, and Kembel. 2022. Regional variation drives differences in microbial communities associated with sugar maple across a latitudinal range. Ecology (published online ahead of print). doi:10.1002/ecy.3727\nFor this workshop, we will work with a subset of samples from six of the nine sites sampled for the article. These samples are located in three different biomes (temperate, mixed, and boreal forests). At each site, several sugar maple seedlings were planted and harvested, and we collected bacterial DNA from the leaves. Each sample thus represents the bacterial communities on the leaves of a single sugar maple seedling. For each seedling, we have associated metadata on the provenance of the seed, the site where the seed was planted, and the biome/stand type where the seed was planted. We’ll talk more about these samples and the biological hypotheses we want to test in day 2 of the workshop.\n\nData file setup and download\nTo follow along with the code, you will need to download the raw data (FASTQ sequence files) and the SILVA taxonomy files, and place a copy of these data sets in the working directory from which you are running the code.\nThe data files we will use for this workshop are available for download at the following URL: https://figshare.com/articles/dataset/Data_files_for_BIOS2_Microbiome_Analysis_Workshop/19763077.\nThe sequence data in the folder rawdata-Qleaf_BACT consist of 74 compressed FASTQ files (2 files for each of 36 samples). These files were provided by the UQAM CERMO-FC Genomics Platform where the samples were sequenced in a multiplexed run on an Illumina MiSeq. The samples were demultiplexed into separate files based on the barcode associated with each sample, so there is a separate pair of files for each sample. One file labelled R2 contains the forward read for the sample, the other file labelled R1 contains the reverse read for the sample.\nIf your samples were not automatically demultiplexed by the sequencing centre, you will instead have just two files which are normally labelled R2 for forward reads and R1 for reverse reads, and you should also receive information about the sequence barcode associated with each sample. You will need to demultiplex this file yourself by using the unique barcode associated with each sample to split the single large file containing all samples together into a separate file for each sample. It is easier to not have to do this step yourself, so I recommend asking your sequencing centre how they will provide the data and request that the data be provided demultiplexed with separate files for each sample. If you do need to do the multiplexing yourself, there are numerous tools available to carry out demultplexing, including QIIME, mothur, or the standalone idemp program.\nAfter downloading the compressed file “rawdata-Qleaf_BACT.zip” (the file is around 150MB in size), you will need to unzip the file into a folder on your computer. I suggest you place this folder in the working directory from which you are running the code for the workshop.\nThe SILVA taxonomy reference database files we will use for this workshop are available for download at the same URL where we found the sequence data: https://figshare.com/articles/dataset/Data_files_for_BIOS2_Microbiome_Analysis_Workshop/19763077\nAfter downloading the compressed file “SILVA138.1.zip” (the file is around 200MB in size), you will need to unzip the file into a folder on your computer. As for the raw data, I suggest you place this folder in the working directory from which you are running the code for the workshop.\nThis workshop uses the most recent version of the SILVA taxonomy currently available, which is version 138.1. There are up-to-date DADA2 formatted versions of the SILVA database along with other databases available at the DADA2 website.\n\n\nInstall the DADA2 package\nYou will need to install the most recent version of the DADA2 R package. At the time of writing this workshop, this is DADA2 version 1.24.0. There are instructions for installing the DADA2 package here. The command below works to install DADA2 version 1.24.0 using R version 4.2.0:\n\n# install dada2 if needed\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\", version = \"3.15\")"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#sequence-analysis-with-dada2",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#sequence-analysis-with-dada2",
    "title": "Introduction to Microbiome Analysis",
    "section": "Sequence analysis with DADA2",
    "text": "Sequence analysis with DADA2\nThis workshop is based directly on the excellent DADA2 tutorial. I highly recommend following the DADA2 tutorial when working with your own data, it is constantly updated to incorporate the latest version of the analyses available in the package and has helpful tips for analysing your own data. The DADA2 tutorial goes into more depth about each of the steps in this analysis pipeline, and includes useful advice about each step of the pipeline. In this workshop, we have adapted the tutorial materials to run through the basic steps of sequence analysis to go from sequence files to an ecological matrix of ASV abundances in samples, and the taxonomic annotation of those ASVs.\nWe begin by loading the DADA2 package.\n\nLoad package\n\n# load library, check version\nlibrary(dada2); packageVersion(\"dada2\")\n\nLoading required package: Rcpp\n\n\n[1] '1.22.0'\n\n\n\n\nSet file paths\nWe need to set file paths so DADA2 knows where to find the raw data FASTQ files. I recommend creating a RStudio project for this workshop and placing a copy of the folder with the raw data FASTQ files in the working directory; by default in RStudio the working directory is the folder where you created the project. The exact paths below will need to be changed depending on where you placed the raw data files.\n\n# set path to raw data FASTQ files\n# here we assume the raw data are in a folder named \"rawdata-Qleaf_BACT\"\n# in the current working directory (the directory where we created our\n# RStudio project for this workshop, by default)\npath <- \"rawdata-Qleaf_BACT/\"\n# list the files found in the path - this should include 74 files with\n# names like '2017-11-miseq-trimmed_R1.fastq_BC.C2.3X2.fastq.gz\"\nlist.files(path)\n\n [1] \"2017-11-miseq-trimmed_R1.fastq_AUC.C1.2X1.fastq.gz\" \n [2] \"2017-11-miseq-trimmed_R1.fastq_AUC.C1.3.fastq.gz\"   \n [3] \"2017-11-miseq-trimmed_R1.fastq_AUC.N1.1.fastq.gz\"   \n [4] \"2017-11-miseq-trimmed_R1.fastq_AUC.N1.3.fastq.gz\"   \n [5] \"2017-11-miseq-trimmed_R1.fastq_AUC.N2.1.X1.fastq.gz\"\n [6] \"2017-11-miseq-trimmed_R1.fastq_AUC.N2.1X2.fastq.gz\" \n [7] \"2017-11-miseq-trimmed_R1.fastq_AUC.N2.2.fastq.gz\"   \n [8] \"2017-11-miseq-trimmed_R1.fastq_AUC.S1.1.X2.fastq.gz\"\n [9] \"2017-11-miseq-trimmed_R1.fastq_AUC.S1X1.fastq.gz\"   \n[10] \"2017-11-miseq-trimmed_R1.fastq_AUC.S2.2.fastq.gz\"   \n[11] \"2017-11-miseq-trimmed_R1.fastq_BC.C1.2.fastq.gz\"    \n[12] \"2017-11-miseq-trimmed_R1.fastq_BC.C2.1X1.fastq.gz\"  \n[13] \"2017-11-miseq-trimmed_R1.fastq_BC.C2.3X2.fastq.gz\"  \n[14] \"2017-11-miseq-trimmed_R1.fastq_BC.C2.3X2b.fastq.gz\" \n[15] \"2017-11-miseq-trimmed_R1.fastq_BC.N1.3X2.fastq.gz\"  \n[16] \"2017-11-miseq-trimmed_R1.fastq_BC.N1.fastq.gz\"      \n[17] \"2017-11-miseq-trimmed_R1.fastq_BC.N2.1.X3.fastq.gz\" \n[18] \"2017-11-miseq-trimmed_R1.fastq_BC.N2.1X1.fastq.gz\"  \n[19] \"2017-11-miseq-trimmed_R1.fastq_BC.N2.1X2.fastq.gz\"  \n[20] \"2017-11-miseq-trimmed_R1.fastq_PAR.C1.1.fastq.gz\"   \n[21] \"2017-11-miseq-trimmed_R1.fastq_PAR.C1.2.fastq.gz\"   \n[22] \"2017-11-miseq-trimmed_R1.fastq_PAR.C2.3.fastq.gz\"   \n[23] \"2017-11-miseq-trimmed_R1.fastq_PAR.N1.3.fastq.gz\"   \n[24] \"2017-11-miseq-trimmed_R1.fastq_QLbactNeg19.fastq.gz\"\n[25] \"2017-11-miseq-trimmed_R1.fastq_QLbactNeg20.fastq.gz\"\n[26] \"2017-11-miseq-trimmed_R1.fastq_S.M.C1.3X2.fastq.gz\" \n[27] \"2017-11-miseq-trimmed_R1.fastq_S.M.C2.3.fastq.gz\"   \n[28] \"2017-11-miseq-trimmed_R1.fastq_S.M.C2.3X2.fastq.gz\" \n[29] \"2017-11-miseq-trimmed_R1.fastq_S.M.S1.1X1.fastq.gz\" \n[30] \"2017-11-miseq-trimmed_R1.fastq_SF.C1.3.fastq.gz\"    \n[31] \"2017-11-miseq-trimmed_R1.fastq_SF.C2.2.fastq.gz\"    \n[32] \"2017-11-miseq-trimmed_R1.fastq_SF.N2.3.fastq.gz\"    \n[33] \"2017-11-miseq-trimmed_R1.fastq_SF.S1.3.fastq.gz\"    \n[34] \"2017-11-miseq-trimmed_R1.fastq_T2.C1.1.fastq.gz\"    \n[35] \"2017-11-miseq-trimmed_R1.fastq_T2.C2.3.fastq.gz\"    \n[36] \"2017-11-miseq-trimmed_R1.fastq_T2.N1.1.fastq.gz\"    \n[37] \"2017-11-miseq-trimmed_R1.fastq_T2.S2.1.fastq.gz\"    \n[38] \"2017-11-miseq-trimmed_R2.fastq_AUC.C1.2X1.fastq.gz\" \n[39] \"2017-11-miseq-trimmed_R2.fastq_AUC.C1.3.fastq.gz\"   \n[40] \"2017-11-miseq-trimmed_R2.fastq_AUC.N1.1.fastq.gz\"   \n[41] \"2017-11-miseq-trimmed_R2.fastq_AUC.N1.3.fastq.gz\"   \n[42] \"2017-11-miseq-trimmed_R2.fastq_AUC.N2.1.X1.fastq.gz\"\n[43] \"2017-11-miseq-trimmed_R2.fastq_AUC.N2.1X2.fastq.gz\" \n[44] \"2017-11-miseq-trimmed_R2.fastq_AUC.N2.2.fastq.gz\"   \n[45] \"2017-11-miseq-trimmed_R2.fastq_AUC.S1.1.X2.fastq.gz\"\n[46] \"2017-11-miseq-trimmed_R2.fastq_AUC.S1X1.fastq.gz\"   \n[47] \"2017-11-miseq-trimmed_R2.fastq_AUC.S2.2.fastq.gz\"   \n[48] \"2017-11-miseq-trimmed_R2.fastq_BC.C1.2.fastq.gz\"    \n[49] \"2017-11-miseq-trimmed_R2.fastq_BC.C2.1X1.fastq.gz\"  \n[50] \"2017-11-miseq-trimmed_R2.fastq_BC.C2.3X2.fastq.gz\"  \n[51] \"2017-11-miseq-trimmed_R2.fastq_BC.C2.3X2b.fastq.gz\" \n[52] \"2017-11-miseq-trimmed_R2.fastq_BC.N1.3X2.fastq.gz\"  \n[53] \"2017-11-miseq-trimmed_R2.fastq_BC.N1.fastq.gz\"      \n[54] \"2017-11-miseq-trimmed_R2.fastq_BC.N2.1.X3.fastq.gz\" \n[55] \"2017-11-miseq-trimmed_R2.fastq_BC.N2.1X1.fastq.gz\"  \n[56] \"2017-11-miseq-trimmed_R2.fastq_BC.N2.1X2.fastq.gz\"  \n[57] \"2017-11-miseq-trimmed_R2.fastq_PAR.C1.1.fastq.gz\"   \n[58] \"2017-11-miseq-trimmed_R2.fastq_PAR.C1.2.fastq.gz\"   \n[59] \"2017-11-miseq-trimmed_R2.fastq_PAR.C2.3.fastq.gz\"   \n[60] \"2017-11-miseq-trimmed_R2.fastq_PAR.N1.3.fastq.gz\"   \n[61] \"2017-11-miseq-trimmed_R2.fastq_QLbactNeg19.fastq.gz\"\n[62] \"2017-11-miseq-trimmed_R2.fastq_QLbactNeg20.fastq.gz\"\n[63] \"2017-11-miseq-trimmed_R2.fastq_S.M.C1.3X2.fastq.gz\" \n[64] \"2017-11-miseq-trimmed_R2.fastq_S.M.C2.3.fastq.gz\"   \n[65] \"2017-11-miseq-trimmed_R2.fastq_S.M.C2.3X2.fastq.gz\" \n[66] \"2017-11-miseq-trimmed_R2.fastq_S.M.S1.1X1.fastq.gz\" \n[67] \"2017-11-miseq-trimmed_R2.fastq_SF.C1.3.fastq.gz\"    \n[68] \"2017-11-miseq-trimmed_R2.fastq_SF.C2.2.fastq.gz\"    \n[69] \"2017-11-miseq-trimmed_R2.fastq_SF.N2.3.fastq.gz\"    \n[70] \"2017-11-miseq-trimmed_R2.fastq_SF.S1.3.fastq.gz\"    \n[71] \"2017-11-miseq-trimmed_R2.fastq_T2.C1.1.fastq.gz\"    \n[72] \"2017-11-miseq-trimmed_R2.fastq_T2.C2.3.fastq.gz\"    \n[73] \"2017-11-miseq-trimmed_R2.fastq_T2.N1.1.fastq.gz\"    \n[74] \"2017-11-miseq-trimmed_R2.fastq_T2.S2.1.fastq.gz\"    \n\n\n\n\nFind data files and extract sample names\nNow that we have the list of raw data FASTQ files, we need to process these file names to identify forward and reverse reads. For each sequence, the sequencer will create two files labelled R2 and R1. One of these files contains the forward reads, the other the reverse reads. Which file is which will depend on the protocol used to create sequencing libraries. In our case, the R2 files contain the forward reads, and the R1 files contain the reverse reads.\n\n# Identify forward and reverse reads\n# Filenames containing R2 are forward reads, R1 are reverse reads\nfnFs <- sort(list.files(path, pattern=\"R2\", full.names = TRUE))\nfnRs <- sort(list.files(path, pattern=\"R1\", full.names = TRUE))\n\nWe also need to process the filenames to extract and clean up the sample names.\n\n# FASTQ filenames have format:\n# BLABLA_R1.fastq_SAMPLENAME.fastq.gz\n# Extract sample names from filenames\n# Split filename at \"_\" character, the sample name is in the third chunk\nsample.names <- sapply(strsplit(basename(fnFs), \"_\"), `[`, 3)\n# Remove extra text from the end of filenames\nsample.names <- gsub(\".fastq.gz\", \"\", sample.names)\n# Look at sample names after processing\nsample.names\n\n [1] \"AUC.C1.2X1\"  \"AUC.C1.3\"    \"AUC.N1.1\"    \"AUC.N1.3\"    \"AUC.N2.1.X1\"\n [6] \"AUC.N2.1X2\"  \"AUC.N2.2\"    \"AUC.S1.1.X2\" \"AUC.S1X1\"    \"AUC.S2.2\"   \n[11] \"BC.C1.2\"     \"BC.C2.1X1\"   \"BC.C2.3X2\"   \"BC.C2.3X2b\"  \"BC.N1.3X2\"  \n[16] \"BC.N1\"       \"BC.N2.1.X3\"  \"BC.N2.1X1\"   \"BC.N2.1X2\"   \"PAR.C1.1\"   \n[21] \"PAR.C1.2\"    \"PAR.C2.3\"    \"PAR.N1.3\"    \"QLbactNeg19\" \"QLbactNeg20\"\n[26] \"S.M.C1.3X2\"  \"S.M.C2.3\"    \"S.M.C2.3X2\"  \"S.M.S1.1X1\"  \"SF.C1.3\"    \n[31] \"SF.C2.2\"     \"SF.N2.3\"     \"SF.S1.3\"     \"T2.C1.1\"     \"T2.C2.3\"    \n[36] \"T2.N1.1\"     \"T2.S2.1\"    \n\n\nWe now have the list of filenames for the forward and reverse reads, the list of sample names, and we know the sample name associated with each file.\n\n\nLook at sequence quality profiles\nThe first step of working with our raw data is to look at the sequence quality profiles. These raw data FASTQ files were generated by sequencing a multiplexed library using the Illumina MiSeq sequencer. For this study, we used the Illumina V3 2x300nt sequencing chemistry, which sequences the first 300 nucleotides at the start (forward read) and end (reverse read) of each amplicon.\nThere is a quality score associated with each nucleotide known as the Phred quality score. This score indicates how confident we are in the identity of that nucleotide. Normally, the quality score should be above 30 (= 1 in 1000 chance of an incorrect base call) for much of the read, but it will begin to drop towards the end of the read. We want to identify where the quality begins to drop so we can trim out the low quality nucleotides.\nWe will visualize the forward and reverse read quality scores separately. Here, we begin by looking at the quality profile of forward reads for the first 9 samples.\n\n# Plot quality profile of forward reads\nplotQualityProfile(fnFs[1:9])\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the dada2 package.\n  Please report the issue at <\u001b]8;;https://github.com/benjjneb/dada2/issues\u0007https://github.com/benjjneb/dada2/issues\u001b]8;;\u0007>.\n\n\nWarning: Removed 937 rows containing missing values (`geom_tile()`).\n\n\n\n\n\nWe can see that the quality is quite good, but around 200-220 nucleotides it begins to decrease. It’s not a sharp drop (a “crash”) but the median quality starts to decline around 200 nucleotides and dips below a median quality of around 20 by around 230-240 nucleotides.\nNow let’s look at the reverse reads.\n\n# Plot quality profile of reverse reads\nplotQualityProfile(fnRs[1:9])\n\nWarning: Removed 188 rows containing missing values (`geom_tile()`).\n\n\n\n\n\nThese are similar to the forward reads, but the quality stays high for longer. This is common (the R1 read tends to be of high quality than the R2). Here, the quality stays quite high almost all the way to the end of the read, although the median quality does begin to decline starting around 230-240 nucleotides.\n\n\nTrimming and quality control\nNow that we have visualized the quality of the reads, we need to make some decisions about where to trim the reads.\nIt is very important to trim barcode and primer nucleotides from the reads as they can cause problems when identifying ASVs. Our reads contain the 12 nucleotide barcode sequence at the beginning of each read that will need to be trimmed. In our case, we used the primers 799F and 1115R to amplify the bacterial 16S rRNA gene. These primers are 18 nucleotides (799F) and 16 nucleotides (1115R) in length, and are found after the barcode sequence at the beginning of each read. Sometimes FASTQ files will be supplied with the barcodes and/or primers already removed from the sequences; you will need to check or ask the sequencing centre about this for your own data. In our case we will need to remove the forward barcode + primer nucleotides (30 nucleotides) and the reverse barcode + primer nucleotides (28 nucleotides) from the beginning of each read.\nWe also need to trim the reads to remove low-quality nucleotides from the end of the reads. As we saw above, the quality of both the forward and reverse reads are quite good, but forward read quality did begin to decline around 200 nucleotides and reverse read quality declines slightly beginning around 230-240 nucleotides.\nAn important consideration when deciding where to trim our reads is that we want to be able to merge together the forward and reverse reads later in the pipeline. To do this, we need the reads to overlap with one another. To figure out whether our forward and reverse reads will overlap after trimming, we need to know the length of the amplicon that we sequenced. In our case, the 799F-1115R primers create an amplicon that is 316 nucleotides in length. Thus, we need to make sure that the total length of the forward and reverse reads that remains after trimming is longer than 316 nucleotides in order to have some overlapping nucleotides that can be used to merge the reads together. If the forward and reverse reads cannot be merged by overlap after trimming, it will not be possible to create a merged read that covers the full length of the amplicon. This is not a fatal problem but it can reduce accuracy of ASV binning (since we are missing data for a portion of the amplicon) and it makes taxonomic annotation more challenging. The overlapping portion of the reads also serves to increase confidence in the quality of ASVs, since for the overlapping portion we have two estimates of the nucleotide at each position which serves to reduce error and increase confidence in the base call at that position.\nThis is an important consideration when choosing what primers to use and what sequencing technology to use. You should aim to ensure that the total length of your forward and reverse reads is enough to cover the length of the region targeted by your primers, with as much overlap as possible to make merging the reads together easier. In our case, with an amplicon length of 312 nucleotides, and forward/reverse reads of around 300 nucleotides each, we will be sure to have lots of overlap, so we don’t have to worry too much if we need to trim our reads.\nNow that we know the number of nucleotides to trim from the start and end of each sequence, we can now filter and trim our reads. Here, we will trim out barcode and primer nucleotides from the start of reads and low-quality nucleotides from the end of reads. We also filter out reads with low quality after trimming, as well as reads that contain N (unknown) nucleotides and reads that map to the phiX genome (a viral genome that is often added to sequencing runs to increase library quality).\nThis will create filtered versions of the FASTQ files in a new folder named dada2-filtered.\n\n# set filtered file folder paths and filenames\nfilt_path <- file.path(getwd(), \"dada2-filtered\")\nfiltFs <- file.path(filt_path, paste0(sample.names, \"_F_filt.fastq.gz\"))\nfiltRs <- file.path(filt_path, paste0(sample.names, \"_R_filt.fastq.gz\"))\n\n# Filter and trim reads\n# multithread argument = number of cores to use (set TRUE to use all)\n# note need to trim out primers at the start of the reads\n# and trim the end of the reads when the quality crashes\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(30,28),\n                     truncLen=c(200,230), maxN=0, maxEE=2,\n                     truncQ=2, rm.phix=TRUE, compress=TRUE, \n                     multithread=TRUE, verbose=TRUE)\n\n\n\nInferring ASVs\nNow that we have our trimmed quality-control sequences, we can infer the identity and abundance of ASVs in our sequence files.\nThe first step of this process is to learn the error rates of our sequences. DADA2 uses information about error rates to infer the identity of ASVs in the samples.\n\n# Learn error rates\nerrF <- learnErrors(filtFs, multithread=TRUE)\n\n103149200 total bases in 606760 reads from 32 samples will be used for learning the error rates.\n\nerrR <- learnErrors(filtRs, multithread=TRUE)\n\n101062620 total bases in 500310 reads from 22 samples will be used for learning the error rates.\n\n\nOnce we have learned the error rates, we do some bookkeeping to reduce the time required to infer ASVs by removing replicate sequences from each sample. A given FASTQ file might contain large numbers of identical sequences. We don’t need to analyse each of these sequences separately; we’ll dereplicate the file by removing duplicates of identical sequences. This keeps track of the number of duplicates so that at the end of our process we know the actual abundance of each of these identical sequences.\n\n# Dereplicate the filtered fastq files\nderepFs <- derepFastq(filtFs)\nderepRs <- derepFastq(filtRs)\n# Name the derep-class objects by the sample names\nnames(derepFs) <- sample.names\nnames(derepRs) <- sample.names\n\nNow we are ready to use the DADA2 algorithm to identify ASVs in our samples. DADA2 uses a model of the error rate in sequences to identify the unique amplicon sequence variants (ASVs) that are present in our samples.\nWhen carrying out ASV binning and chimera identification, we have the option to pool together samples (the pool argument). There are different strategies for pooling samples together. It is quicker to bin ASVs on a per-sample basis, but this might miss extremely rare ASVs that occur with low abundance in multiple samples. By pooling samples together, we can increase our ability to accurately identify these rare ASVs. The downside is that pooling samples together takes more time since we not have to analyse a larger number of sequences at the same time. The “pseudo-pooling” option strikes a balance between these options and is the approach we’ll use today. There is an in-depth explanation of the different pooling options at the DADA2 website.\n\n# Infer the sequence variants in each sample\ndadaFs <- dada(derepFs, err=errF, pool=\"pseudo\", multithread=TRUE)\n\nSample 1 - 43893 reads in 4133 unique sequences.\nSample 2 - 1535 reads in 279 unique sequences.\nSample 3 - 342 reads in 80 unique sequences.\nSample 4 - 36815 reads in 4992 unique sequences.\nSample 5 - 6901 reads in 796 unique sequences.\nSample 6 - 69130 reads in 6170 unique sequences.\nSample 7 - 13883 reads in 1640 unique sequences.\nSample 8 - 35769 reads in 2874 unique sequences.\nSample 9 - 35178 reads in 3504 unique sequences.\nSample 10 - 49487 reads in 5858 unique sequences.\nSample 11 - 15594 reads in 1607 unique sequences.\nSample 12 - 28849 reads in 2366 unique sequences.\nSample 13 - 529 reads in 110 unique sequences.\nSample 14 - 29475 reads in 2956 unique sequences.\nSample 15 - 22328 reads in 1714 unique sequences.\nSample 16 - 17615 reads in 2171 unique sequences.\nSample 17 - 24895 reads in 3116 unique sequences.\nSample 18 - 31171 reads in 2974 unique sequences.\nSample 19 - 11127 reads in 1205 unique sequences.\nSample 20 - 20 reads in 16 unique sequences.\nSample 21 - 255 reads in 57 unique sequences.\nSample 22 - 25519 reads in 2519 unique sequences.\nSample 23 - 7057 reads in 645 unique sequences.\nSample 24 - 29 reads in 11 unique sequences.\nSample 25 - 86 reads in 23 unique sequences.\nSample 26 - 11570 reads in 1413 unique sequences.\nSample 27 - 21806 reads in 2227 unique sequences.\nSample 28 - 8836 reads in 1092 unique sequences.\nSample 29 - 500 reads in 108 unique sequences.\nSample 30 - 19733 reads in 2783 unique sequences.\nSample 31 - 347 reads in 94 unique sequences.\nSample 32 - 36486 reads in 5477 unique sequences.\nSample 33 - 22106 reads in 2394 unique sequences.\nSample 34 - 24618 reads in 2881 unique sequences.\nSample 35 - 608 reads in 124 unique sequences.\nSample 36 - 25074 reads in 3998 unique sequences.\nSample 37 - 22197 reads in 2188 unique sequences.\n\n   selfConsist step 2\n\ndadaRs <- dada(derepRs, err=errR, pool=\"pseudo\", multithread=TRUE)\n\nSample 1 - 43893 reads in 3352 unique sequences.\nSample 2 - 1535 reads in 196 unique sequences.\nSample 3 - 342 reads in 71 unique sequences.\nSample 4 - 36815 reads in 4930 unique sequences.\nSample 5 - 6901 reads in 557 unique sequences.\nSample 6 - 69130 reads in 5237 unique sequences.\nSample 7 - 13883 reads in 1296 unique sequences.\nSample 8 - 35769 reads in 2357 unique sequences.\nSample 9 - 35178 reads in 2961 unique sequences.\nSample 10 - 49487 reads in 4856 unique sequences.\nSample 11 - 15594 reads in 1250 unique sequences.\nSample 12 - 28849 reads in 2007 unique sequences.\nSample 13 - 529 reads in 83 unique sequences.\nSample 14 - 29475 reads in 2315 unique sequences.\nSample 15 - 22328 reads in 1413 unique sequences.\nSample 16 - 17615 reads in 1728 unique sequences.\nSample 17 - 24895 reads in 2801 unique sequences.\nSample 18 - 31171 reads in 2593 unique sequences.\nSample 19 - 11127 reads in 1113 unique sequences.\nSample 20 - 20 reads in 16 unique sequences.\nSample 21 - 255 reads in 48 unique sequences.\nSample 22 - 25519 reads in 2316 unique sequences.\nSample 23 - 7057 reads in 571 unique sequences.\nSample 24 - 29 reads in 8 unique sequences.\nSample 25 - 86 reads in 18 unique sequences.\nSample 26 - 11570 reads in 1256 unique sequences.\nSample 27 - 21806 reads in 1916 unique sequences.\nSample 28 - 8836 reads in 1008 unique sequences.\nSample 29 - 500 reads in 96 unique sequences.\nSample 30 - 19733 reads in 2245 unique sequences.\nSample 31 - 347 reads in 87 unique sequences.\nSample 32 - 36486 reads in 5014 unique sequences.\nSample 33 - 22106 reads in 2037 unique sequences.\nSample 34 - 24618 reads in 2683 unique sequences.\nSample 35 - 608 reads in 83 unique sequences.\nSample 36 - 25074 reads in 3689 unique sequences.\nSample 37 - 22197 reads in 1876 unique sequences.\n\n   selfConsist step 2\n\n\n\n\nMerge denoised forward and reverse reads\nNow that we have identified ASVs from our sequences, we can merge together the forward and reverse reads into a single sequence that will cover the full length of our amplicon. This approach works by looking for matching nucleotides in the overlapping portion of the forward and reverse reads.\nAs noted above, if your forward and reverse reads do not overlap, they cannot be merged. It is still possible to merge them by concatenating together the forward and reverse reads, but this approach makes it more difficult to do taxonomic annotation.\n\n# Merge the denoised forward and reverse reads:\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n\n\n\nConstruct sequence table\nWe have identified ASVs and merged the forward and reverse reads into a single ASV. We can now construct the sequence table, which is a matrix of ASV abundances in each sample.\n\n# construct sequence table\nseqtab <- makeSequenceTable(mergers)\n# look at dimension of sequence table (# samples, # ASVs)\ndim(seqtab)\n\n[1]   37 2540\n\n# look at length of ASVs\ntable(nchar(getSequences(seqtab)))\n\n\n295 296 297 298 299 300 301 302 303 304 305 306 307 308 312 318 \n 12 113  73  99  82 458 264 697 484 109 140   1   5   1   1   1 \n\n\n\n\nRemove chimeras\nThe final step of processing the sequence files involves identifying and removing chimeric sequences. These are sequences that contain portions of two distinct organisms. Chimeric sequences can arise at different stages in the process of sequencing samples including during PCR and during sequencing.\n\n# remove chimeras\nseqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=TRUE, verbose=TRUE)\n\nIdentified 1425 bimeras out of 2540 input sequences.\n\n# look at dimension of sequence table (# samples, # ASVs) after chimera removal\ndim(seqtab.nochim)\n\n[1]   37 1115\n\n# what % of reads remain after chimera removal?\nsum(seqtab.nochim)/sum(seqtab)\n\n[1] 0.9836811\n\n\n\n\nTracking read fate through the pipeline\nNow we have completed the processing of our files. We can track how many reads per sample remain after the different steps of the process.\n\n# summary - track reads through the pipeline\ngetN <- function(x) sum(getUniques(x))\ntrack <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))\ncolnames(track) <- c(\"input\", \"filtered\", \"denoised\", \"merged\", \"tabled\", \"nonchim\")\nrownames(track) <- sample.names\nhead(track)\n\n            input filtered denoised merged tabled nonchim\nAUC.C1.2X1  45852    43893    43836  43297  43297   43268\nAUC.C1.3     1605     1535     1535   1532   1532    1522\nAUC.N1.1      362      342      332    324    324     324\nAUC.N1.3    38442    36815    36731  35069  35069   33707\nAUC.N2.1.X1  7193     6901     6899   6857   6857    6854\nAUC.N2.1X2  72307    69130    69116  68251  68251   68226\n\n\n\n\nTaxonomic annotation\nNow we will annotate the taxonomic identity of each ASV by comparing its sequence with a reference database. Our data are bacterial 16S rRNA sequences so we’ll use the SILVA database for this taxonomic annotation. A version of the SILVA database formatted for use with DADA2 is available at the DADA2 website, along with other databases for different barcode genes.\n\nHigher-level taxonomic annotation\nFor taxonomic annotation at ranks from domains to genera, DADA2 uses the RDP Naive Bayesian Classifier algorithm. This algorithm compares each ASV sequence to the reference database. If the bootstrap confidence in the taxonomic annotation at each rank is 50% or greater, we assign that annotation to the ASV at that rank. If an ASV cannot be confidently identified at a given rank, there will be a NA value in the taxonomy table.\nNote that this step can be time consuming, especially if you have a large number of ASVs to identify. Running this annotation step on a computer with numerous cores/threads will speed up the process.\n\n# identify taxonomy\ntaxa <- assignTaxonomy(seqtab.nochim, \"SILVA138.1/silva_nr99_v138.1_train_set.fa.gz\", multithread=TRUE, tryRC=TRUE)\n\n\n\nSpecies-level taxonomic annotation\nWe use the RDP Naive Bayesian Classifier algorithm for taxonomic annotations at ranks from domains to genera. This allows for annotations even if there is not an exact match between ASV sequences and sequences in the reference database. At the species level, we need a different strategy, since we do not want to identify an ASV as belonging to a species unless its sequence matches that species exactly. Note that this approach means that we cannot carry out species-level taxonomic annotation if we concatenated our sequences rather than merging them together into a single sequence.\nThis step can also be somewhat time consuming depending on the number of ASVs to be annotated.\n\n# exact species matching (won't work if concatenating sequences)\ntaxa.sp <- addSpecies(taxa,  \"SILVA138.1/silva_species_assignment_v138.1.fa.gz\", allowMultiple = TRUE, tryRC = TRUE)\n\nNow that we have the taxonomic annotations, we can inspect them. We create a special version of the taxonomy table with the ASV names removed (the name of each ASV is its full sequence, so the names are very long).\n\n# inspect the taxonomic assignments\ntaxa.print <- taxa.sp # Removing sequence rownames for display only\nrownames(taxa.print) <- NULL\nhead(taxa.print, n=20)\n\n      Kingdom    Phylum             Class                 Order             \n [1,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n [2,] \"Bacteria\" \"Cyanobacteria\"    \"Cyanobacteriia\"      \"Chloroplast\"     \n [3,] \"Bacteria\" \"Bacteroidota\"     \"Bacteroidia\"         \"Cytophagales\"    \n [4,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n [5,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n [6,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n [7,] \"Bacteria\" \"Actinobacteriota\" \"Actinobacteria\"      \"Micrococcales\"   \n [8,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n [9,] \"Bacteria\" \"Actinobacteriota\" \"Actinobacteria\"      \"Micrococcales\"   \n[10,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n[11,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n[12,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n[13,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n[14,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n[15,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n[16,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n[17,] \"Bacteria\" \"Proteobacteria\"   \"Gammaproteobacteria\" \"Burkholderiales\" \n[18,] \"Bacteria\" \"Proteobacteria\"   \"Gammaproteobacteria\" \"Burkholderiales\" \n[19,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Sphingomonadales\"\n[20,] \"Bacteria\" \"Proteobacteria\"   \"Alphaproteobacteria\" \"Rhizobiales\"     \n      Family              Genus                           \n [1,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n [2,] NA                  NA                              \n [3,] \"Hymenobacteraceae\" \"Hymenobacter\"                  \n [4,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n [5,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n [6,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n [7,] \"Microbacteriaceae\" \"Amnibacterium\"                 \n [8,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n [9,] \"Microbacteriaceae\" \"Frondihabitans\"                \n[10,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n[11,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n[12,] \"Beijerinckiaceae\"  \"1174-901-12\"                   \n[13,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n[14,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n[15,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n[16,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n[17,] \"Oxalobacteraceae\"  \"Massilia\"                      \n[18,] \"Oxalobacteraceae\"  \"Massilia\"                      \n[19,] \"Sphingomonadaceae\" \"Sphingomonas\"                  \n[20,] \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n      Species                                        \n [1,] NA                                             \n [2,] NA                                             \n [3,] \"arcticus/bucti/perfusus\"                      \n [4,] \"aerolata/faeni\"                               \n [5,] NA                                             \n [6,] NA                                             \n [7,] NA                                             \n [8,] \"ginsenosidivorax\"                             \n [9,] \"australicus/cladoniiphilus/peucedani/sucicola\"\n[10,] NA                                             \n[11,] NA                                             \n[12,] NA                                             \n[13,] NA                                             \n[14,] \"faeni\"                                        \n[15,] \"echinoides/glacialis/mucosissima/rhizogenes\"  \n[16,] NA                                             \n[17,] NA                                             \n[18,] \"aurea/oculi\"                                  \n[19,] NA                                             \n[20,] NA                                             \n\n\n\n\n\nFinal steps - clean up and save data objects and workspace\nWe have now completed our analysis of the sequence files. We have created an ASV table seqtab.nochim that contains the error-corrected non-chimeric ASV sequences and their abundances in each sample. For each ASV we also have the taxonomic annotations of the ASV in the object taxa.sp. We will save these data objects to the file system, along with a copy of the entire workspace containing all the output of the analyses so far. We will use these files to continue our analyses in the next part of the workshop.\n\n## Save sequence table and taxonomic annotations to individual files\nsaveRDS(seqtab.nochim, \"seqtab.nochim.rds\")\nsaveRDS(taxa.sp, \"taxa.sp.rds\")\n## Save entire R workspace to file\nsave.image(\"Microbiome-sequence-analysis-workspace.RData\")"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#about-the-data-set-1",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#about-the-data-set-1",
    "title": "Introduction to Microbiome Analysis",
    "section": "About the data set",
    "text": "About the data set\nIn this workshop, we will be analyzing the bacterial communities found on leaves of sugar maple seedlings of different provenances planted at sites across eastern North America.\nThese data are taken from the article:\nDe Bellis, Laforest-Lapointe, Solarik, Gravel, and Kembel. 2022. Regional variation drives differences in microbial communities associated with sugar maple across a latitudinal range. Ecology (published online ahead of print). doi:10.1002/ecy.3727\nFor this workshop, we will work with a subset of samples from six of the nine sites sampled for the article. These samples are located in three different biomes (temperate, mixed, and boreal forests). At each site, several sugar maple seedlings were planted and harvested, and we collected bacterial DNA from the leaves. Each sample thus represents the bacterial communities on the leaves of a single sugar maple seedling. For each seedling, we have associated metadata on the provenance of the seed, the site where the seed was planted, and the biome/stand type where the seed was planted."
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#install-and-load-required-packages-and-data",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#install-and-load-required-packages-and-data",
    "title": "Introduction to Microbiome Analysis",
    "section": "Install and load required packages and data",
    "text": "Install and load required packages and data\nFor this part of the workshop we will need to install several R packages. The commands below should work to install these packages, you should make sure you have the latest version of R installed (version 4.2.0 at the time of writing this document).\n\n# install packages from CRAN\ninstall.packages(pkgs = c(\"Rcpp\", \"RcppArmadillo\", \"picante\", \"ggpubr\", \"pheatmap\"), dependencies = TRUE)\n# install packages from Bioconductor\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\", version = \"3.15\", update = FALSE)\n# if the dada2 install returns a warning for BiocParallel, install from binary using this command:\n# BiocManager::install(\"BiocParallel\", version = \"3.15\", type=\"binary\", update = FALSE)\nBiocManager::install(\"DESeq2\")\nBiocManager::install(\"phyloseq\")\nBiocManager::install(\"ANCOMBC\")\n\nTo begin, we will load the required packages.\n\n### load packages\nlibrary(picante)\nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(ggpubr)\nlibrary(DESeq2)\nlibrary(pheatmap)\nlibrary(ANCOMBC)\nlibrary(phyloseq)\n\n\nLoad DADA2 results\nWe will continue our analyses picking up where we left off at the end of day 1 of the workshop. We used the DADA2 package to identify ASVs and their abundances in samples, and to annotate ASVs taxonomically by comparing them to the SILVA rRNA database. We saved these data objects as files. You will need to place a copy of the files “seqtab.nochim.rds” and “taxa.sp.rds” in the working directory, and then we load them into our R workspace.\n\n# load sequence table (nonchimeric ASV abundances in samples)\nseqtab.nochim <- readRDS(\"seqtab.nochim.rds\")\n# load taxonomic annotations (taxonomic ID of each ASV)\ntaxa.sp <- readRDS(\"taxa.sp.rds\")\n\n\n\nLoad metadata\nThe metadata file we will use for this workshop is available for download at the following URL: https://figshare.com/articles/dataset/Data_files_for_BIOS2_Microbiome_Analysis_Workshop/19763077.\nYou will need to download the file “metadata-Qleaf_BACT.csv” and place a copy of this file in the working directory. Then we can load the metadata.\n\n# load metadata\nmetadata <- read.csv(\"metadata-Qleaf_BACT.csv\", row.names = 1)\n# inspect metadata\nhead(metadata)\n\n            Description    SampleType StandType TransplantedSite\nASH.S2.2       ASH.S2.2 Leaf.Bacteria    Boreal    Ashuapmushuan\nASH.N1.1.X1 ASH.N1.1.X1 Leaf.Bacteria    Boreal    Ashuapmushuan\nASH.N1.1.X3 ASH.N1.1.X3 Leaf.Bacteria    Boreal    Ashuapmushuan\nASH.N1.1X3   ASH.N1.1X3 Leaf.Bacteria    Boreal    Ashuapmushuan\nASH.C1.1       ASH.C1.1 Leaf.Bacteria    Boreal    Ashuapmushuan\nASH.C1.2       ASH.C1.2 Leaf.Bacteria    Boreal    Ashuapmushuan\n            SeedSourceRegion SeedSourceOrigin TransplantedSiteLat\nASH.S2.2               South         Kentucky               48.81\nASH.N1.1.X1            North        Montmagny               48.81\nASH.N1.1.X3            North        Montmagny               48.81\nASH.N1.1X3             North        Montmagny               48.81\nASH.C1.1             Central     Pennsylvania               48.81\nASH.C1.2             Central     Pennsylvania               48.81\n            TransplantedSiteLon SeedSourceOriginLat SeedSourceOriginLon\nASH.S2.2                 -72.77               38.26              -84.95\nASH.N1.1.X1              -72.77               46.95              -70.46\nASH.N1.1.X3              -72.77               46.95              -70.46\nASH.N1.1X3               -72.77               46.95              -70.46\nASH.C1.1                 -72.77               41.13              -77.62\nASH.C1.2                 -72.77               41.13              -77.62\n\n\nThe metadata contains information about each sample, including the following columns:\n\nDescription: the name of the sample (these match the names used on the sequence files)\nSampleType: the samples we are working with are all leaf bacteria (we collected other types of data in the original study, but here we focus just on leaf bacteria)\nStandType: the stand/biome type of the site where the seedling were planted. Either boreal, mixed, or temperate forest.\nTransplantedSite: the site where the seedling was planted\nSeedSourceRegion: the region from which the seed was collected\nSeedSourceOrigin: the site from which the seed was collected\nTransplantedSiteLat and TransplantedSiteLon: the latitude and longitude of the site where the seedling was transplanted"
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#cleaning-and-summarizing-dada2-results",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#cleaning-and-summarizing-dada2-results",
    "title": "Introduction to Microbiome Analysis",
    "section": "Cleaning and summarizing DADA2 results",
    "text": "Cleaning and summarizing DADA2 results\nOur first steps will be to do some cleaning up of the DADA2 results to make it easier to work with them. We create community and taxonomy objects that we will work with.\n\n# community object from nonchimeric ASV sequence table\ncomm <- seqtab.nochim\n# taxonomy object - make sure order of ASVs match between community and taxonomy\ntaxo <- taxa.sp[colnames(comm),]\n# keep metadata only for community samples\nmetadata <- metadata[rownames(comm),]\n\nWe will also rename the ASVs in our community and taxonomy data objects. By default, ASVs are named by their full sequence. This makes it hard to look at these objects since the names are too long to display easily. We will replace the names of ASVs by “ASV_XXX” where XXX is a number from 1 to the number of observed ASVs.\n\n# ASV names are their full sequences by default\n# This makes it very hard to look at them (the names are too long)\n# Store ASV sequence information and rename ASVs as \"ASV_XXX\"\nASV.sequence.info <- data.frame(ASV.name=paste0('ASV_',1:dim(comm)[2]),\n                                ASV.sequence=colnames(comm))\ncolnames(comm) <- ASV.sequence.info$ASV.name\nrownames(taxo) <- ASV.sequence.info$ASV.name\n\n\nExploring community data\nNow we can begin to look at our data in more depth.\n\n# community - number of samples (rows) and ASVs (columns)\ndim(comm)\n\n[1]   37 1115\n\n\nWe have 37 samples and 1115 ASVs. If we look at our data objects, we can see the type of data in each of the different objects.\n\n# look at community data object\nhead(comm)[,1:6]\n\n            ASV_1 ASV_2 ASV_3 ASV_4 ASV_5 ASV_6\nAUC.C1.2X1  10335   158  3993  4092   584  3225\nAUC.C1.3      123    98    15     0     3     3\nAUC.N1.1       84    17    16     5     0     3\nAUC.N1.3     4738   113   628  3978     0   329\nAUC.N2.1.X1  1871  1853  1431    20    58     2\nAUC.N2.1X2  19648  4200  6465   191  7221  2054\n\n# look at taxonomy object\nhead(taxo)\n\n      Kingdom    Phylum           Class                 Order             \nASV_1 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \nASV_2 \"Bacteria\" \"Cyanobacteria\"  \"Cyanobacteriia\"      \"Chloroplast\"     \nASV_3 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Cytophagales\"    \nASV_4 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Sphingomonadales\"\nASV_5 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Sphingomonadales\"\nASV_6 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \n      Family              Genus                           \nASV_1 \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\nASV_2 NA                  NA                              \nASV_3 \"Hymenobacteraceae\" \"Hymenobacter\"                  \nASV_4 \"Sphingomonadaceae\" \"Sphingomonas\"                  \nASV_5 \"Sphingomonadaceae\" \"Sphingomonas\"                  \nASV_6 \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\"\n      Species                  \nASV_1 NA                       \nASV_2 NA                       \nASV_3 \"arcticus/bucti/perfusus\"\nASV_4 \"aerolata/faeni\"         \nASV_5 NA                       \nASV_6 NA                       \n\n# look at metadata object\nhead(metadata)\n\n            Description    SampleType StandType TransplantedSite\nAUC.C1.2X1   AUC.C1.2X1 Leaf.Bacteria     Mixed          Auclair\nAUC.C1.3       AUC.C1.3 Leaf.Bacteria     Mixed          Auclair\nAUC.N1.1       AUC.N1.1 Leaf.Bacteria     Mixed          Auclair\nAUC.N1.3       AUC.N1.3 Leaf.Bacteria     Mixed          Auclair\nAUC.N2.1.X1 AUC.N2.1.X1 Leaf.Bacteria     Mixed          Auclair\nAUC.N2.1X2   AUC.N2.1X2 Leaf.Bacteria     Mixed          Auclair\n            SeedSourceRegion SeedSourceOrigin TransplantedSiteLat\nAUC.C1.2X1           Central     Pennsylvania               47.75\nAUC.C1.3             Central     Pennsylvania               47.75\nAUC.N1.1               North        Montmagny               47.75\nAUC.N1.3               North        Montmagny               47.75\nAUC.N2.1.X1            North   Rivere-du-Loup               47.75\nAUC.N2.1X2             North   Rivere-du-Loup               47.75\n            TransplantedSiteLon SeedSourceOriginLat SeedSourceOriginLon\nAUC.C1.2X1               -68.08               41.13              -77.62\nAUC.C1.3                 -68.08               41.13              -77.62\nAUC.N1.1                 -68.08               46.95              -70.46\nAUC.N1.3                 -68.08               46.95              -70.46\nAUC.N2.1.X1              -68.08               47.73              -69.48\nAUC.N2.1X2               -68.08               47.73              -69.48\n\n\n\n\nSubset community to remove host DNA\nBefore proceeding we need to remove host DNA and other contaminants and low-quality sequences. We should do this before rarefaction and subsetting samples since it could change the number of sequences per sample if there is a lot of non-target DNA. We will carry this step out first so that subsequent analyses don’t include these non-bacterial ASVs in any of the analyses.\nA reminder that primer choice and the habitat you are studying will have a big effect on the amount of non-target DNA that is in your samples. Many commonly used primers (for example, the widely used Earth Microbiome Project 16S rRNA primers 515F–806R which target the V4 region of the 16S gene) will amplify not only free living bacteria, but also DNA from host cells including chloroplasts and mitochondria. If you are working with host-associated microbiomes you may want to consider using a different primer that will not amplify host DNA. For example, when quantifying plant-associated microbiomes we commonly use the 16S primer 799F-1115R which targets the V5 region of the 16S gene and does not amplify chloroplasts or mitochondria.\nBecause we used the 799F-1115R primers for this study, we do not expect to find much host DNA, but we should check anyways, since a few host DNA sequences might have been amplified anyways. This is a very important step of the data analysis process since host DNA is not part of the microbial community and should not be included in data analyses. Otherwise, the amount of host DNA in a sample will lead to changes in the abundance of ASVs that are not actually part of the bacterial community being targeted.\nWhen using a primer that targets bacteria, host DNA and DNA of non-target organisms in the samples will typically show up ASVs annotated as belonging to a non-Bacteria domain (Archaea or Eukaryota), or the taxonomic order is “Chloroplast”, or the taxonomic family is “Mitochondria”.\nFirst we check whether any of our sequence are classified into a domain other than the bacteria.\n\n# How many ASVs are classified to different domains?\ntable(taxo[,\"Kingdom\"])\n\n\nBacteria \n    1115 \n\n\nAll of the ASVs are classified as belonging to the domain Bacteria. Now let’s check for chloroplasts and mitochondria.\n\n# How many ASVs are classified as Chloroplasts?\ntable(taxo[,\"Order\"])[\"Chloroplast\"]\n\nChloroplast \n         12 \n\n# How many ASVs are classified as Mitochondria?\ntable(taxo[,\"Family\"])[\"Mitochondria\"]\n\nMitochondria \n           2 \n\n\nThere are only a few ASVs classified as chloroplasts and mitochondria. This is expected since we used a primer that should exclude this host DNA. We will remove these ASVs before continuing our analyses.\n\n# Remove chloroplast and mitochondria ASVs from taxonomy table\ntaxo <- subset(taxo, taxo[,\"Order\"]!=\"Chloroplast\" & taxo[,\"Family\"]!=\"Mitochondria\")\n# Remove these ASVs from the community matrix as well\ncomm <- comm[,rownames(taxo)]\n\nIf you find a large number of ASVs classified as non-Bacteria, or as chloroplasts or mitochondria, you will need to remove them from your data set prior to further analysis. Sometimes this can mean that many of your samples will contain few sequences after removing the host DNA and non-bacterial ASVs. If this is the case, you can consider using a primer that will not amplify host DNA, or sequencing your samples more deeply so that enough bacterial DNA will remain after removing the host DNA. However, this latter approach will waste a lot of your sequences, and in some cases if your samples contain mostly host DNA it will be difficult to obtain enough bacterial sequences to carry out ecological analyses.\n\n\nSummary statistics\nTo begin with, we can calculate summary statistics for our community and taxonomy data. We mentioned earlier that the number of sequences per sample (library size) is normalized, meaning we aimed to sequence approximately the same number of reads per sample.\n\n# number of reads per sample\nrowSums(comm)\n\n AUC.C1.2X1    AUC.C1.3    AUC.N1.1    AUC.N1.3 AUC.N2.1.X1  AUC.N2.1X2 \n      43110        1305         307       33424        4966       63971 \n   AUC.N2.2 AUC.S1.1.X2    AUC.S1X1    AUC.S2.2     BC.C1.2   BC.C2.1X1 \n      13344       19407       33599       45548        8670       25333 \n  BC.C2.3X2  BC.C2.3X2b   BC.N1.3X2       BC.N1  BC.N2.1.X3   BC.N2.1X1 \n        271       24140        7507       15009       22164       20475 \n  BC.N2.1X2    PAR.C1.1    PAR.C1.2    PAR.C2.3    PAR.N1.3 QLbactNeg19 \n      10486          13         161       23407        6927          27 \nQLbactNeg20  S.M.C1.3X2    S.M.C2.3  S.M.C2.3X2  S.M.S1.1X1     SF.C1.3 \n         84       10137       20858        8403         485       19081 \n    SF.C2.2     SF.N2.3     SF.S1.3     T2.C1.1     T2.C2.3     T2.N1.1 \n        273       31314       19664       23534         602       20712 \n    T2.S2.1 \n      21663 \n\n# visualize log10 number of reads per sample\nhist(log10(rowSums(comm)))\n\n\n\n\nWe can see that most samples have at least 4000-10000 sequences/sample. But there are some samples with fewer reads. This includes both the negative control samples as well as some of the ‘real’ samples. We will need to remove samples with too few reads - we’ll return to this shortly.\nWe can also look at the number of sequences per ASV.\n\n# log10 of number of reads per ASV\nhist(log10(colSums(comm)))\n\n\n\n\nWe can see that ASV abundances follow a roughly lognormal distribution, meaning that there are a few highly abundant ASVs, but most ASVs are rare. Most ASVs are represented by fewer than 100 sequences (10^2).\n\n\nRemove negative controls and samples with low-quality data\nOne of the first steps of analysing ASV data sets is to remove control samples, as well as samples with too few sequences. We will also need to normalize our data to take into account the fact that samples differ in the number of sequences they contain.\n\nRarefaction curves\nWe will visualize the relationship between the number of sequences in each sample (“sample size” in the figures below) versus the number of ASVs they contain (“species” in the figures below). These rarefaction curves make it clear why we need to take the number of sequences per sample into account when we analyse sample diversity.\nFirst we plot a rarefaction curve for all samples. Each curve shows how the number of ASVs per sample changes with number of sequences in a particular sample.\n\nrarecurve(comm, step=200, label=TRUE)\n\n\n\n\nFor these rarefaction curves, the key thing to look for is where ASV richness (number of ASVs per sample) reaches a plateau relative to the number of reads per sample. It does look like the samples we can see have reached a plateau of ASV richness. But it’s hard to see the curve for many of the samples because of the x-axis limits - a few samples with a lot of sequences make it hard to see the less diverse samples. Let’s zoom in to look just at the range from 0 to 5000 sequences/sample.\n\nrarecurve(comm, step=200, label=TRUE, xlim=c(0,5000))\n\n\n\n\nNow we can see that the vast majority of samples reach a plateau in their ASV richness by around 3000-5000 sequences/sample. There are a few samples with very low numbers of both sequences and ASVs. We will now look at the composition of the communities in these different samples.\n\n\nVisualize community composition\nAnother way to quickly visualize the composition of all of our samples is to do an ordination to visualize similarity in composition among samples. Here we will visualize a principal components analysis on Hellinger-transformed ASV abundance data. Normally we should normalize the data before doing any diversity analysis but here we just want a quick look at whether some samples have very different composition from the rest. We do a PCA analysis and then plot the ordination results, overlaying confidence ellipses around the samples from the different transplanted stand types/biomes.\n\n# PCA on Hellinger-transformed community data\ncomm.pca <- prcomp(decostand(comm,\"hellinger\"))\n# plot ordination results\nordiplot(comm.pca, display=\"sites\", type=\"text\",cex=0.5)\nordiellipse(comm.pca, metadata$StandType, label=TRUE,cex=0.8)\n\n\n\n\nThere are two things to note in this ordination figure. First, we can see that along the first axis there is a separation between the negative control samples and a few other samples that cluster with the negative controls, versus all the other samples. Second, we can see that the composition of the communities seems to differ among stand type/biome along the second axis, varying from boreal to mixed to temperate forests. We’ll return to this observation later, but for now the important thing is that we can see that there are some samples that are more similar to the negative control samples than they are to the other ‘real’ samples. Let’s visualize the number of sequences per sample (library size) mapped onto these ordination axes.\n\n# Surface plot - plot number of sequences/sample (library size)\nordisurf(comm.pca, rowSums(comm), bubble=TRUE, cex=2,\n         main=\"Library size (sequences/sample)\")\n\n\n\n\nWe can see that the common characteristic of the samples that cluster on the right side of the ordination plot, next to the negative control samples, is that these samples contain very few sequences.\nThere are several reasons why some samples contain few sequences, but in general this is an indication that these samples contain data of low quality and should be excluded from further analysis. This can be caused by low microbial biomass in the original samples, with some problem with the extraction or amplification of microbial DNA in these samples, or simply by chance. Regardless, the fact that the composition of the microbial communities in these samples is more similar to the negative controls than to the other ‘real’ samples means we need to get rid of these samples.\nIf many of your samples contain few sequences, this could indicate some problem with the protocol being used for sample processing and sequencing, or the normalization step of library preparation. It could also indicate low microbial biomass. In the data we are analysing here, I suspect that the problem with these samples was low microbial biomass. The maple seedlings from which we extracted microbial DNA were sometimes small with only 1-2 tiny leaves, which doesn’t contain enough bacterial cells to reliably extract and sequence their DNA. When microbial biomass in a sample is small, the potential importance of contaminant DNA increases. Regardless of the cause, we will have to exclude these samples.\nDeciding what number of sequences to use as a cutoff when getting rid of samples with a low number of sequences will depend on the nature of your data set and your biological questions. You should always aim to ensure that you have enough reads per sample to reach a plateau in the rarefaction curve. Beyond that, you may want set a cutoff for a minimum number of reads per sample that ensures that you keep enough samples to have sufficient replicates in your different treatments.\n\n\nCheck negative controls\nWe sequenced negative controls in this study to ensure that our samples were not contaminated and that the sequences we obtained come from bacteria on leaves and not from some other step of the sample processing such as from the DNA extraction kits or during PCR. As we saw above, the inclusion of these negative control samples is useful because we can look at their composition to identify ‘real’ samples that might contain contaminants. As a final step, let’s look at the negative control samples to see what ASVs they contain.\n\n# Abundance of ASVs in negative controls\ncomm['QLbactNeg19',][comm['QLbactNeg19',]>0]\n\n  ASV_1  ASV_44  ASV_77 ASV_138 \n      1      11      11       4 \n\ncomm['QLbactNeg20',][comm['QLbactNeg20',]>0]\n\n   ASV_8   ASV_30   ASV_77   ASV_91  ASV_418  ASV_625 ASV_1002 \n       1        1        2        1       72        3        4 \n\n\nOur negative controls look clean - they contain only a few sequences (fewer than 100) belonging to a small number of ASVs. This is the type of result we are looking for, and we will remove these negative controls shortly when we remove low quality samples with small numbers of sequences. We can also look at the taxonomic identity of the ASVs that are present in the negative controls.\n\n# Taxonomic identity of ASVs present in negative controls\ntaxo[names(comm['QLbactNeg19',][comm['QLbactNeg19',]>0]),]\n\n        Kingdom    Phylum           Class                 Order             \nASV_1   \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \nASV_44  \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Enterobacterales\"\nASV_77  \"Bacteria\" \"Firmicutes\"     \"Bacilli\"             \"Lactobacillales\" \nASV_138 \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Enterobacterales\"\n        Family             Genus                           \nASV_1   \"Beijerinckiaceae\" \"Methylobacterium-Methylorubrum\"\nASV_44  \"Shewanellaceae\"   \"Shewanella\"                    \nASV_77  \"Streptococcaceae\" \"Streptococcus\"                 \nASV_138 \"Shewanellaceae\"   \"Shewanella\"                    \n        Species                                               \nASV_1   NA                                                    \nASV_44  \"algae/haliotis\"                                      \nASV_77  \"cristatus/gordonii/ictaluri/mitis/porcorum/sanguinis\"\nASV_138 NA                                                    \n\ntaxo[names(comm['QLbactNeg20',][comm['QLbactNeg20',]>0]),]\n\n         Kingdom    Phylum           Class                 Order             \nASV_8    \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Sphingomonadales\"\nASV_30   \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Cytophagales\"    \nASV_77   \"Bacteria\" \"Firmicutes\"     \"Bacilli\"             \"Lactobacillales\" \nASV_91   \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Acetobacterales\" \nASV_418  \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Xanthomonadales\" \nASV_625  \"Bacteria\" \"Firmicutes\"     \"Bacilli\"             \"Lactobacillales\" \nASV_1002 \"Bacteria\" \"Firmicutes\"     \"Bacilli\"             \"Staphylococcales\"\n         Family               Genus           \nASV_8    \"Sphingomonadaceae\"  \"Sphingomonas\"  \nASV_30   \"Hymenobacteraceae\"  \"Hymenobacter\"  \nASV_77   \"Streptococcaceae\"   \"Streptococcus\" \nASV_91   \"Acetobacteraceae\"   \"Roseomonas\"    \nASV_418  \"Rhodanobacteraceae\" \"Fulvimonas\"    \nASV_625  \"Streptococcaceae\"   \"Streptococcus\" \nASV_1002 \"Staphylococcaceae\"  \"Staphylococcus\"\n         Species                                                                               \nASV_8    \"ginsenosidivorax\"                                                                    \nASV_30   NA                                                                                    \nASV_77   \"cristatus/gordonii/ictaluri/mitis/porcorum/sanguinis\"                                \nASV_91   NA                                                                                    \nASV_418  NA                                                                                    \nASV_625  \"anginosus/mitis/oralis/pneumoniae/pseudopneumoniae\"                                  \nASV_1002 \"aureus/capitis/caprae/epidermidis/haemolyticus/saccharolyticus/saprophyticus/warneri\"\n\n\n\n\n\nSubset community to remove low sequence number samples\nNow that we have done some exploratory data analyses, we will take a subset of our communities remove the negative controls as well as samples with a low number of sequences.\nWhen we looked at the rarefaction curves, it looked like most samples reached a plateau in number of ASVs per sequence by around 3000-5000 sequences/sample. When we looked at the distribution of number of sequences per sample, we saw that most samples contained at least 4000-5000 sequences. The samples that had fewer than that number of sequences were the ones that clustered with the negative control samples to the right of the ordination figure. Thus, we will set a cutoff of 4000 sequences per sample as a minimum to include a sample in further analysis. This will remove both the negative control samples and the samples of questionable quality that contained very few sequences.\n\n# what is the dimension of the full community data set\ndim(comm)\n\n[1]   37 1027\n\n# take subset of communities with at least 4000 sequences\ncomm.sub <- comm[rowSums(comm)>=4000,]\n# also take subset of ASVs present in the remaining samples\ncomm.sub <- comm.sub[,apply(comm.sub,2,sum)>0]\n# what is the dimension of the subset community data set?\ndim(comm.sub)\n\n[1]   27 1008\n\n# subset metadata and taxonomy to match\nmetadata.sub <- metadata[rownames(comm.sub),]\ntaxo.sub <- taxo[colnames(comm.sub),]\n# descriptive stats for samples and ASVs\n# number of sequences per sample\nhist(rowSums(comm.sub))\n\n\n\n# log10 number of sequences per ASV\nhist(log10(colSums(comm.sub)))\n\n\n\n\n\nPCA on subset data\nBefore we normalize the data, let’s do the same PCA analysis on Hellinger-transformed ASV abundances, this time only for the samples that remain after getting rid of those with fewer than 5000 sequences/sample.\n\ncomm.sub.pca <- prcomp(decostand(comm.sub,\"hellinger\"))\n# plot ordination results\nordiplot(comm.sub.pca, display=\"sites\", type=\"text\",cex=0.5)\nordiellipse(comm.sub.pca, metadata.sub$StandType, label=TRUE)\n\n\n\n\nThis looks much better, there are no longer negative controls or ‘outlier’ samples that are highly distinct compositionally from the other sampes. We can still clearly see the gradient in composition from boreal-mixed-temperate stand types, which is now falling along the first two axes of the ordination. As mentioned previously, we now need to normalize the data to account for the remaining variation in sequence depth per sample."
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#data-normalization-for-diversity-analysis",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#data-normalization-for-diversity-analysis",
    "title": "Introduction to Microbiome Analysis",
    "section": "Data normalization for diversity analysis",
    "text": "Data normalization for diversity analysis\nData normalization for the analysis of microbiome data is a subject of great debate in the scientific literature. Different normalization approaches have been suggested to control for the compositional nature of microbiome data, to maximize statistical power, to account for variation in sequencing depth among samples, and to meet the assumptions of different statistical analysis methods.\nI recommend rarefaction as an essential data normalization approach that is needed when analysing the ecological diversity based on amplicon sequencing data sets. Because samples differ in sequencing depth, and this sequencing depth variation is due to the way we prepare libraries for sequencing and not any biological attribute of samples, we need to control for this variation. Rarefaction involves randomly selecting the same number of sequences per sample so that we can compare diversity metrics among samples.\nThe simplest justification for rarefaction is that that most measures of diversity are sensitive to sampling intensity. For example, taxa richness increases as we increase the number of individuals we sample from a community. If we take the same community and sample 10 individuals from it, and then sample 1000 individuals, we will obviously find more species when we sample more individuals. This is the fundamental problem that rarefaction addresses - we want to separate variation in diversity caused by some biological process from variation in diversity that is due to variation in library size.\nSimulation and empirical studies have shown that rarefaction outperforms other normalization approaches when samples differ in sequencing depth (e.g. Weiss et al. 2017. Normalization and microbial differential abundance strategies depend upon data characteristics. Microbiome 5:27. https://doi.org/10.1186/s40168-017-0237-y).\nDr. Pat Schloss has an excellent series of videos on rarefaction as part of his Riffomonas project where he demonstrates very clearly why rarefaction is necessary when calculating diversity from sequencing data sets.\n\nRarefaction\nTo rarefy our data, we will randomly select the same number of sequences per sample for all of our samples. Because we already excluded samples containing fewer than 4000 sequences, we know that we can rarefy the remaining samples to at least 4000 sequences. Let’s look at the rarefaction curve for our subset of samples.\n\n# What is the smallest number of sequences/sample for subset of samples?\nmin(rowSums(comm.sub))\n\n[1] 4966\n\n# Rarefaction curve for subset of samples\nrarecurve(comm.sub, step=200, label=FALSE, xlim=c(0,8000))\n\n\n\n\nWe can see that nearly all samples have reached a plateau of ASV richness by around 3000-4000 sequences/sample. The sample with the smallest number of sequences remaining contains 4966 sequences. We will randomly rarefy our data to this number of sequences. This way, we can feel confident that our samples contain enough sequences to adequately quantify the diversity of ASVs present in the sample even after rarefaction.\n\nset.seed(0)\n# Randomly rarefy samples\ncomm_rarfy <- rrarefy(comm.sub, sample = min(rowSums(comm.sub)))\n# Remove any ASVs whose abundance is 0 after rarefaction\ncomm_rarfy <- comm_rarfy[,colSums(comm_rarfy)>1]\n# Match ASV taxonomy to rarefied community\ntaxo_rarfy <- taxo.sub[colnames(comm_rarfy),]\n\n\nCheck the effect of rarefaction\nLet’s check to see what effect rarefaction had on the ASV richness of samples.\n\n# Rarefaction is not expected to have a major influence on the diversity (as rarefaction curves have reached the plateau).\nrichness_raw <- rowSums((comm.sub>0)*1)\nrichness_rarfy <- rowSums((comm_rarfy>0)*1)\nplot(richness_rarfy~richness_raw,xlab='number of ASVs in raw data',ylab='number of ASVs in rarefied data')\n\n\n\n\nThis supports the idea that rarefaction is not expected to have a major influence on diversity, since we saw in the rarefaction curves that richness reaches a plateau below the threshold number of sequences that we used for rarefaction. However, now that we have normalized the number of reads per sample, any differences in diversity among samples are not due to potential differences in library size. We are ready to begin our diversity analyses. We will use the rarefied community data set for these analyses."
  },
  {
    "objectID": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#community-analysis",
    "href": "posts/2022-05-19-introduction-to-microbiome-analysis/index.html#community-analysis",
    "title": "Introduction to Microbiome Analysis",
    "section": "Community analysis",
    "text": "Community analysis\nBefore we dive into analysing the diversity of leaf bacterial communities on sugar maples, this is a good time to return to the biological questions we posed in the article from which the data are taken.\n\nBiological questions\nIn the study for which these data were collected, we looked at different taxonomic groups (bacteria, fungi, mycorrhizal fungi) living on sugar maple leaves and roots. Here we will concentrate just on leaf bacteria.\nOur aim was to assess the relative influence of host genotype (seed provenance) and environment (transplanted site and region) in structuring the microbiome of sugar maple seedlings. When analyzing the data, let’s keep focused on these biological questions to help guide our decisions about how to analyze the data. We can take different perspectives to respond to this question by looking at different aspects of diversity. And beyond specific tests of our biological hypotheses, it is also useful to carry out basic descriptive analysis of community structure in order to determine what organisms were living in our samples.\nIn the metadata, the variables StandType and TransplantedSite represent the region and site into which the seedlings were planted (environment), and the variables SeedSourceRegion and SeedSourceOrigin represent the region and site from which the seeds were collected (genotype).\n\n\nVisualize taxonomic composition of communities\nA fundamental question we can address using microbiome data is simply ‘who is there’? What are the abundant taxa in different samples?\n\nPhylum-level taxonomic composition of samples\nRemember that for each ASV, we have taxonomic annotations at different ranks. Here we’ll look at the relative abundance of bacterial phyla in each sample. We can repeat these analyses at different taxonomic ranks, but as we go to finer ranks there will be more ASVs with missing data because we could not confidently determine their taxonomic annotation, so there will be more unidentified taxa. Nearly all ASVs have an annotation at the phylum level. We first manipulate the ASV data to create a new data object of phylum abundances, and then we can visualize those abundances.\n\n# community data aggregation at a taxonomic level. e.g. phylum \n# take the sum of each phylum in each sample\ntaxa.agg <- aggregate(t(comm_rarfy),by=list(taxo.sub[colnames(comm_rarfy),2]),FUN=sum)\n# clean up resulting object\nrownames(taxa.agg) <- taxa.agg$Group.1\nphylum_data <- t(taxa.agg[,-1])\n# convert abundances to relative abundances\nphylum_data <- phylum_data/rowSums(phylum_data)\n# remove rare phyla\nphylum_data <- phylum_data[,colSums(phylum_data)>0.01]\n# now reshape phylum data to long format\nphylum_data <- reshape2::melt(phylum_data)\n# rename columns\ncolnames(phylum_data)[1:2] <- c('Samples','Bacteria_phylum')\n# now we can plot phylum relative abundance per sample\nggplot(phylum_data, aes(Samples, weight = value, fill = Bacteria_phylum)) +\n  geom_bar(color = \"black\", width = .7, position = 'fill') +\n  labs( y = 'Relative abundance (%)') +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_fill_viridis_d(direction = -1L) +\n  theme_classic() +\n  coord_flip()\n\n\n\n\n\n\nPhylum-level composition for each transplant site\nIt is also useful to see how taxonomic composition varies with respect to different variables we measured. For example, how does phylum-level taxonomic composition differ among different transplant sites?\n\n# aggregate average phylum abundances per transplanted site\nphylum_data_agg <- aggregate(phylum_data$value,by=list(metadata.sub[phylum_data$Samples,]$TransplantedSite,phylum_data$Bacteria_phylum),FUN=mean)\n# rename columns\ncolnames(phylum_data_agg) <- c('TransplantedSite','Bacteria_phylum','value')\n# now we can plot phylum abundance by transplant site\nggplot(phylum_data_agg, aes(TransplantedSite, weight =value, fill = Bacteria_phylum)) +\n  geom_bar(color = \"black\", width = .7, position = 'fill') +\n  labs( y = 'Relative abundance (%)') +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_fill_viridis_d(direction = -1L) +\n  theme_classic()\n\n\n\n\n\n\n\nCommunity diversity (alpha diversity)\nTo look at how diversity differed among samples as a function of the different variables we are interested in, we’ll begin by looking at the alpha-diversity, or within-community diversity. Two commonly used measures of alpha diversity are ASV richness (the number of ASVs present in the sample), and the Shannon index, also sometimes referred to as Shannon diversity or Shannon diversity index, which measures both the number of taxa in the sample (ASV richness) as well as the equitability of their abundances (evenness).\n\nCalculate ASV richness and Shannon diversity of bacterial community\n\n# calculate ASV richness\n# calculate Shannon index\nShannon <- diversity(comm_rarfy)\nhist(richness_rarfy)\n\n\n\nhist(Shannon)\n\n\n\n\n\n\nCompare bacterial diversity among categories\nWe can ask how diversity differs with respect to seedling genotype and environment. Here, let’s ask specifically whether leaf bacterial alpha diversity differs between stand types. See Figure 2 of the article by De Bellis et al. 2022 for a comparable analysis.\n\n# create data frame to hold alpha diversity values by stand type\ndiv_standtype <- data.frame(richness=richness_rarfy, Shannon=Shannon, standtype=metadata.sub$StandType)\n# set up analysis to compare diversity among different pairs of stand types\nmy_comparisons <- list( c(\"Mixed\", \"Boreal\"), c(\"Mixed\", \"Temperate\"), c(\"Boreal\", \"Temperate\") )\n# plot ASV richness as a function of stand type\nggboxplot(div_standtype, x = \"standtype\", y = \"richness\", hide.ns=F,\n          color = \"standtype\", palette = \"jco\",add = \"jitter\") + \n  stat_compare_means(comparisons = my_comparisons,method = \"t.test\")\n\n[1] FALSE\n\n\n\n\n# plot Shannon index as a function of stand type\nggboxplot(div_standtype, x = \"standtype\", y = \"Shannon\",hide.ns=F,\n          color = \"standtype\", palette = \"jco\",add = \"jitter\")+ \n  stat_compare_means(comparisons = my_comparisons,method = \"t.test\")\n\n[1] FALSE\n\n\n\n\n\nThese figures indicate that ASV richness does not differ significantly among pairs of stand types. However, the Shannon index does differ somewhat between temperate and boreal stands, where the Shannon index tends to be lower in boreal stands than in temperate stands. These analyses are limited somewhat by the sample size that remains after removing low quality samples; in the original article (De Bellis et al 2022) we compared more sites which increased sample size enough that we found a significant (P<0.05) difference in the Shannon index between temperate and boreal stand types.\n\n\nBacterial diversity related to numeric variables\nWe can also ask if bacterial diversity differs with respect to numeric variables, for example with respect to latitude. To ask how richness varies with latitude, we will use a generalized linear model to take into account that richness counts are count data and thus are more likely to follow a Poisson distribution than a normal distribution.\n\n# plot richness versus latitude\nplot(richness_rarfy~metadata.sub$TransplantedSiteLat,xlab='Latitude',ylab='ASV richness')\n# add best fit line\nabline(glm(richness_rarfy~metadata.sub$TransplantedSiteLat), family=\"poisson\")\n\n\n\n# generalized linear model of richness vs. latitude\nsummary(glm(richness_rarfy~metadata.sub$TransplantedSiteLat, family=\"poisson\"))\n\n\nCall:\nglm(formula = richness_rarfy ~ metadata.sub$TransplantedSiteLat, \n    family = \"poisson\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-8.3234  -2.7974   0.4008   2.0747   8.9199  \n\nCoefficients:\n                                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                       7.98399    0.59571  13.403  < 2e-16 ***\nmetadata.sub$TransplantedSiteLat -0.06996    0.01246  -5.615 1.97e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 409.78  on 26  degrees of freedom\nResidual deviance: 378.60  on 25  degrees of freedom\nAIC: 555.3\n\nNumber of Fisher Scoring iterations: 4\n\n\nASV richness of leaf bacteria decreases with increasing latitude.\nWhat about the Shannon index? Here we’ll fit a linear model since we expect the data to be normally distributed.\n\n# plot Shannon index versus latitude\nplot(Shannon~metadata.sub$TransplantedSiteLat,xlab='Latitude',ylab='Shannon diversity')\n# add best fit line\nabline(lm(Shannon~metadata.sub$TransplantedSiteLat))\n\n\n\n# linear model of Shannon index vs. latitude\nsummary(lm(Shannon~metadata.sub$TransplantedSiteLat))\n\n\nCall:\nlm(formula = Shannon ~ metadata.sub$TransplantedSiteLat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.56982 -0.27830  0.07116  0.35948  1.11564 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                        7.8728     3.6067   2.183   0.0387 *\nmetadata.sub$TransplantedSiteLat  -0.1030     0.0752  -1.370   0.1829  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5839 on 25 degrees of freedom\nMultiple R-squared:  0.06983,   Adjusted R-squared:  0.03262 \nF-statistic: 1.877 on 1 and 25 DF,  p-value: 0.1829\n\n\nThe Shannon index of leaf bacteria decreases with increasing latitude, but the relationship is not statistically significant.\n\n\n\nCommunity composition (beta diversity)\nWe can now look at the beta-diversity of the samples, which measures the between-community differences in composition.\nBeta diversity approaches to studying composition involve calculating a measure of the compositional difference between pairs of communities (= beta diversity, dissimilarity, or distance). Then these distance metrics can be analysed using approaches such as ordination (to summarize the overall trends in community composition among samples) or PERMANOVA (to test whether groups of samples differ significantly in their composition).\nThere are a huge number of different beta diversity/dissimilarity metrics that have been used in ecology. There is an ongoing debate about the relative advantages and disadvantages of these beta diversity measures and approaches. There are several different beta diversity metrics and ordination approaches that work well based on simulation studies and empirical analysis. For those wanting to learn more about multivariate analysis and beta diversity approaches for the analysis of ecological communities, the book “Numerical Ecology with R” by Borcard, Gillet and Legendre is an excellent reference. Dr. Pierre Legendre also maintains a website with links to several excellent courses that serve as a complete introduction to multivariate methods in ecology.\nIn this workshop we will focus on a few different beta diversity metrics and ordination approaches that have been shown to perform well in theory and practise when applied to ecological community data. How should you choose which of these approaches to your own data? This is not an easy question to answer, as long as you are using a method that works well with ecological community data, there are different reasons to prefer one method over another. Ultimately, it can be useful to try different approaches and see if you obtain similar results with your data.\n\nOrdination - Principal Components Analysis (PCA)\nPrincipal components analysis (PCA) is a commonly used approach to analysing multivariate data. PCA uses eigenanalysis of Euclidean distances among samples computed from ASV abundance data to identify the axes of correlated variation that explain the most possible variation in your multivariate data. These axes correspond to major gradients of changes in community composition. We typically focus on the first few axes of the PCA ordination, since these axes should capture the majority of the variation in community composition among samples.\nIt’s important to note that PCA should never be used to analyze untranformed ecological community data. PCA is based on the Euclidean distance among samples. Ecological community data violate many of the assumptions of PCA - recall that there are many zeroes in our matrix of ASV abundances, and the distribution of abundance values among ASVs is very non-normal. Legendre and Gallagher (2001) showed that several transformations including the Chord and Hellinger transformation allow ecological community data matrices to be analyzed using PCA. Here we will use the Hellinger transformation to transform ASV abundances prior to analyzing them with a PCA ordination.\n\n# Create Hellinger-transformed version of rarefied community data\ncomm_hel <- decostand(comm_rarfy,method='hellinger')\n# PCA analysis of Hellinger-transformed community data\ncomm_hel_PCA <- prcomp(comm_hel)\n# Summarize variance in beta diversity explained by PCA axes\nsummary(comm_hel_PCA)\n\nImportance of components:\n                          PC1     PC2     PC3    PC4     PC5     PC6     PC7\nStandard deviation     0.2599 0.20969 0.19519 0.1718 0.16095 0.15289 0.14424\nProportion of Variance 0.1497 0.09746 0.08445 0.0654 0.05742 0.05181 0.04612\nCumulative Proportion  0.1497 0.24712 0.33157 0.3970 0.45439 0.50620 0.55232\n                           PC8    PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.13835 0.1327 0.12813 0.12268 0.12067 0.11446 0.11003\nProportion of Variance 0.04242 0.0390 0.03639 0.03336 0.03228 0.02904 0.02684\nCumulative Proportion  0.59474 0.6338 0.67014 0.70350 0.73577 0.76481 0.79165\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.10440 0.10234 0.09813 0.09351 0.09258 0.09083 0.08976\nProportion of Variance 0.02416 0.02321 0.02134 0.01938 0.01900 0.01829 0.01786\nCumulative Proportion  0.81581 0.83902 0.86037 0.87975 0.89875 0.91704 0.93490\n                          PC22    PC23    PC24    PC25    PC26      PC27\nStandard deviation     0.08279 0.08176 0.07641 0.07181 0.06955 2.431e-16\nProportion of Variance 0.01519 0.01482 0.01294 0.01143 0.01072 0.000e+00\nCumulative Proportion  0.95009 0.96491 0.97785 0.98928 1.00000 1.000e+00\n\n# Plot PCA results\nordiplot(comm_hel_PCA, display = 'sites', type = 'text',cex=0.8, main=\"PCA on Hellinger transformed data\")\n# Add ellipses around samples from different stand types\nordiellipse(comm_hel_PCA, metadata.sub$StandType, label=TRUE)\n\n\n\n\nThe PCA ordination diagram indicates the overall compositional similarity of samples. Samples that are close together in the ordination space contain similar ASVs with similar abundances. We can see that the gradient in community composition among different stand types is visible along the first two axes. Samples from each stand type tend to contain compositionally similar leaf bacterial communities.\nRecall that when we were first looking at the composition of communities in our samples (prior to subsetting and rarefaction), we obtained a similar looking result. As noted, because we used a rarefaction threshold that was sufficient for the rarefaction curves of the samples to reach a plateau in ASV richness, we do not expect major differences between the analysis of rarefied versus non-rarefied data. However, by analyzing the rarefied data we are now confident that the differences in composition among the samples are due to true differences in community composition and not due to differences in library size among samples.\n\n\nOrdination - Non-metric Multidimensional Scaling (NMDS)\nAnother commonly used ordination approach in ecology is non-metric multidimensional scaling (NMDS). NMDS can be used with any beta diversity distance metric. Unlike PCA, NMDS is not based on eigenanalysis of the distance metric. Rather, NMDS uses an algorithm to find an ordination of samples in a few dimensions that represents as best as possible the rank transformed pairwise distances among samples measured with the original distance metric. When carrying out a NMDS analysis, rather than obtaining many PCA axes, the user specifies how many axes to identify. NMDS analysis is based on a heuristic algorithm that may give slightly different results when run multiple times on the same data, whereas PCA has a unique analytical solution.\nNMDS can be used with any distance metric. Commonly in microbial ecology it is used with the Bray-Curtis distance. The Bray-Curtis distance, like the Hellinger distance, has been shown to perform well compared with other distance measures (Faith et al. 1987, Gallagher and Legendre 2001).\nHere we’ll calculate a NMDS ordination using Bray-Curtis distance.\n\n# NMDS ordination based on Bray-Curtis distances\ncomm_NMDS <- metaMDS(comm_hel, distance=\"bray\", trace=FALSE)\nordiplot(comm_NMDS, cex = 0.5, type = 'text', display='sites')\nordiellipse(comm_NMDS, metadata.sub$StandType, label=TRUE)\n# overlay the direction of latitude effect on bacteria community composition\nef <- envfit(comm_NMDS, metadata.sub$TransplantedSiteLat)\nrownames(ef$vectors$arrows)='Latitude'\nplot(ef)\n\n\n\n\nHere we can see that the NMDS ordination based on Bray-Curtis distances among samples looks quite similar to the PCA ordination, with community composition differing among different stand types. We have also added an arrow indicating the correlation between latitude and the ordination axes, which also supports the idea that communities vary as we move from boreal stands in the north to temperate stands in the south.\n\n\nPERMANOVA\nOrdination analyses allow us to visualize how the composition of communities differs among samples and how it relates to different variables in a qualitative way. For example, we can see from the ordination diagrams above that it seems that community composition differs among stand types. We can statistically test whether stand types differ in composition using permutational multivariate analysis of variance (PERMANOVA). A PERMANOVA works in a way similar to an ANOVA, but with multivariate compositional data. PERMANOVA tests indicate whether community composition (beta diversity) differs among groups of samples.\nHere we can first use a PERMANOVA to test whether community composition differs among stand types. As with ordination methods, a PERMANOVA can be run on any distance metric. Let’s try computing a PERMANOVA using both Hellinger distance (used for the PCA) and Bray-Curtis distance (used for the NMDS).\n\n# set number of permutations for PERMANOVA\nperm <- how(nperm = 999)\n# PERMANOVA on Hellinger-transformed Euclidean distances\nadonis2(formula = dist(comm_hel) ~ StandType, data = metadata.sub, permutations = perm)\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = dist(comm_hel) ~ StandType, data = metadata.sub, permutations = perm)\n          Df SumOfSqs      R2      F Pr(>F)    \nStandType  2   1.8390 0.15678 2.2311  0.001 ***\nResidual  24   9.8911 0.84322                  \nTotal     26  11.7301 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# PERMANOVA on Bray-Curtis distances\nadonis2(formula = vegdist(comm_rarfy, method=\"bray\") ~ StandType, data = metadata.sub, permutations = perm)\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = vegdist(comm_rarfy, method = \"bray\") ~ StandType, data = metadata.sub, permutations = perm)\n          Df SumOfSqs      R2      F Pr(>F)    \nStandType  2   0.9716 0.17856 2.6085  0.001 ***\nResidual  24   4.4697 0.82144                  \nTotal     26   5.4413 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth of these analyses indicate that community composition differs significantly among stand types, explaining around 16-18% of variation in the distance metric.\nPERMANOVA can also be used to test more complex experimental designs. Here, let’s replicate the analyses by De Bellis et al. (2022). We used PERMANOVA on Bray-Curtis distances to measure the relative importance of transplanted region and site (environment) versus origin region and site (genotype) in determining the composition of leaf bacterial communities. Site is nested within region for both variable types, which we represent using “/” in the formula (region/site).\n\npermanova.bc <- adonis2(formula = vegdist(comm_rarfy, method=\"bray\") ~ StandType/TransplantedSite * SeedSourceRegion/SeedSourceOrigin, data = metadata.sub, permutations = perm)\nprint(permanova.bc)\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = vegdist(comm_rarfy, method = \"bray\") ~ StandType/TransplantedSite * SeedSourceRegion/SeedSourceOrigin, data = metadata.sub, permutations = perm)\n                                                             Df SumOfSqs\nStandType                                                     2   0.9716\nSeedSourceRegion                                              2   0.2277\nStandType:TransplantedSite                                    3   0.6410\nStandType:SeedSourceRegion                                    3   0.5562\nStandType:TransplantedSite:SeedSourceRegion                   3   0.7350\nStandType:TransplantedSite:SeedSourceRegion:SeedSourceOrigin  5   1.1043\nResidual                                                      8   1.2056\nTotal                                                        26   5.4413\n                                                                  R2      F\nStandType                                                    0.17856 3.2236\nSeedSourceRegion                                             0.04184 0.7554\nStandType:TransplantedSite                                   0.11780 1.4177\nStandType:SeedSourceRegion                                   0.10222 1.2302\nStandType:TransplantedSite:SeedSourceRegion                  0.13507 1.6256\nStandType:TransplantedSite:SeedSourceRegion:SeedSourceOrigin 0.20294 1.4654\nResidual                                                     0.22157       \nTotal                                                        1.00000       \n                                                             Pr(>F)    \nStandType                                                     0.001 ***\nSeedSourceRegion                                              0.835    \nStandType:TransplantedSite                                    0.058 .  \nStandType:SeedSourceRegion                                    0.191    \nStandType:TransplantedSite:SeedSourceRegion                   0.019 *  \nStandType:TransplantedSite:SeedSourceRegion:SeedSourceOrigin  0.031 *  \nResidual                                                               \nTotal                                                                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis analysis shows that, as mentioned by De Bellis et al. (2022), stand type of planting was the most important variable explaining variation in leaf bacterial community composition. There was a marginally significant variation among transplanted sites. There were also significant interactions among transplant region and site as a function of seed source region and seed source site of origin, indicating that the leaf bacterial communities of some genotypes respond differently to different environments. Taken together, this analysis indicates that environment (transplant region and site) have a greater impact on the composition of leaf bacterial communities than genotype, although there is also an interaction between genotype and environment.\n\n\n\nDifferentially abundant taxa\nBy analyzing alpha and beta diversity, we have determined that some groups of samples differ in their diversity and composition. But measures of diversity look at the entire community. We may also be interested in knowing which individual taxa differ in abundance among groups of samples. To address this question we can use differential abundance analysis. There are many methods that can be used for differential abundance analysis, and as for other ecological analysis there is active debate about which methods should be used to detect differences in taxa abundances between groups of samples.\nDifferential abundance analysis allows you to identify differentially abundant taxa between two groups. There are many methods for this type of analysis such as ALDEx2, ANCOM-BC, and DESeq2. Several recent articles have compared the performance of these differential abundance approaches when applied to microbiome data (Calgaro et al. (2020), Nearing et al. (2022)), finding that some methods perform better than others, but also finding that different methods are sensitive to different aspects of data structure and with differing statistical power and sensitivity. Here we will compare two different methods for detecting differentially abundant taxa.\nHaving seen a clear difference in leaf bacterial community composition between temperate and boreal forest stand types, we may ask which bacterial ASVs are differentially abundant between temperate versus boreal forest stand types.\n\nDESeq2\nThe DESeq2 approach (Love et al. 2014. Genome Biol 15:550) models taxa abundances using a negative binomial distribution to detect taxa that are differentially abundant between groups. This approach was initially developed for gene expression analyses, but it is commonly used in the analysis of differential taxa abundances in microbial ecology.\n\n# conduct DESeq analysis of ASV abundances between temperate and boreal stand types\n# We add a pseudocount (+1) to each abundance value to avoid zeroes\ncountdata <- data.frame(ASV=colnames(comm_rarfy),t(comm_rarfy+1))\nmetas <- metadata.sub\nmetas$StandType <- relevel(as.factor(metas$StandType),ref='Boreal')\ndds <- DESeqDataSetFromMatrix(countData=countdata, \n                              colData=metas,\n                              design=~StandType, tidy = TRUE)\n\nconverting counts to integer mode\n\ndds2 <- DESeq(dds, fitType=\"local\", quiet=TRUE)\nres <- results(dds2, name=\"StandType_Temperate_vs_Boreal\")\n#coef of differently abundant ASV between temperate and boreal forest\nhead(res[order(res$padj),])\n\nlog2 fold change (MLE): StandType Temperate vs Boreal \nWald test p-value: StandType Temperate vs Boreal \nDataFrame with 6 rows and 6 columns\n        baseMean log2FoldChange     lfcSE      stat      pvalue        padj\n       <numeric>      <numeric> <numeric> <numeric>   <numeric>   <numeric>\nASV_8  102.71993        6.31596  1.070810   5.89830 3.67260e-09 1.31686e-06\nASV_37  16.25372        5.24749  0.906315   5.78992 7.04201e-09 1.31686e-06\nASV_94   8.43527        4.85485  0.860202   5.64385 1.66291e-08 2.07309e-06\nASV_26  29.32340        5.88306  1.086821   5.41309 6.19468e-08 5.79203e-06\nASV_22  36.19921        5.19054  0.994117   5.22126 1.77713e-07 1.32929e-05\nASV_44  20.59886       -5.09305  1.001330  -5.08629 3.65139e-07 2.27603e-05\n\n#Number of significantly different ASVs (adjusted_p_value<0.05)\ndim(res[is.na(res$padj)==F&res$padj<0.05,])[1]\n\n[1] 77\n\nsig_des <- rownames(res[is.na(res$padj)==F&res$padj<0.05,])\n# show the distribution of these marker ASV in a heatmap\nmetatoshow <- subset(metas,!metas$StandType%in%'Mixed')\ndatatoshow <- comm_rarfy[rownames(metatoshow),rownames(res[order(res$padj),])[1:20]]\nannotation_col = data.frame(standtype = as.factor(metatoshow$StandType))\nrownames(annotation_col)=rownames(datatoshow)\npheatmap(t(datatoshow),scale= \"row\", annotation_col = annotation_col,cluster_cols = F)\n\n\n\n\n\n\nANCOM-BC\nThe ANCOM-BC approach (Lin and Peddada. 2020. Nat. Comm. 35:14) detects differentially abundant taxa by analysis of compositions of microbiomes with bias correction. Nearing et al. (2022) found this method to be among the best-performing methods for detection of differentially abundant taxa.\n\n# conduct ANCOM-BC analysis of differentially abundant taxa between boreal/temperate stand types\nASV <- otu_table(countdata[,-1],taxa_are_rows<-T)\nTAX <- tax_table(as.matrix(taxo.sub))\nMETA <- sample_data(metas)\nphyloseqobj = phyloseq(ASV, TAX, META)\nancob <- ancombc(phyloseq=phyloseqobj,formula='StandType',p_adj_method = \"holm\", zero_cut = 0.90, lib_cut = 1000,\n                                 group = 'StandType', struc_zero = TRUE, neg_lb = F,\n                                 tol = 1e-5, max_iter = 100, conserve = TRUE,\n                                 alpha = 0.05, global = TRUE)\nres_ancob <- ancob$res\n#show coefficients of differential abundance test between temperate vs. boreal\ncoef_ancob <- data.frame(beta=res_ancob$beta$StandTypeTemperate,se=res_ancob$se$StandTypeTemperate,W=res_ancob$W$StandTypeTemperate,\n           p_val=res_ancob$p_val$StandTypeTemperate,p_adj=res_ancob$q_val$StandTypeTemperate,sign_dif=res_ancob$diff_abn$StandTypeTemperate,\n           row.names = rownames(res_ancob$beta))\ncoef_ancob <- coef_ancob[order(coef_ancob$p_adj),]\nhead(coef_ancob)\n\n              beta        se         W        p_val        p_adj sign_dif\nASV_44  -2.2721929 0.4081187 -5.567481 2.584489e-08 2.101189e-05     TRUE\nASV_22   2.5469174 0.5179469  4.917333 8.773119e-07 7.123773e-04     TRUE\nASV_11   2.2662741 0.4980824  4.549998 5.364645e-06 4.350727e-03     TRUE\nASV_101 -1.8558870 0.4108073 -4.517658 6.252726e-06 5.064708e-03     TRUE\nASV_37   2.3964439 0.5694741  4.208170 2.574473e-05 2.082749e-02     TRUE\nASV_129  0.4651886 0.1182296  3.934619 8.332862e-05 6.732952e-02    FALSE\n\n#Number of significantly different ASVs (adjusted p_value<0.05)\ndim(coef_ancob[coef_ancob$p_adj<0.05,])\n\n[1] 5 6\n\nsig_ancob <- rownames(coef_ancob[1:dim(coef_ancob[coef_ancob$p_adj<0.05,])[1],])\n# show the significant ASVs\nmetatoshow <- subset(metas,!metas$StandType%in%'Mixed')\ndatatoshow <- comm_rarfy[rownames(metatoshow),sig_ancob]\nannotation_col = data.frame(standtype = as.factor(metatoshow$StandType))\nrownames(annotation_col)=rownames(datatoshow)\npheatmap(t(datatoshow),scale= \"row\", annotation_col = annotation_col,cluster_cols = F)\n\n\n\n\n\n\nComparing differentially abundant taxa among methods\nIn a comparison between boreal and temperate forest stand types, ANCOM-BC and DESeq2 detected different numbers of differentially abundant ASVs. As noted earlier, there is still debate about which methods perform best for differentially abundant taxa detection. While DESeq2 and ANCOM identified differentially abundant taxa that are potentially unique to each method, there may be some ASVs that were detected as differentially abundant using both methods. This type of cross-method comparison can help us to identify statistically robust results - if taxa are differentially abundant enough to be detected by multiple methods, they should differ strongly in their abundance between groups. Let’s look at the distribution of these differentially abundant ASVs identified by both methods.\n\n# identify significantly differentially abundant ASVs according to both methods\nsig_both <- sig_des[sig_des%in%sig_ancob]\nsig_both\n\n[1] \"ASV_11\"  \"ASV_22\"  \"ASV_37\"  \"ASV_44\"  \"ASV_101\"\n\n# show the distribution of these differentially abundant ASVs\nmetatoshow <- subset(metas,!metas$StandType%in%'Mixed')\ndatatoshow <- comm_rarfy[rownames(metatoshow),sig_both]\nannotation_col = data.frame(standtype = as.factor(metatoshow$StandType))\nrownames(annotation_col)=rownames(datatoshow)\npheatmap(t(datatoshow),scale= \"row\", annotation_col = annotation_col,cluster_cols = F)\n\n\n\n\nWe can also inspect the taxonomic annotations of these differentially abundant ASVs.\n\ntaxo_rarfy[sig_both,]\n\n        Kingdom    Phylum           Class                 Order             \nASV_11  \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \nASV_22  \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Cytophagales\"    \nASV_37  \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Sphingomonadales\"\nASV_44  \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Enterobacterales\"\nASV_101 \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \n        Family              Genus                            Species         \nASV_11  \"Beijerinckiaceae\"  \"Methylobacterium-Methylorubrum\" NA              \nASV_22  \"Hymenobacteraceae\" \"Hymenobacter\"                   NA              \nASV_37  \"Sphingomonadaceae\" \"Sphingomonas\"                   NA              \nASV_44  \"Shewanellaceae\"    \"Shewanella\"                     \"algae/haliotis\"\nASV_101 \"Beijerinckiaceae\"  \"Methylocella\"                   NA              \n\n\n\n\n\nFinal steps - clean up and save data objects and workspace\nWe have now completed our ecological analyses of leaf bacterial communities. You may want to save the R workspace containing all the different data objects so that you can reload it in the future without having to re-run the analyses.\n\n## Save entire R workspace to file\nsave.image(\"Microbiome-ecological-analysis-workspace.RData\")"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html",
    "title": "Making Websites with HUGO",
    "section": "",
    "text": "I am only 10 hours of a crash course in web development ahead of you. As part of a major research project on setting a biodiversity observation network, I had to develop a prototype of a portal for the project, for biodiversity information and bunch of dashboards on biodiversity trends. Never made a website before. I know how to code in a few langages, and I know that I hate playing with boxes, menus, importing images manually, and most of all, dealing with a crash of the system and having to redo the whole thing because I made a mistake somewhere. Not that a bug when I try to compile is better, but at least it is more tractable.\nHugo made it very easily because of its fundamental feature (which is the same reason I edit papers with LaTeX): the distinction between the view and the content. Once you have set up the rules defining the visual aspects of the pages, then you can focus on the content and let the software automatically constructing the html code for you. It’s fast, accessible, scriptable and could be version-controlled. All qualities for an open and reproducible science.\nTook me a few hours to learn the basics (much harder to get the higher level skills, especially to write your own Go scripts), I took some tricks here and there in different templates and at looking what others do, and that was it I had my website. Realized that it could be a good entry level course to BIOS2 fellows and decided to turn that experience into a training workshop.\nYou will find below basic instructions to install and run a template. The following is not a full tutorial, for that I recommend simply to take time looking at the documentation provided on the Hugo page (https://gohugo.io/). I also consulted the online book Hugo in action (https://www.manning.com/books/hugo-in-action). There are many other references, all of them with goods and bads. But it’s nice to have multiple ones because sometimes the description of a concept may be obscure in one reference but better in the other and it’s by comparing and switching between them that you can make progress."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#exercise-edit-the-toml-file-to-include-your-own-information.",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#exercise-edit-the-toml-file-to-include-your-own-information.",
    "title": "Making Websites with HUGO",
    "section": "Exercise : Edit the toml file to include your own information.",
    "text": "Exercise : Edit the toml file to include your own information.\nYou may want to change the section People to Collaborators and also provide a proper reference to your on github page. You can also add or remove sections, this will affect the menu at the top of the page. For instance, you can add a blog section."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#build-for-local-development",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#build-for-local-development",
    "title": "Making Websites with HUGO",
    "section": "Build for local development",
    "text": "Build for local development\nHugo will use all of the material to generate static html files that will be displayed on your browser. The command is really easy to use to run it on your own computer, you simply have to type the following in the main folder :\nhugo server\nAnd that’s it, it compiles and you can simply open it in your browser by clicking on the adress indicated in the terminal. Congratulations for your first Hugo webste !\nThere are useful information in the terminal about the building process."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#build-for-publishing-your-website",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#build-for-publishing-your-website",
    "title": "Making Websites with HUGO",
    "section": "Build for publishing your website",
    "text": "Build for publishing your website\nThe command hugo server is very fast and useful to test your website while you develop it. But once you’ll be ready to distribute it, you’ll need all of the html files and related material to distribute the website. This is easily done with the even simpler command\nhugo\nYou will find in the directory that a new folder named public appeared, with all of the material needed to deploy the website. If you click on the index.html file, you’ll get to the home page of the website. It is interesting to open this file in your text editor, you’ll get a sense of the html code that hugo generated automatically for you. You can also take a look at other files."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#exercise",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#exercise",
    "title": "Making Websites with HUGO",
    "section": "Exercise",
    "text": "Exercise\nTake 15 minutes to remove Tim’s material and replace it by the three chapters of your thesis."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#github-user-or-organization-pages",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#github-user-or-organization-pages",
    "title": "Making Websites with HUGO",
    "section": "GitHub User or Organization Pages",
    "text": "GitHub User or Organization Pages\n\nStep-by-step Instructions\n\nCreate a  (e.g. blog) repository on GitHub. This repository will contain Hugo’s content and other source files.\nCreate a .github.io GitHub repository. This is the repository that will contain the fully rendered version of your Hugo website.\ngit clone  && cd \nPaste your existing Hugo project into the new local  repository. Make sure your website works locally (hugo server or hugo server -t ) and open your browser to http://localhost:1313.\nOnce you are happy with the results: Press Ctrl+C to kill the server Before proceeding run rm -rf public to completely remove the public directory\ngit submodule add -b main https://github.com//.github.io.git public. This creates a git submodule. Now when you run the hugo command to build your site to public, the created public directory will have a different remote origin (i.e. hosted GitHub repository).\nMake sure the baseURL in your config file is updated with: .github.io\n\n\n\nPut it Into a Script\nYou’re almost done. In order to automate next steps create a deploy.sh script. You can also make it executable with chmod +x deploy.sh.\nThe following are the contents of the deploy.sh script:\n    #!/bin/sh\n\n    # If a command fails then the deploy stops\n    set -e\n\n    printf \"\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\"\n\n    # Build the project.\n    hugo # if using a theme, replace with `hugo -t <YOURTHEME>`\n\n    # Go To Public folder\n    cd public\n\n    # Add changes to git.\n    git add .\n\n    # Commit changes.\n    msg=\"rebuilding site $(date)\"\n    if [ -n \"$*\" ]; then\n        msg=\"$*\"\n    fi\n    git commit -m \"$msg\""
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#using-a-theme",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#using-a-theme",
    "title": "Making Websites with HUGO",
    "section": "Using a theme",
    "text": "Using a theme\nIt is usually a good idea to not modify a template directly, but to have the template and the site in a separate folder. The basic concept when doing this is that the config.toml file of the site has to link to the proper folder of the theme.\nFor example\ntheme = \"template-site\"\nthemesDir = \"../..\"\nThis means that the template site is in a folder named template-site which is a parent folder of the site folder. Other options are possible.\nUsually, all the content should go in the site folder, not in the theme folder.\n\nExercise 1\n\nStart modifying the theme to make it look like a website for a Zoo. Choose your preferred color scheme by changing the style= parameter in the config.toml file.\nFeel free to download some images from unsplash and save them in the static/img folder. You can then use these images in the carrousel, as “testimonial” photos or as background images for some of the sections. You can add or remove sections from the home page by editing the config.toml file and changing the enable= parameter in the params. segment at the bottom.\nYou can also try to create a new blog entry by adding a new file in the content/blog folder. This file will have a .md extension and will be written in markdown format."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#customizing-a-theme",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#customizing-a-theme",
    "title": "Making Websites with HUGO",
    "section": "Customizing a theme",
    "text": "Customizing a theme"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#basics-of-html",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#basics-of-html",
    "title": "Making Websites with HUGO",
    "section": "Basics of HTML",
    "text": "Basics of HTML\nCore structure of an HTML page\n<!DOCTYPE html>\n<html>\n<head>\n<title>This is my great website</title>\n<style>\n.css_goes_here{\n\n}\n</style>\n</head>\n<body>\n<h1>Main title</h1>\n<div>Main content goes here</div>\n</body>\n</html>\n\nA divider, used to organize content into blocks\n<div></div>\n\n\nA span, used to organize content or text into sections with different styles. Usually on the same line.\n<span></span>\n\n\nA paragraph\n<p></p>\n\n\nHeadings at different levels\n<h1>Main title</h1>\n<h2>Second level</h2>\n<h3>Third level</h3>\n\n\nAn image\n<img src='img/image_name.jpg'>\n\n\nA link\n<a href=\"https://bios2.github.io\">Great website here!</a>"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#link-between-html-and-css",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#link-between-html-and-css",
    "title": "Making Websites with HUGO",
    "section": "Link between HTML and CSS",
    "text": "Link between HTML and CSS\n\nIn html\nid is always unique. Class is not.\n<div id=\"this-div-only\" class=\"this-type-of-div\">\nOne great div!\n</div>\n\n\nIn CSS\n“#” is applied to id and “.” is applied to class. When nothing is specified, applies to tag.\n#this-div-only{\n    font-size:24px;\n}\n\n.this-type-of-div{\n    color: #bb0000;\n}\n\ndiv{\n    display:block;\n}"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#basics-of-css",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#basics-of-css",
    "title": "Making Websites with HUGO",
    "section": "Basics of CSS",
    "text": "Basics of CSS\nW3 Schools CSS reference\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\nwidth, height\nwidth of item\n200px, 200pt, 100%, 100vw/vh\n\n\nmin-width, min-height\nminimum size of item\n200px, 200pt, 100%, 100vw\n\n\ncolor\nfont color\n#aa0000, red or rgb(255,0,0)\n\n\nbackground-color\ncolor of background\n#aa0000, red or rgb(255,0,0)\n\n\nborder-color\ncolor of border\n#aa0000, red or rgb(255,0,0)\n\n\nborder\nsize, type and color of border\n1px solid black\n\n\nmargin\nmargin around item (top right bottom left)\n1px, or 1px 2px 2px 1px\n\n\npadding\npadding within item, inside div for example\n10px\n\n\nfont-family\nname of font\nVerdana, Arial\n\n\nfont-size\nsize of text\n14px, 2em\n\n\ndisplay\nshould item be on the same line, or in a separate block?\ninline, block, inline-block, flex, …\n\n\n\n\nExercise 2\n\nCreate a file named custom.css under template-site/my-site/static/css/.\nRight-click on elements on the web page that you want to modify, then click on Inspect element and try to find CSS properties that you could modify to improve the look of the page. Then, choosing the proper class, add entries in the custom.css file that start with a dot (.) followed by the proper class names.\n\n.this-class {\n    font-size:28px;\n}"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#partials",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#partials",
    "title": "Making Websites with HUGO",
    "section": "Partials",
    "text": "Partials\nPartials are snippets of HTML code that could be reused on different places on the website. For example, you will see that the layouts/index.html file in the template-site folder lists all the partials that create the home page.\nAn important point to remember is that Hugo will look for files first in the site’s folders, and if it doesn’t find the files there, it will look for them in the theme’s folder. So site folder layouts and CSS take priority over the theme folder.\n\nExercise 3\n\nCreate a new folder template-site/my-site/layouts. In this folder, create a new file named index.html and copy the content of the template-site/layouts/index.html file into it. Remove the testimonials section from the newly created file.\nCreate a new folder template-site/my-site/layouts/partials. In this folder, create a new file named featured-species.html put the following content into it, replacing the information with the species you selected.\n\n<div class=\"featured-species\">\n<img src=\"img/species/frog.jpg\" class=\"species-image\" alt=\"\" >\n<div class=\"species-description\">\n<h3>Red-Eyed Tree Frog</h3>\n<p>This frog can be found in the tropical rain forests of Costa Rica.</p>\n</div>\n</div>\n\nThen, add this section to the index.html file created above.\n\n\n{{ partial \"featured_species.html\" . }}\n\nYou will probably need to restart the Hugo server to see the changes appear on the site.\nNow, you need to edit the CSS! In your custom.css file, add the following lines.\n\n\n.featured-species{\n    height:300px;\n    background-color: #1d1f20;\n    color:white;\n}\n\n.species-image{\n    height:300px;\n    float:left;\n}\n\n.featured-species h3{\n    color:white;\n    font-size:1.5em;\n}\n\n.species-description{\n    float:left;\n    padding:20px;\n    font-size:2em;\n}\nModify this as you see fit!"
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#now-a-bit-of-go-lang-to-make-the-featured-species-different.",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#now-a-bit-of-go-lang-to-make-the-featured-species-different.",
    "title": "Making Websites with HUGO",
    "section": "Now a bit of GO lang to make the featured species different.",
    "text": "Now a bit of GO lang to make the featured species different.\nIntroduction to Hugo templating\n\nExercise 4\n\nReplace your partial featured-species.html content with this one\n\n{{ range .Site.Data.species }}\n    {{ if eq (.enable) true }}\n            <div class=\"featured-species\">\n            <img src=\"img/species/{{ .image }}\" class=\"species-image\" alt=\"\" >\n            <div class=\"species-description\">\n            <h3>{{ .name }}</h3>\n            <p> {{ .description }}</p>\n            </div>\n            </div>\n    {{end}}\n{{end}}\n\nNow, create a new folder /template-site/my-site/data/species.\nIn this folder, create new file named frog.yaml with the following content.\n\nenable: true\nname: \"Red-eyed tree frog\"\ndescription: \"This frog can be found in the forests of Costa Rica\"\nimage: \"frog.jpg\"\n\nFind other species photos and add them to the img folder. Then you can add new .yaml files in the data/species folder for each species."
  },
  {
    "objectID": "posts/2020-12-07-making-websites-with-hugo/index.html#iframes",
    "href": "posts/2020-12-07-making-websites-with-hugo/index.html#iframes",
    "title": "Making Websites with HUGO",
    "section": "iFrames",
    "text": "iFrames\nAn iFrame is a HTML tag that essentially allows you to embed another web page inside of your site.\n\nExercise 5\nFind a Youtube video and click on the share option below the video. Find the Embed option and copy the code that starts with <iframe> to a new partial that will be shown on a new page. Surround the iframe with a div tag with class=\"video\". For example:\n<div class=\"video\">\n<iframe \nwidth=\"560\" \nheight=\"315\" \nsrc=\"https://www.youtube.com/embed/42GAn4v5MgE\" \nframeborder=\"0\" \nallow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \nallowfullscreen>\n</iframe>\n</div>\nEdit the custom.css file and add this section\n.video{\n    width:100%;\n    background-color:black;\n    text-align:center;\n}"
  },
  {
    "objectID": "posts/2021-01-22-introduction-aux-concepts-edi-en-contexte-scientifique/index.html",
    "href": "posts/2021-01-22-introduction-aux-concepts-edi-en-contexte-scientifique/index.html",
    "title": "Introduction to EDI Concepts in a Scientific Context",
    "section": "",
    "text": "Texte en français à la suite.\n\n1 Introduction to EDI Concepts in a Scientific Context\nIn 2021, the BIOS2 training program will be holding a series of training and reflection activities on equity, diversity and inclusion issues. The goal is to develop an EDI action plan for the program in order to consolidate a more inclusive, respectful and open environment.\nThe objectives of this workshop are:\n\nDefine the concepts of equity, diversity and inclusion\nIdentify the benefits and challenges of EDI in the university context\nRecongnize how to become an EDI bearer during one’s university career\nRaise awareness of intercultural communication (professional competence of tomorrow)\n\nThe workshop is developed by Agathe Riallan, Faculty Coordinator for Equity, Diversity and Inclusion (EDI) at the Faculty of Science, Université de Sherbrooke, in collaboration with Marie-José Naud, Equity, Diversity and Inclusion Advisor and Coordinator at the Centre d’études nordiques (CEN).\n\n\n    \n\n\n\n2 Introduction aux concepts EDI en contexte scientifique\nEn 2021, nous aurons une série de formations et d’activités de réflexion sur les questions d’équité, diversité et d’inclusion. Notre objectif est de mettre en place un plan d’action EDI pour le programme afin de consolider un environnement plus inclusif, respectueux et ouvert.\nLes objectifs de cet ateliers sont:\n\nDéfinir les concepts d’équité, de diversité et d’inclusion\nIdentifier les avantages et les défis de l’ÉDI en contexte universitaire\nIdentifier comment être porteuse ou porteur de l’ÉDI lors de son parcours universitaire\nSe sensibiliser à la communication interculturelle (compétence professionnelle de demain)\n\nL’atelier est développé par Agathe Riallan, Coordinatrice facultaire de l’Équité, de la Diversité et de l’Inclusion (ÉDI) de la Faculté des Sciences à Université de Sherbrooke, en collaboration avec Marie-José Naud, Conseillère en équité, diversité et inclusion et coordonnatrice au Centre d’études nordiques (CEN).\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{riallan2021,\n  author = {Agathe Riallan and Marie-José Naud},\n  title = {Introduction to {EDI} {Concepts} in a {Scientific} {Context}},\n  date = {2021-01-22},\n  url = {https://bios2.github.io/posts/2021-01-22-introduction-aux-concepts-edi-en-contexte-scientifique},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAgathe Riallan, and Marie-José Naud. 2021. “Introduction to EDI\nConcepts in a Scientific Context.” BIOS2 Education\nResources. January 22, 2021. https://bios2.github.io/posts/2021-01-22-introduction-aux-concepts-edi-en-contexte-scientifique."
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html",
    "title": "Point-count Data Analysis",
    "section": "",
    "text": "This course is aimed towards researchers analyzing field observations, who are often faced by data heterogeneities due to field sampling protocols changing from one project to another, or through time over the lifespan of projects, or trying to combine ‘legacy’ data sets with new data collected by recording units.\nSuch heterogeneities can bias analyses when data sets are integrated inadequately, or can lead to information loss when filtered and standardized to common standards. Accounting for these issues is important for better inference regarding status and trend of species and communities.\nAnalysts of such ‘messy’ data sets need to feel comfortable with manipulating the data, need a full understanding the mechanics of the models being used (i.e. critically interpreting the results and acknowledging assumptions and limitations), and should be able to make informed choices when faced with methodological challenges.\nThe course emphasizes critical thinking and active learning through hands on programming exercises. We will use publicly available data sets to demonstrate the data manipulation and analysis. We will use freely available and open-source R packages.\nThe expected outcome of the course is a solid foundation for further professional development via increased confidence in applying these methods for field observations."
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#instructor",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#instructor",
    "title": "Point-count Data Analysis",
    "section": "Instructor",
    "text": "Instructor\nDr. Peter Solymos\nBoreal Avian Modelling Project and the Alberta Biodiversity Monitoring Institute\nDepartment of Biological Sciences, University of Alberta"
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#outline",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#outline",
    "title": "Point-count Data Analysis",
    "section": "Outline",
    "text": "Outline\nEach day will consist of 3 sessions, roughly one hour each, with short breaks in between.\n\nThe video recordings from the workshop can be found on YouTube.\n\n\n\n\nSession\nTopic\nFiles\nVideos\n\n\n\n\nDay 1\nNaive techniques\n\n\n\n\n\n1. Introductions\nSlides\nVideo\n\n\n\n2. Organizing point count data\nNotes\nPart 1, Part 2\n\n\n\n3. Regression techniques\nNotes\nPart 1, Part 2\n\n\nDay 2\nBehavioral complexities\n\n\n\n\n\n1. Statistical assumptions and nuisance variables\nSlides\nVideo\n\n\n\n2. Behavioral complexities\nNotes\nbSims, Video\n\n\n\n3. Removal modeling techniques\nNotes\nVideo\n\n\n\n4. Finite mixture models and testing assumptions\nNotes\nMixtures, Testing\n\n\nDay 3\nThe detection process\n\n\n\n\n\n1. The detection process\nSlides\nVideo\n\n\n\n2. Distance sampling and density\nNotes\nVideo\n\n\n\n3. Estimating population density\nNotes\nVideo\n\n\n\n4. Assumptions\nNotes\nVideo\n\n\nDay 4\nComing full circle\n\n\n\n\n\n1. QPAD overview\nSlides\nVideo\n\n\n\n2. Models with detectability offsets\nNotes\nOffsets, Models\n\n\n\n3. Model validation and error propagation\nNotes\nValidation, Error\n\n\n\n4. Recordings, roadsides, closing remarks\nNotes\nVideo"
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#get-course-materials",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#get-course-materials",
    "title": "Point-count Data Analysis",
    "section": "Get course materials",
    "text": "Get course materials\n\nInstall required software\nFollow the instructions at the R website to download and install the most up-to-date base R version suitable for your operating system (the latest R version at the time of writing these instructions is 4.0.4).\nThen run the following script in R:\nsource(\"https://raw.githubusercontent.com/psolymos/qpad-workshop/main/src/install.R\")\nHaving RStudio is not absolutely necessary, but it will make life easier. RStudio is also available for different operating systems. Pick the open source desktop edition from here (the latest RStudio Desktop version at the time of writing these instructions is 1.4.1106).\nPrior exposure to R programming is not necessary, but knowledge of basic R object types and their manipulation (arrays, data frames, indexing) is useful for following hands-on exercises. Software Carpentry’s Data types and structures in R is a good resource to brush up your R skills.\n\n\nGet the notes\nIf you don’t want to use git:\n\nDownload the workshop archive release into a folder\nExtract the zip archive\nOpen the workshop.Rproj file in RStudio (or open any other R GUI/console and setwd() to the directory where you downloaded the file)\n(You can delete the archive)\n\nIf you want to use git: fork or clone the repository\ncd into/your/dir\ngit clone https://github.com/psolymos/qpad-workshop.git"
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#useful-resources",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#useful-resources",
    "title": "Point-count Data Analysis",
    "section": "Useful resources",
    "text": "Useful resources\n\nUsing the QPAD package to get offsets based on estimates from the Boreal Avian Modelling Project’s database\nNA-POPS: Point count Offsets for Population Sizes of North America landbirds"
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#references",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#references",
    "title": "Point-count Data Analysis",
    "section": "References",
    "text": "References\nSólymos, P., Toms, J. D., Matsuoka, S. M., Cumming, S. G., Barker, N. K. S., Thogmartin, W. E., Stralberg, D., Crosby, A. D., Dénes, F. V., Haché, S., Mahon, C. L., Schmiegelow, F. K. A., and Bayne, E. M., 2020. Lessons learned from comparing spatially explicit models and the Partners in Flight approach to estimate population sizes of boreal birds in Alberta, Canada. Condor, 122: 1-22. PDF\nSólymos, P., Matsuoka, S. M., Cumming, S. G., Stralberg, D., Fontaine, P., Schmiegelow, F. K. A., Song, S. J., and Bayne, E. M., 2018. Evaluating time-removal models for estimating availability of boreal birds during point-count surveys: sample size requirements and model complexity. Condor, 120: 765-786. PDF\nSólymos, P., Matsuoka, S. M., Stralberg, D., Barker, N. K. S., and Bayne, E. M., 2018. Phylogeny and species traits predict bird detectability. Ecography, 41: 1595-1603. PDF\nVan Wilgenburg, S. L., Sólymos, P., Kardynal, K. J. and Frey, M. D., 2017. Paired sampling standardizes point count data from humans and acoustic recorders. Avian Conservation and Ecology, 12(1):13. PDF\nYip, D. A., Leston, L., Bayne, E. M., Sólymos, P. and Grover, A., 2017. Experimentally derived detection distances from audio recordings and human observers enable integrated analysis of point count data. Avian Conservation and Ecology, 12(1):11. PDF\nSólymos, P., and Lele, S. R., 2016. Revisiting resource selection probability functions and single-visit methods: clarification and extensions. Methods in Ecology and Evolution, 7:196-205. PDF\nMatsuoka, S. M., Mahon, C. L., Handel, C. M., Sólymos, P., Bayne, E. M., Fontaine, P. C., and Ralph, C. J., 2014. Reviving common standards in point-count surveys for broad inference across studies. Condor 116:599-608. PDF\nSólymos, P., Matsuoka, S. M., Bayne, E. M., Lele, S. R., Fontaine, P., Cumming, S. G., Stralberg, D., Schmiegelow, F. K. A. & Song, S. J., 2013. Calibrating indices of avian density from non-standardized survey data: making the most of a messy situation. Methods in Ecology and Evolution 4:1047-1058. PDF\nMatsuoka, S. M., Bayne, E. M., Sólymos, P., Fontaine, P., Cumming, S. G., Schmiegelow, F. K. A., & Song, S. A., 2012. Using binomial distance-sampling models to estimate the effective detection radius of point-counts surveys across boreal Canada. Auk 129:268-282. PDF"
  },
  {
    "objectID": "posts/2021-03-25-point-count-data-analysis/index.html#license",
    "href": "posts/2021-03-25-point-count-data-analysis/index.html#license",
    "title": "Point-count Data Analysis",
    "section": "License",
    "text": "License\nThe course material is licensed under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Source code is under MIT license."
  },
  {
    "objectID": "posts/2021-07-19-glm-community-ecology/index.html",
    "href": "posts/2021-07-19-glm-community-ecology/index.html",
    "title": "Generalized Linear Models for Community Ecology",
    "section": "",
    "text": "Generalized Linear Model for Community Ecology\nPedro Peres-Neto, Concordia University\nBIOS2 workshop, May 17 to 21, 2021\nThis document was put together for the first time for this workshop.\nLet me know if you have suggestions or find any issues in the document.\n\n\n\nTentative schedule\n\nDay 1:\nIntroduction to types of data and approaches using GLMs in community ecology.\nTypes of patterns in species distributions involving trait and environmental variation.\nSimulating data as a path to understand GLMs in community ecology.\nThe simplest GLM: widely used bivariate correlations.\nThe challenges of statistical inference regarding linking different types of information from communities and species.\nUnderstanding estimators and their properties in GLMs.\nDay 2:\nFrom bivariate correlations to a variety of more complex GLMs: the case of Binomial and Poisson.\nThe role of latents in specifying GLMs for community ecology.\nThe issues underlying autocorrelation in ecological data: the cases of spatial and phylogenetic autocorrelation.\nSimple GLMM approaches (Generalized Linear Mixed Models).\nDay 3:\nMore complex GLMM approaches.\nPotential approaches for incorporating intraspecific data on traits.\nDiscussion with participants: your research interests, your questions or your data (or anything really).\n\nPhilosophy: We can’t cover everything with extreme details. I’ve chosen a level that should be interesting enough and cover many different important aspects of GLMs applied to community ecology.\nNote: I mostly apply here base functions so that participants without strong knowledge of certain packages (e.g., ggplot, dplyr) can follow the code more easily.\nQuestions: Participants should feel free to ask questions either directly or in the zoom chat. I’ve also set a good doc where participants can put questions there during the week when we are not connected. I’ll read them and try to provide an answer or cover the question somehow:\nhttps://docs.google.com/document/d/17GQvGkBFs9MmLv6Yn473_Dr1t1Ps03VdHOHMhbZhBKk/edit\n\n\n\nSimulating data\n\nSimulating a single species\nOne way to develop good intuition underlying quantitative methods is to be able to simulate data according to certain desired characteristics. We can then apply methods (GLMs here) to see how well they retrieve the data characteristics.\nLet’s start with a very simple GLM, the logistic regression for one single species. Here, for simplicity, we considered one predictor. In many ecological simulations, this single predictor is considered an “environmental gradient” containing many environmental predictors. We can consider more gradients and we will discuss that later on in the workshop.\n\nset.seed(100) # so that we all have the same results\nn.sites <- 100\nX <- rnorm(n.sites)\nb0 <- 0.5 # controls the max prob. values\nprob.presence <- 1./(1+exp(-(b0+3*X)))\nplot(prob.presence ~ X)\n\n\n\n\nThis model is pretty simple and its form is:\n\\[p=\\frac{1}{1+e^{-(\\beta_0+\\beta_1X_1)}} = \\frac{1}{1+e^{-(0.5+3X_1)}}\\]\nNow let’s generate presences and absences according to the logistic model expectation. Since is a logistic model, we use rbinom,i.e., binomial trials:\n\nDistribution <- rbinom(n.sites,1,prob.presence)\n\nView(cbind(prob.presence,Distribution))\nLet’s model the data using logistic regression:\n\nmodel <- glm(Distribution ~ X,family=binomial(link=logit))\ncoefficients(model)\n\n(Intercept)           X \n 0.09200133  3.66168492 \n\n\nView(cbind(prob.presence,Distribution,model$fitted.values))\nPlotting the predicted versus the observed presence-absence values:\n\nplot(model$fitted.values ~ Distribution)\n\n\n\n\nAt this point, we won’t cover model diagnostics. Data were simulated according to the model and, as such, assumptions hold well. Plus, this is a single-species model; and this workshop is about community data, i.e., multiple species :).\nThis is a good blog explaining how to check for assumptions of logistic regressions.\nSimulating a more realistic single species\nSpecies don’t tend to respond linearly to environmental features:\n\n\n\n\n\nNow that we understand some basics of presence-absence data, let’s concentrate on more realistic species distribution data and multi-species data. There are many ways (found in the ecological literature) in which we can simulate these type of data. Below we will generate data using a standard Gaussian model for presence-absence data according to a trait and environmental feature (we will cover abundance data later on). This is a commonly used way to simulate data. Let’s start with a single species and one environmental variable.\n\nset.seed(100) # so that we all have the same results\nn.sites <- 100\nX <- rnorm(n.sites)\noptimum <- 0.2\nniche.breadth <- 0.5\nb0 <- 1 # controls the max prob. values\nb1 <- -2\n# this is a logistic model:\nprob.presence <- 1./(1+exp(-(b0+(b1*(X-optimum)^2)/(2*niche.breadth^2)))) \n\nThis more “complex” model has the following form:\n\\[p=\\frac{1}{1+e^{-(\\beta_0+\\beta_1\\frac{(X-\\mu)^2}{2\\sigma^2})}}=\\frac{1}{1+e^{-(1+2\\frac{(X-0.2)^2}{2\\cdot0.5^2})}}\\] where \\(\\mu\\) represents the species optimum and \\(\\sigma\\) its niche breadth.\nLet’s plot these probabilities:\n\nplot(X,prob.presence,ylim=c(0,1))\n# plot optimum\nabline(v=optimum,col=\"red\")\n\n\n\n\nNow let’s simulate the distribution, i.e., presences and absences according to the model. Since is a logistic, we use rbinom,i.e., binomial trials:\n\nDistribution <- rbinom(n.sites,1,prob.presence)\n\nPlot the species distribution against the environmental variable:\n\nplot(X,prob.presence,ylim=c(0,1))\nsites.present <- which(Distribution==1)\npoints(X[sites.present],Distribution[sites.present],col=\"green\",pch=16,cex=0.5)\nsites.absent <- which(Distribution==0)\npoints(X[sites.absent],Distribution[sites.absent],col=\"red\",pch=16,cex=0.5)\n\n\n\n\nThe parameters can be then estimated from the data using a logistic regression:\n\npredictor <- cbind(X,X^2) \nmodel <- glm(Distribution ~ predictor,family=binomial(link=logit))\ncoeffs <- coefficients(model)\nb0 <- coeffs[1]\nb1 <- coeffs[2]\nb2 <- coeffs[3]\nestimated.optimum <- -b1/(2*b2) # as in ter Braak and Looman 1986\nestimated.niche.breadth <- 1/sqrt(-2*b2)\nc(estimated.optimum,estimated.niche.breadth) # estimated by the glm\n\npredictorX  predictor \n 0.3317377  0.3922625 \n\nc(optimum,niche.breadth) # set in our simulations above\n\n[1] 0.2 0.5\n\n\nWe can demonstrate computationally that the parameter estimations are unbiased as they are maximum likelihood via the GLM. Here we will be showing the sampling variation only for niche optimum and breadth. The other two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) can be placed in the code below as well, demonstrating that they are also not biased.\n\nn.samples <- 1000\nestimation.matrix <- matrix(0,n.samples,2)\ncolnames(estimation.matrix) <- c(\"optimum\",\"niche.breadth\")\n# remember that we already set the parameters for the model above, i.e., optimum and niche breadth\nfor (i in 1:n.samples){\n   X <- rnorm(n.sites)\n   prob.presence <- 1./(1+exp((-b0+((X-optimum)^2)/(2*niche.breadth^2)))) # this is a logistic model\n   Distribution <- rbinom(n.sites,1,prob.presence)\n   predictor <- cbind(X,X^2) \n   model <- glm(Distribution ~ predictor,family=binomial(link=logit))\n   coeffs <- coefficients(model)\n   intercept <- coeffs[1]\n   b1 <- coeffs[2]\n   b2 <- coeffs[3]\n   estimation.matrix[i,\"optimum\"] <- -b1/(2*b2)\n   estimation.matrix[i,\"niche.breadth\"] <- 1/sqrt(-2*b2)\n}\n\nThere may be warnings “glm.fit: fitted probabilities numerically 0 or 1 occurred”. But that is not a problem per se. It just tells us that the for some species, their models capture the distribution values perfectly. By the way, that also happens with real data.\nLet’s observe the random variation around the parameter estimates and the average values:\n\nboxplot(estimation.matrix)\nabline(h=apply(estimation.matrix,2,mean),col=\"firebrick\")\n\n\n\napply(estimation.matrix,2,mean)\n\n      optimum niche.breadth \n    0.2016913     0.4834230 \n\nc(optimum,niche.breadth) # set in our simulations above\n\n[1] 0.2 0.5\n\n\nNote how the mean values are pretty close to the true values used to generate the data. This small simulation helps one understand the principles of sampling variation and unbiased estimation.\nSimulating multiple species\nNow, let’s generalize our code to multiple species. We will create a function that allow us to make sure that all sites have at least one species present and all species are present at least in one site; this is a common (but not necessary) characteristic of data used in community ecology.\n\ngenerate_communities <- function(tolerance,E,T,n.species,n.communities){\n    repeat {\n        # generates variation in niche breadth across species\n        niche.breadth <- runif(n.species)*tolerance \n        b0 <- runif(n.species,min=-4,max=4)\n        prob.presence <- matrix(data=0,nrow=n.communities,ncol=n.species)\n        Dist.matrix <- matrix(data=0,nrow=n.communities,ncol=n.species)\n        for(j in 1:n.species){\n          # species optima are trait values; which makes sense ecologically\n          prob.presence[,j] <- 1./(1+exp((-b0[j]+((E-T[j])^2)/(2*niche.breadth[j]^2)))) \n          Dist.matrix[,j] <- rbinom(n.communities,1,prob.presence[,j])\n        }\n        n_species_c <- sum(colSums(Dist.matrix)!=0) # _c for check\n        n_communities_c <- sum(rowSums(Dist.matrix)!=0)\n        if ((n_species_c == n.species) & (n_communities_c==n.communities)){break}\n    }\n    result <- list(Dist.matrix=Dist.matrix,prob.presence=prob.presence)\n    return(result)\n}\n\nNow let’s generate a community. Note that we are using one environmental gradient and one trait. We could consider more variables (trait or environmental features) by adding terms to the logistic equation above. But for the time being, that will suffice. Note, thout, that in many ecological simulations, this single predictor is considered an “environmental gradient” containing many environmental predictors. We can consider more gradients and we will discuss that later on in the workshop.\n\nset.seed(12351) # so that we all have the same results\nn.communities <- 100\nn.species <- 50\nE <- rnorm(n.communities)\nT <- rnorm(n.species)\nDist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\nProbs <- Dist$prob.presence\nDistribution <- Dist$Dist.matrix\n\nLet’s plot the probability values across environmental values. To do that nicely, we need to order the communities according to their environmental values as follows:\n\nE.sorted <- sort(E, index.return=TRUE)\nProbs.sorted <- Probs[E.sorted$ix,]\nE.sorted <- E.sorted$x\nmatplot(E.sorted, Probs.sorted, cex.lab=1.5,cex.axis=2,lty = \"solid\", type = \"l\", pch = 1:10, cex = 0.8,\n        xlab = \"Enviroment\", ylab = \"Probability of presence\")\n\n\n\n\nAnd let’s plot presences and absences against ordered environmental and trait values:\n\nheatmap(as.matrix(Distribution),scale=\"none\",Rowv=E,Colv=T,col=c(\"white\",\"black\"),labCol=\"species\",labRow=\"communities\")\n\n\n\n\n\n\n\nClassic and simple GLMs for one trait and one environment (bivariate correlations)\n\nNow that we have a basic understanding of one GLM (logistic) and how they can model different types of community ecology data, i.e., species distributions, traits and environmental variation, we can start looking into approaches that that are used by ecologists to estimate the importance of environmental and trait variation to species distributions.\nLet’s start by calculating the simplest and widely used metric of the community weighted trait mean:\n\nCWM <- Distribution %*% T / rowSums(Distribution)\n\nFor data that are based on presence-absence, this is simply the average of species trait values within communities.\nLet’s now correlate CWM with the environment, i.e., community weighted means correlation. This is a widely used approach by ecologists:\n\nplot(CWM ~ E)\n\n\n\ncor(CWM,E)\n\n          [,1]\n[1,] 0.9296338\n\n\nAgain, the community weighted means correlation is likely the most commonly used approach with 1000s of studies having been published with it.\nAnother approach is to calculate the species weighted environment means and correlate with trait values. This approach is less common (but still quite used in the ecological literature) and is sometimes referred as to species niche centroid (SNC):\n\nSNC <- t(E %*% Distribution) / colSums(Distribution)\n\nLet’s now correlate SNC with species traits:\n\nplot(T ~ SNC)\n\n\n\ncor(T,SNC)\n\n          [,1]\n[1,] 0.7700065\n\n\nNote that the two correlations (CWM- and SNC-based) differ. That’s odd as they were both calculated on exactly the same information: the same Species Matrix, Environment and Trait. Peres-Neto, Dray & ter Braak (2017) demonstrated (mathematically) that this issue is related to the fact that although CWM and SNC are based on weights, they don’t standardized and correlate them with the proper weights. CWM is based on averages calculated based on the sum of species (richness or total abundance per community) and SNC is based on averages calculated based on the number of communities in which species are presence (prevalence) or their total abundance.\nBecause of that, we have found (Peres-Neto et al. 2017) a few undesirable properties of these two correlations (CWM and SNC-based correlations). One is that when the correlation is expected to be zero, the sampling variation of these correlations are quite large (i.e., low precision). Let’s evaluate this issue: Below we generate data with structure, but then use a false trait completely independent of the original one, thus destroying the link between trait and environmental variation.\n\nset.seed(120) # so that we all have the same results\nn.communities <- 100\nn.species <- 50\nn.samples <- 100 # set to larger later\n\nCWM.cor <- as.matrix(0,n.samples)\nfor (i in 1:n.samples){\n  E <- rnorm(n.communities)\n  T <- rnorm(n.species)\n  Dist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\n  T.false <- rnorm(n.species)\n  CWM <- Dist$Dist.matrix %*% T.false / rowSums(Dist$Dist.matrix)\n  CWM.cor[i] <- cor(CWM,E)\n}\n\nLet’s plot the correlations:\n\nboxplot(CWM.cor)\nabline(h=mean(CWM.cor),col=\"firebrick\")\n\n\n\n\nNote that the variation is quite large for correlations based on a random trait (i.e., T.false). For example, some correlations were greater than 0.7 and smaller than -0.60. We showed that although the variation is quite large, the expected value is zero; we would need 10000 or more simulations to make mean(CWM.cor) approach almost zero. A similar issue (i.e., large variation, low precision) happens for correlations based on SNO but we won’t simulate here for brevity. One can easily adapt the code above to do so though.\nWe have shown that precision is much increased when using the 4th corner statistic. Originally described in matrix form by Legendre et al. (1997), we (Peres-Neto et al. 2017) demonstrated that the 4th corner statisticit is a GLM assuming an identity link, i.e., normally distributed residuals.\nThe basis of the 4th corner correlation is that it starts by the standardization of the trait by the sum of their species abundances (or number of sites occupied for presence-absence data), and the standardization of the environment by the sum of their community abundances. The default standardization (function scale) transforms the variable (trait or environment) in a way that its mean and standard deviation are 0 and 1, respectively. A weighted standardization makes the weighted mean and weighted standard deviation to be 0 and 1, respectively.\nR doesn’t have a default function for weighted standardization. But this can be done using the follow function:\n\nstandardize_w <- function(X,w){\n   ones <- rep(1,length(w))\n   Xc <- X - ones %*% t(w)%*% X\n   Xc / ones%*%sqrt(t(ones)%*%(Xc*Xc*w)) \n} \n\nLet’s get back to the original data used to calculate CWM and SNC based correlations:\n\nset.seed(12351) # so that we all have the same results\nn.communities <- 100\nn.species <- 50\nE <- rnorm(n.communities)\nT <- rnorm(n.species)\nDist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\nDistribution <- Dist$Dist.matrix\n\nWe then standardize environment and trait by their respective abundance sums (rows for environment & columns for trait).\n\n# make distribution matrix relative to its total sum; it makes calculations easier\nDist.rel <- Distribution/sum(Distribution)\nWn <- rowSums(Dist.rel)\nWs <- colSums(Dist.rel)\nE.std_w <- standardize_w(E,Wn)\nT.std_w <-  standardize_w(T,Ws)\n\nNote: In the future, include here the calculation of the weighted mean and standard deviation of E.std_w and T.std_w to show that they are zero and one, respectively (when weighted).\nWe then calculate the community average trait (weighted standardized) or the species niche centroid (weighted standardized):\n\nCWM.w <- Dist.rel %*% T.std_w / Wn\nSNC.w <- t(E.std_w) %*% Dist.rel / Ws\n\nWe then calculate the weighted correlation between the two vectors above but their appropriate weights. In the case of CWM.w:\n\n# either using weighted correlation:\nt(CWM.w) %*% (E.std_w*Wn)\n\n          [,1]\n[1,] 0.2813709\n\n# or the same value using weighted regression:\nlm(CWM.w ~ E.std_w,weights = Wn)\n\n\nCall:\nlm(formula = CWM.w ~ E.std_w, weights = Wn)\n\nCoefficients:\n(Intercept)      E.std_w  \n  8.804e-17    2.814e-01  \n\n\nIn the case of SNC.w:\n\n# either using weighted correlation:\nSNC.w %*% (T.std_w*Ws)\n\n          [,1]\n[1,] 0.2813709\n\n# or the same value using weighted regression:\nlm(t(SNC.w)~T.std_w,weights = Ws)\n\n\nCall:\nlm(formula = t(SNC.w) ~ T.std_w, weights = Ws)\n\nCoefficients:\n(Intercept)      T.std_w  \n -5.000e-17    2.814e-01  \n\n\nNote that regardless whether SNC or CWM were used, the 4th corner correlation gives the same result, which makes sense mathematically as both correlations use the exact same information. The reason (again) that the CWM and SNC standard correlation approaches differ is because they don’t use appropriate weights in their standardization and weighted correlation.\nAnother issue to notice is that the 4th corner values are smaller than their standard CWM values. Whereas the CWM correlation was 0.9296, the 4th corner was 0.2814. The issue here is that the CWM correlation refers only to the trait variation among communities (trait beta-diversity), whereas the 4th corner refers to the total variation in traits (within, i.e., trait alpha diversity, and among communities, i.e., trait beta-diversity). This was demonstrated by algebraic proofs in Peres-Neto et al. (2017) but we won’t get into these details here.\nThis does bring an interesting point for the analysis of trait in a community ecology context. The relative trait variation among communities (i.e., total trait beta-diversity) and within communities (i.e., gamma trait diversity) can be estimated as follows:\n\n# Among communities \nAmong.Variation <- sum(diag(t(CWM.w)%*%(CWM.w* Wn))) * 100\n# Within communities \nWithin.Variation <- 100 - Among.Variation\nc(Among.Variation,Within.Variation)\n\n[1]  9.460287 90.539713\n\n\nThe standard CWM correlation is high because it pertains to only 9.46% of the total variation, whereas the 4th corner correlation pertains to all variation, i.e., both within and among. As the among communities component become large, the two correlations become somewhat more similar.\nLet’s now investigate the sampling properties of the 4th corner correlation as we did above for the CWM correlation, i.e., when the trait-environment correlation is expected to be zero:\n\nset.seed(120) # so that we all have the same results and the same communities and traits are generated as before\nn.communities <- 100\nn.species <- 50\nn.samples <- 100 # set to larger later\n\nCWM.4th.cor <- as.matrix(0,n.samples)\nfor (i in 1:n.samples){\n   E <- rnorm(n.communities)\n   T <- rnorm(n.species)\n   Dist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\n   T.false <- rnorm(n.species) # destroys the original generated relationship\n   Dist.rel <- Dist$Dist.matrix/sum(Distribution)\n   Wn <- rowSums(Dist.rel)\n   E.std_w <- standardize_w(E,Wn)\n   T.std_w.false <-  standardize_w(T.false,Ws)\n   CWM.w.false <- Dist.rel %*% T.std_w.false / Wn\n   t(CWM.w.false) %*% (E.std_w*Wn)\n   CWM.4th.cor[i] <- cor(CWM,E)\n}\n\nLet’s compare the two statistics:\n\nboxplot(cbind(CWM.cor,CWM.4th.cor))\n\n\n\n\nNote how the 4th corner correlation is a much more precise predictor around the true value of zero.\n\n\n\nStatistical hypothesis testing\n\nWe know for a while that the bivariate correlations discussed so far have elevated type I error rates based on parametric testing and under certain permutation schemes (Dray and Legendre 2008; and Dray et al. 2014). That means that when the statistical null hypothesis of no link between trait and environment will be rejected more often than the preset alpha level (significance level, e.g., 0.05 or 0.01). More recently, this was also established for more complex models (more on this later). Resolving these issues are challenging and remain a very active field of research.\nThe code so far has helped to build some intuition underlying the different bivariate correlations. We will now use a more complete utility function that allows calculating these different metrics using one single function. This function is part of Peres-Neto et al. (2017).\nDownload the utility function file:\nClick here to download it\nLoad the functions into R:\n\nsource(\"UtilityFunctions.R\")\n\nLet’s use the function that calculates a number of the bivariate correlations, which are essentially simple GLMs.\n\nset.seed(125)\nE <- rnorm(n.communities)\nT <- rnorm(n.species)\nDist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\nTraitEnv.res <- TraitEnvCor(Distribution,E,T, Chessel = TRUE)\nTraitEnv.res\n\n               CWM.cor               wCWM.cor                SNC.cor \n           0.016601826            0.054714685            0.288779046 \n              wSNC.cor           Fourthcorner         Chessel.4thcor \n           0.039080469            0.006235766            0.015313259 \n Among Wn-variance (%) Within Wn-variance (%) \n           1.298888497           98.701111503 \n\n\nIf we want to isolate the 4th corner correlation, we can simply:\n\nTraitEnv.res[\"Fourthcorner\"]\n\nFourthcorner \n 0.006235766 \n\n\nLet’s now run permutation Model 2, Model 4 and p.max for data without a link between trait and environment. We first create the data with no link:\n\nset.seed(125)\nE <- rnorm(n.communities)\nT <- rnorm(n.species)\nDist <- generate_communities(tolerance = 1.5, E, T, n.species, n.communities)\nDistribution <- Dist$Dist.matrix\nT.false <- rnorm(n.species)\nTraitEnv.res <- TraitEnvCor(Distribution,E,T.false, Chessel = FALSE)\nTraitEnv.res[\"Fourthcorner\"]\n\nFourthcorner \n  0.04168433 \n\n\nNote how the 4th corner correlation is quite low, i.e., 0.0417. Now let’s check this value with permutations:\n\nset.seed(125)\nnrepet <- 99\nobs <- TraitEnv.res[\"Fourthcorner\"]\nsim.row <- matrix(0, nrow = nrepet, ncol = 1)\nsim.col <- matrix(0, nrow = nrepet, ncol = 1)\nfor(i in 1:nrepet){\n   per.row <- sample(nrow(Distribution)) # permute communities\n   per.col <- sample(ncol(Distribution)) # permute species\n   sim.row[i] <- TraitEnvCor(Distribution,E[per.row],T.false)[\"Fourthcorner\"]\n   sim.col[i] <- TraitEnvCor(Distribution,E,T.false[per.col])[\"Fourthcorner\"]\n}\npval.row <- (length(which(abs(sim.row) >= abs(obs))) + 1) / (nrepet + 1)\npval.col <- (length(which(abs(sim.col) >= abs(obs))) + 1) / (nrepet + 1)\np.max <- max(pval.row,pval.col)\nc(pval.row,pval.col,p.max)\n\n[1] 0.01 0.47 0.47\n\n\nAs we can see, although only environmental features were important but not traits, the row permutation (across commmunities) detected the relationship as significant. Note, however, that the permutation across species did not. ter Braak et al. (2012) determined that the maximum value between the two p-values (row and column based) assures appropriate type I error rate as expected alpha.\nThe function above allows understanding the permutation procedures. That said, the utility function file has a more complete function:\n\nset.seed(125)\nCorPermutationTest(Distribution, E, T.false, nrepet = 99)\n\n                    cor prow pcol pmax\nCWM.cor      0.36605145 0.01 0.33 0.33\nwCWM.cor     0.28014652 0.01 0.44 0.44\nSNC.cor      0.14836877 0.39 0.36 0.39\nwSNC.cor     0.09492897 0.26 0.47 0.47\nFourthcorner 0.04168433 0.01 0.47 0.47\n\n\n\n\n\nBivariate correlations in practice\n\nI hope by now you are convinced that the 4th corner is a more robust metric of bivariate correlation (one trait and one environment). Here we will use the Aravo community plant data set (Massif du Grand Gabilier, France; Choler 2005) contained in the package ade4. We provide more explanation on the data in Dray et al. (2012). Here we will replicate the analysis in that paper. The data contain species abundances for 82 species distributed into 75 sites. Sites are described by 6 environmental variables: mean snowmelt date over the period 1997–1999, slope inclination, aspect, index of microscale landform, index of physical disturbance due to cryoturbation and solifluction, and an index of zoogenic disturbance due to trampling and burrowing activities of the Alpine marmot. All variables are quantitative except the landform and zoogenic disturbance indexes that are categorical variables with five and three categories, respectively. And eight quantitative functional traits (i.e., vegetative height, lateral spread, leaf elevation angle, leaf area, leaf thickness, specific leaf area, mass-based leaf nitrogen content, and seed mass) were measured on the 82 most abundant plant species (out of a total of 132 recorded species).\n\n\n\n\n\nLoad the package and the data:\n\n# install.packages(\"ade4\") in case you don't have it installed\nlibrary(ade4)\ndata(aravo)\ndim(aravo$spe)\n\n[1] 75 82\n\ndim(aravo$env)\n\n[1] 75  6\n\ndim(aravo$trait)\n\n[1] 82  8\n\n\nLet’s estimate the 4th corner correlations between each trait and environmental variable. nrept is the number of permutations and should be set to a reasonable high number (say 9999). Here we will use 999 to speed calculations. Note that all permutation tests (not only the ones for the 4th corner) include the observed correlation as part of the null distribution (i.e., permuted); hence the use of nrept as 999, i.e., 1000 possible permutations (the observed correlation is a possible permutation if we run the test infinite times; which is not possible; so we consider it as default. modeltype is set to 6, which is the largest p-value between model 2 permutation (entire communities in the distribution matrix) and model 4 (entire species in the distribution matrix). The p-max procedure is detailed in ter Braak et. 2012. Finally, p-values are adjusted using the false discovery rate for multiple testing.\n\nfour.comb.aravo.adj <- fourthcorner(aravo$env, aravo$spe,\naravo$traits, modeltype = 6, p.adjust.method.G = \"none\",\np.adjust.method.D = \"fdr\", nrepet = 999)\n\nResults can be retrieved by simply typing:\nfour.comb.aravo \nThe ‘classic’ table of results can be produced as follows. D2 indicates that the 4th corner correlation is to be used between the quantitative variable and each category of the qualitative variables. Other bivariate metrics of 4th corner association are also described in Dray and Legendre (2008) for qualitative-quantitative associations. In the default plot, blue cells correspond to negative signicant relationships while red cells correspond to positive signicant relationships (this can be modified using the argument col in the function fourthcorner).\n\nplot(four.comb.aravo.adj, alpha = 0.05, stat = \"D2\")\n\n\n\n\n\n\n\nStacking species information as a way to understand how to build more complex GLMs\n\nAlthough widely used (1000s of studies published using them), bivariate correlations are the simplest forms of GLMs for community data. That said, the 4th corner correlation can be calculated in a way that allows us (hopefully) to understand how more complex GLMs can be produced. They allow us understanding species stacking. Perhaps I should have considered this presentation before the calculations based on weights and standardizations (will inverst in the next version of the workshop). Let’s build a small data so that we understand this principle. Consider a very artificial distribution matrix with 4 communities and 4 species. It was made artificial so that we can understand well its structure:\n\nDistribution <- as.matrix(rbind(c(1,1,0,0),c(1,0,0,0),c(0,0,1,1),c(0,0,1,0)))\nDistribution\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    0    0\n[2,]    1    0    0    0\n[3,]    0    0    1    1\n[4,]    0    0    1    0\n\n\nLet’s create some traits and environmental features:\n\nT <- c(1,2,5,8)\nE <- c(10,12,100,112)\n\nNow let’s calculate its 4th corner correlation:\n\nTraitEnvCor(Distribution,E,T)[\"Fourthcorner\"]\n\nFourthcorner \n   0.8902989 \n\n\nThe 4th corner correlation is pretty high given the highly structured data. Another way to calculate a 4th corner correlation is by using what we refer to as an “inflated approach”. This approach allows understanding the structure of stacked information. This figure demonstrates the process and calculation:\n\n\n\n\n\nNext we stack species distributions, environment and trait information:\n\nn.species <- ncol(Distribution)\nn.sites <- nrow(Distribution)\nDist.stacked <- as.vector(Distribution)\nE.stacked <- rep(1, n.species) %x% E\nT.stacked <- T %x% rep(1, n.sites)  \n\nView(cbind(Dist.stacked,E.stacked,T.stacked))\nWe then eliminatate the cells for which the distribution is zero and calculate the correlation:\n\nzeros.dist <- which(Dist.stacked==0)\ncor(E.stacked[-zeros.dist],T.stacked[-zeros.dist])\n\n[1] 0.8902989\n\n\nNote: perhaps I should have started with this explanation and then move to the more complicated way of using weights. The inflated approach is in fact a way to see how weights are given.\n\n\n\nSimulating abundance data and understanding link functions in GLMs\n\nSimple model\nAlthough community ecologists commonly work with presence-absence data, abundance data are also commonly used in many approaches. Here we will use a Poisson model to simulate community data involving species distributions, traits and environment. Although link functions are used in all families of GLMs (poisson, binomial, negative binomial, gamma, etc), we will try to provide an explanation here of what they mean using abundance data. But the same rationale will apply to all families.\n\nset.seed(100) # so that we all have the same results\nn.sites <- 100\nX <- rnorm(n.sites)\nb0 <- 0.05\nb1 <- 2\nY <- exp(b0 + b1*X)\nAbundance <- rpois(n.sites,Y)\nplot(Abundance ~ X)\n\n\n\n\nThis Poisson model has the following form. Note that multiple predictors can be considered. Here, for simplicity, we considered one predictor. Again, in many ecological simulations, this single predictor is considered an “environmental gradient” containing many environmental predictors. We can consider more gradients and we will discuss that later on in the workshop.\n\\[Y={e^{(\\beta_0+\\beta_1X_1)}}=e^{(0.05+1.2X_1)}=log(Y)=0.05+1.2X_1\\] The model can be estimated as:\n\nmodel <- glm(Abundance ~ X, family=\"poisson\")\ncoefficients(model)\n\n(Intercept)           X \n 0.03095719  2.02323099 \n\nplot(model$fitted.values ~ Abundance)\n\n\n\n\nGLMs are linear models because they use link-functions to map non-linear relationships to a linear one. In this way, the link function connects the predictors in a model with the expected (mean) value of the response variable (dependent variable). In other words, the link-functions transforms the response values into new values that can be then regressed using linear approaches (Maximum Likelihood-based approaches and not simple OLS, ordinary least square approaches as in linear regression) against the X values. As we saw in the first example of the Poisson regression above, the relationship is not linear. The link-function for the Poisson distribution is ln of the response. Let’s understand this point by plotting the log(Y) and X. Note that Y has no error and Abundance has error (i.e., based on the rpois, i.e., poisson trials)\n\n# without error:\nplot(log(Y) ~ X)\n\n\n\n# with error\nplot(log(Abundance) ~ X)\n\n\n\n\nWhat does the passage “the link-function connects the predictors in a model with the expected (mean) value of the response variable (dependent variable)” mean? As you can notice, we used Poisson trials to generate error around the initial Y values. Let’s create a 100 possible trials:\n\nmult.Y <- replicate(n=100,expr=rpois(n.sites,Y))\n\nView(mult.Y)\nEach column in mult.Y contains one single trial. This would mimic, for instance, your error in estimating abundances when sampling real populations and assuming that a Poisson GLM would model your abundances across sites well. As such, in real data, obviously, we only have one “trial”. But this small demonstration hopefully helps you understand what the GLM is trying to estimate via the link-function.\nLet’s repeat the predictor X 100 times`so that it becomes compatible in size with the multiple trials; and we can then plot them:\n\nrep.X <- rep(X, times = 100)\nplot(as.vector(mult.Y) ~ rep.X,pch=16,cex=0.5,col=\"firebrick\")\n\n\n\n\nNote that larger values of abundances tend to have more error under the poisson model (which is also ecologically plausible).\nNow we can understand what the passage “the link-function connects the predictors in a model with the expected (mean) value of the response”. Let’s first increase the number of trials to 10000 and for each site (for which we have an X value), calculate its mean:\n\nmult.Y <- replicate(n=10000,expr=rpois(n.sites,Y))\nmean.mult.Y <- apply(mult.Y,1,mean)\nplot(mean.mult.Y ~ Y)\n\n\n\n\nAs we can observe, the mean across all trials (errors) equal the response variable Y, i.e., without error. Hopefully this provides a general understanding of what link functions are. A similar explanation can be given for the logistic regression (i.e., binomial error). In there we use a logit link function (transformation) instead.\nMissing predictors in GLMs as a source of error\nObviously the fit is great, particularly because we considered all the important predictors in the model. We don’t usually have all predictors in a model and this can be simulated as well. Considering the following example where two environmental predictors were used to generate the abundance data but only was used in the regression model:\n\\[p={e^{(\\beta_0+\\beta_1X_1+\\beta_1X_2)}}\\]\n\nset.seed(100) # so that we all have the same results\nn.sites <- 100\nX <- matrix(rnorm(n.sites*2),n.sites,2)\nb0 <- 2\nb1 <- 0.5\nb2 <- 1.2\nY <- exp(b0 + b1*X[,1] + b1*X[,2]) # there are more direct matricial ways to do that\nAbundance <- rpois(n.sites,Y)\nmodel <- glm(Abundance ~ X[,1], family=\"poisson\")\nmodel\n\n\nCall:  glm(formula = Abundance ~ X[, 1], family = \"poisson\")\n\nCoefficients:\n(Intercept)       X[, 1]  \n     2.0472       0.5294  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      532.1 \nResidual Deviance: 258.8    AIC: 637.7\n\nplot(model$fitted.values ~ Abundance)\n\n\n\nAIC(model)\n\n[1] 637.7152\n\n\nNote that the errors around predicted and true values are much greater because only one predictor was included in the GLM even though two predictors were important. Now considering both predictors:\n\nmodel <- glm(Abundance ~ X, family=\"poisson\")\nmodel\n\n\nCall:  glm(formula = Abundance ~ X, family = \"poisson\")\n\nCoefficients:\n(Intercept)           X1           X2  \n     1.9808       0.5300       0.4931  \n\nDegrees of Freedom: 99 Total (i.e. Null);  97 Residual\nNull Deviance:      532.1 \nResidual Deviance: 119  AIC: 500\n\nplot(model$fitted.values ~ Abundance)\n\n\n\nAIC(model)\n\n[1] 500.0011\n\n\nThis is a critical empirical (ecological) consideration because we can’t measure everything. The error is much smaller and, as a consequence, the model fit is much improved (i.e., smaller AIC values) because it considers the two predictors. It can’t be perfect because of the error related to the poisson trials that will always result in random variation (residual variation).\nA more realistic Gaussian Poisson model\nAs for the logistic model, we can also consider a more realistic Poisson model based on a Gaussian distribution:\n\\[Y={h\\cdot e^{-(\\beta_0+\\beta_1\\frac{(X-\\mu)^2}{2\\sigma^2})}}\\] As before, \\(\\mu\\) represents the species optimum and \\(\\sigma\\) its niche breadth. \\(h\\) represents the expected abundance in the optimum environmental value. Using code to simulate this model for one species; for simplicity, we will set \\(\\beta_0=0\\) and \\(\\beta_1=1\\):\n\nset.seed(110)\nX <- rnorm(n.sites)\noptimum <- 1.2\nniche.breadth <- 0.5\nh <- 104\nY <- h * exp(-(X-optimum)^2/(2*niche.breadth^2))\nAbundance <- rpois(n.sites,Y)\nplot(Abundance~X)\nabline(v=1.2,col=\"firebrick\")\n\n\n\n\nLet’s fit the Poisson model:\n\npredictor <- cbind(X,X^2) \nmodel <- glm(Abundance ~ predictor,family=\"poisson\")\nplot(model$fitted ~ X)\n\n\n\ncoeffs <- coefficients(model)\n\nAs before, parameters used to simulate the species distribution can be estimated as:\n\nb0 <- coeffs[1]\nb1 <- coeffs[2]\nb2 <- coeffs[3]\nestimated.optimum <- -b1/(2*b2) \nestimated.niche.breadth <- 1/sqrt(-2*b2)\nh <- exp(b0 + b1*estimated.optimum + b2*estimated.optimum^2)\ncbind(estimated.optimum,estimated.niche.breadth,h)\n\n           estimated.optimum estimated.niche.breadth        h\npredictorX          1.179753               0.4726181 104.8035\n\n\nNow we can generalize the code to generate abundance data for multiple species:\n\ngenerate_community_abundance <- function(tolerance,E,T,preset_nspecies,preset_ncommunities){\n    repeat {\n        # one trait, one environmental variable\n        h <- runif(preset_nspecies,min=0.3,max=1)\n        sigma <- runif(preset_nspecies)*tolerance\n        L <- matrix(data=0,nrow=preset_ncommunities,ncol=preset_nspecies)\n        for(j in 1:preset_nspecies){\n            L[,j] <- 30*h[j]*exp(-(E-T[j])^2/(2*sigma[j]^2))\n              #rpois(preset_ncommunities,30*h[j]*exp(-(E-T[j])^2/(2*sigma[j]^2)))\n        }\n        n_species_c <- sum(colSums(L)!=0) # _c for check\n        n_communities_c <- sum(rowSums(L)!=0)\n        if ((n_species_c == preset_nspecies) & (n_communities_c==preset_ncommunities)){break}\n    }\n    return(L)\n}\n\n\nset.seed(120) # so that we all have the same results\nn.communities <- 100\nn.species <- 50\nE <- rnorm(n.communities)\nT <- rnorm(n.species)\nY <- generate_community_abundance(tolerance = 1.5, E, T, n.species, n.communities)\n\nLet’s plot the expected abundance values across environmental values. To do that nicely, we need to order the communities according to their environmental values as follows:\n\nE.sorted <- sort(E, index.return=TRUE)\nY.sorted <- Y[E.sorted$ix,]\nE.sorted <- E.sorted$x\nmatplot(E.sorted, Y.sorted, cex.lab=1.5,cex.axis=2,lty = \"solid\", type = \"l\", pch = 1:10, cex = 0.8,\n        xlab = \"Enviroment\", ylab = \"Abundance\")\n\n\n\n\nY, however, has no error and to create abundance values for each species (i.e., each column of Y; MARGIN = 2) according to a poisson model we can simply:\n\nAbundances <- apply(Y,MARGIN=2,function(x) rpois(n.communities,x))\n\nLet’s plot the abundance values across environmental values. With erorr, they don’t look as nice, obviously:\n\nE.sorted <- sort(E, index.return=TRUE)\nAbundances.sorted <- Abundances[E.sorted$ix,]\nE.sorted <- E.sorted$x\nmatplot(E.sorted, Abundances.sorted, cex.lab=1.5,cex.axis=2,lty = \"solid\", type = \"l\", pch = 1:10, cex = 0.8, xlab = \"Enviroment\", ylab = \"Abundance\")\n\n\n\n\nFinally, let’s calculate the 4th corner statistics for these data:\n\nTraitEnvCor(Abundances,E,T, Chessel = TRUE)[\"Fourthcorner\"]\n\nFourthcorner \n   0.5365449 \n\n\nAnd now for the data without error, i.e., before the poisson trials:\n\nTraitEnvCor(Y,E,T, Chessel = TRUE)[\"Fourthcorner\"]\n\nFourthcorner \n   0.5333836 \n\n\nDespite the error we observed once we transformed Y (values without error) into abundances (with error via the poisson trials), the bivariate correlations are pretty similar, indicating that these metrics are robust against sampling error in abundances. Note, however, that we only used one predictor which was the one used to simulate the data to begin with; empirical data are much more complex than that.\nFinally, note that, as such, bivariate correlations are calculated in the same way regardless if the data are presence-absence or abundance; biomass data could be also considered.\n\n\n\nMoving to more complex GLMs - the bilinear model\n\nLet’s go back to our stacked model:\n\n\n\n\n\nAnd now let’s get back to our stacked approach using again the simplest example we used earlier. Let’s enter the data again to make sure that we have the same data.\n\nDistribution <- as.matrix(rbind(c(1,1,0,0),c(1,0,0,0),c(0,0,1,1),c(0,0,1,0)))\nT <- c(1,2,5,8)\nE <- c(10,12,100,112)\nn.species <- ncol(Distribution)\nn.sites <- nrow(Distribution)\nDist.stacked <- as.vector(Distribution)\nE.stacked <- rep(1, n.species) %x% scale(E)\nT.stacked <- scale(T) %x% rep(1, n.sites)\n\nThere are two ways in which we can code the analysis. Using the stacked way or using a kronecker product. The kronecker product stacks the data in the same way but it’s a bit more “cryptic” and demonstrating the stacking is then easier by using simple coding. The stacked GLM below estimates the statistical interaction between Environment and Traits only. Note that the distribution for our ficitional example above is for presence and absences, hence we will use a binomial link-function (i.e., logistic regression). This can be referred as to the bilinear model by Gabriel (1998) which is rarely used in ecology but can provide a good introduction to what “stacking” means which is critical to understand more complex GLMs.\nUsing the stacked vectors above, we have:\n\n predictor <- E.stacked * T.stacked\n model <- glm(Dist.stacked ~ predictor,family=binomial(link=logit))\n summary(model)\n\n\nCall:\nglm(formula = Dist.stacked ~ predictor, family = binomial(link = logit))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1282  -0.5101  -0.2570   0.7417   1.3162  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -0.9160     0.7681  -1.193   0.2330  \npredictor     2.4991     1.1975   2.087   0.0369 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 21.170  on 15  degrees of freedom\nResidual deviance: 13.597  on 14  degrees of freedom\nAIC: 17.597\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe stacking we saw can be easily done using matrix algebra (the early presentation is helpful to understand the principles though). To do it in algebra, we use the kronecker product as follows:\n\\[Y= T\\otimes E\\]\nAs such, the bilinear model is testing the statistical interaction between traits and environmental variables.\nwhich in R becomes:\n\npredictor2 <- T %x% E\nmodel <- glm(Dist.stacked ~ predictor,family=binomial(link=logit))\nsummary(model)\n\n\nCall:\nglm(formula = Dist.stacked ~ predictor, family = binomial(link = logit))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1282  -0.5101  -0.2570   0.7417   1.3162  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -0.9160     0.7681  -1.193   0.2330  \npredictor     2.4991     1.1975   2.087   0.0369 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 21.170  on 15  degrees of freedom\nResidual deviance: 13.597  on 14  degrees of freedom\nAIC: 17.597\n\nNumber of Fisher Scoring iterations: 5\n\n\nNote that the two ways to run the model results in the exact same estimates. Here we should interpret the slope for the model as the strength between trait and environment, i.e., \\(\\beta_1 = 2.4991\\). Obviously the bilinear model can be easily extended to any class of GLMs as well depending on the nature of the response variable (e.g., poisson, negative binomial, etc; more on these below).\nNote also that we could have considered multiple traits and environments (as long as we have enough degrees of freedom).\nStacked or bilinear model on the Aravo data set\nLet’s apply the Aravo data set we saw early here. We will reduce the environmental matrix by removing the qualitative variables. These can be easily accommodated but for the sake of speed, we will reduce it.\n\nE <- aravo$env[,c(\"Aspect\",\"Slope\",\"Snow\")]\nE <- as.matrix(E)\nT <- as.matrix(aravo$trait)\n\nNow we apply the kronecker product using the base function kronecker rather than %x% that we saw early. This function generate names for each column of the matrix (i.e., interactions between each trait and predictor). The columns contain all possible two by two (pairwise) combinations of traits and environmental features.\n\nTE <- kronecker(T, E, make.dimnames = TRUE)\ndim(TE)\n\n[1] 6150   24\n\ncolnames(TE)\n\n [1] \"Height:Aspect\" \"Height:Slope\"  \"Height:Snow\"   \"Spread:Aspect\"\n [5] \"Spread:Slope\"  \"Spread:Snow\"   \"Angle:Aspect\"  \"Angle:Slope\"  \n [9] \"Angle:Snow\"    \"Area:Aspect\"   \"Area:Slope\"    \"Area:Snow\"    \n[13] \"Thick:Aspect\"  \"Thick:Slope\"   \"Thick:Snow\"    \"SLA:Aspect\"   \n[17] \"SLA:Slope\"     \"SLA:Snow\"      \"N_mass:Aspect\" \"N_mass:Slope\" \n[21] \"N_mass:Snow\"   \"Seed:Aspect\"   \"Seed:Slope\"    \"Seed:Snow\"    \n\n\nBecause we have abundance data, let’s run the GLM using the poisson model (log link function) and the negative binomial which may work better when data are overdispersed (e.g., too many zeros, i.e., absences).\n\nDist.stacked <- as.vector(as.matrix(aravo$spe))\nColNames <- colnames(TE)\nTE <- scale(TE) # so that slopes can be compared directly to one another\ncolnames(TE) <- ColNames\nmodel.bilinear.poisson <- glm(Dist.stacked ~ TE,family=\"poisson\")\nlibrary(MASS)\nmodel.bilinear.negBinom <- glm.nb(Dist.stacked  ~ TE)\n\nLet’s compare the two models:\n\nc(BIC(model.bilinear.poisson),BIC(model.bilinear.negBinom))\n\n[1] 9310.771 8784.627\n\n\nThe BIC suggests that negative binomial fits the data better (smaller BIC, better fit).\nModel diagnostics for non-Gaussian models are challenging and often standard tools don’t provide a good way to assess quality of models. Let’s check model residuals using the more classic approach. sppVec below will be used to create a different color for each species.\n\n# create a vector of species names compatible with the stacked model:\nsppVec = rep(row.names(aravo$traits),each=nrow(aravo$spe))\nplot(residuals(model.bilinear.negBinom),log(fitted(model.bilinear.negBinom)),col=as.numeric(factor(sppVec)),xlab=\"Fitted values [log scale]\",ylab=\"residuals\")\n\n\n\n\nAs we can see, the plot is pretty bad; and that’s a common feature for GLMs.\nDunn and Smyth (1996) developed a new class of residuals that allows a much better way to diagnose whether the model fits the data well. The package DHARMa implements the approach. A tutorial for this package and the package features can be found at: <a href = “https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html”https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html.\n\n# install.packages(\"DHARMa\")\nlibrary(\"DHARMa\")\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nDunnSmyth.res <- simulateResiduals(fittedModel = model.bilinear.negBinom, plot = F)\nplot(DunnSmyth.res$scaledResiduals~log(fitted(model.bilinear.negBinom)),col=as.numeric(factor(sppVec)),xlab=\"Fitted values [log scale]\",ylab=\"residuals\")\n\n\n\n\nAnd now the Q-Q plot to assess residual normality:\n\nplotQQunif(DunnSmyth.res)\n\n\n\n\nJust for illustration, let’s compare it with the Q-Q plot for the poisson model which had smaller support:\n\nDunnSmyth.res.poisson <- simulateResiduals(fittedModel = model.bilinear.poisson, plot = F)\nplotQQunif(DunnSmyth.res.poisson)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nThe poisson model doesn’t fit as well the expected line as the negative binomial model. As such, the negative binomial model fits better also with the assumption of normality.\nWe can use the package jtools to create model summaries and visualization for GLMs. You can find a good introduction to this package here:\nhttps://cran.r-project.org/web/packages/jtools/vignettes/summ.html\n\n# install.packages(\"jtools\")\n# install.packages(\"ggstance\")\n# install.packages(\"broom.mixed\")\nlibrary(jtools)\nsumm(model.bilinear.negBinom)\n\nError in glm.control(...) : \n  unused argument (family = list(\"Negative Binomial(0.5278)\", \"log\", function (mu) \nlog(mu), function (eta) \npmax(exp(eta), .Machine$double.eps), function (mu) \nmu + mu^2/.Theta, function (y, mu, wt) \n2 * wt * (y * log(pmax(1, y)/mu) - (y + .Theta) * log((y + .Theta)/(mu + .Theta))), function (y, n, mu, wt, dev) \n{\n    term <- (y + .Theta) * log(mu + .Theta) - y * log(mu) + lgamma(y + 1) - .Theta * log(.Theta) + lgamma(.Theta) - lgamma(.Theta + y)\n    2 * sum(term * wt)\n}, function (eta) \npmax(exp(eta), .Machine$double.eps), expression({\n    if (any(y < 0)) stop(\"negative values not allowed for the negative binomial family\")\n    n <- rep(1, nobs)\n    mustart <- y + (y == 0)/6\n}), function (mu) \nall(mu > 0), function (eta) \nTRUE, function (object, nsim) \n{\n    ftd <- fitted(object)\n    rnegbin(nsim * length(ftd), ftd, .Theta)\n}))\n\n\nWarning: Something went wrong when calculating the pseudo R-squared. Returning NA\ninstead.\n\n\n\n\n\n  \n    Observations \n    6150 \n  \n  \n    Dependent variable \n    Dist.stacked \n  \n  \n    Type \n    Generalized linear model \n  \n  \n    Family \n    Negative Binomial(0.5278) \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    𝛘²(NA) \n    NA \n  \n  \n    Pseudo-R² (Cragg-Uhler) \n    NA \n  \n  \n    Pseudo-R² (McFadden) \n    NA \n  \n  \n    AIC \n    8609.80 \n  \n  \n    BIC \n    8784.63 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -1.27 \n    0.03 \n    -40.67 \n    0.00 \n  \n  \n    TEHeight:Aspect \n    0.09 \n    0.11 \n    0.81 \n    0.42 \n  \n  \n    TEHeight:Slope \n    0.08 \n    0.05 \n    1.56 \n    0.12 \n  \n  \n    TEHeight:Snow \n    -0.07 \n    0.11 \n    -0.69 \n    0.49 \n  \n  \n    TESpread:Aspect \n    0.06 \n    0.10 \n    0.56 \n    0.58 \n  \n  \n    TESpread:Slope \n    0.07 \n    0.06 \n    1.18 \n    0.24 \n  \n  \n    TESpread:Snow \n    -0.38 \n    0.10 \n    -3.82 \n    0.00 \n  \n  \n    TEAngle:Aspect \n    0.05 \n    0.11 \n    0.50 \n    0.62 \n  \n  \n    TEAngle:Slope \n    0.21 \n    0.07 \n    2.99 \n    0.00 \n  \n  \n    TEAngle:Snow \n    -0.35 \n    0.10 \n    -3.52 \n    0.00 \n  \n  \n    TEArea:Aspect \n    0.05 \n    0.12 \n    0.36 \n    0.72 \n  \n  \n    TEArea:Slope \n    0.01 \n    0.06 \n    0.16 \n    0.87 \n  \n  \n    TEArea:Snow \n    -0.24 \n    0.12 \n    -1.90 \n    0.06 \n  \n  \n    TEThick:Aspect \n    -0.01 \n    0.09 \n    -0.09 \n    0.93 \n  \n  \n    TEThick:Slope \n    0.10 \n    0.05 \n    2.09 \n    0.04 \n  \n  \n    TEThick:Snow \n    -0.17 \n    0.09 \n    -1.91 \n    0.06 \n  \n  \n    TESLA:Aspect \n    0.45 \n    0.19 \n    2.43 \n    0.02 \n  \n  \n    TESLA:Slope \n    -0.31 \n    0.14 \n    -2.19 \n    0.03 \n  \n  \n    TESLA:Snow \n    -0.42 \n    0.15 \n    -2.81 \n    0.00 \n  \n  \n    TEN_mass:Aspect \n    -0.60 \n    0.19 \n    -3.10 \n    0.00 \n  \n  \n    TEN_mass:Slope \n    -0.06 \n    0.15 \n    -0.38 \n    0.70 \n  \n  \n    TEN_mass:Snow \n    0.67 \n    0.15 \n    4.54 \n    0.00 \n  \n  \n    TESeed:Aspect \n    0.30 \n    0.11 \n    2.80 \n    0.01 \n  \n  \n    TESeed:Slope \n    0.11 \n    0.05 \n    2.21 \n    0.03 \n  \n  \n    TESeed:Snow \n    -0.50 \n    0.11 \n    -4.39 \n    0.00 \n  \n\n\n Standard errors: MLE\n\n\n\nThe package jtools offers a variety of table and graphical outputs and is worth exploring. Here we will produce a confidence interval plot for each slope. This will take a moment (one minute or less):\n\nplot_summs(model.bilinear.negBinom, scale = TRUE)\n\nError in glm.control(...) : \n  unused argument (family = list(\"Negative Binomial(0.5278)\", \"log\", function (mu) \nlog(mu), function (eta) \npmax(exp(eta), .Machine$double.eps), function (mu) \nmu + mu^2/.Theta, function (y, mu, wt) \n2 * wt * (y * log(pmax(1, y)/mu) - (y + .Theta) * log((y + .Theta)/(mu + .Theta))), function (y, n, mu, wt, dev) \n{\n    term <- (y + .Theta) * log(mu + .Theta) - y * log(mu) + lgamma(y + 1) - .Theta * log(.Theta) + lgamma(.Theta) - lgamma(.Theta + y)\n    2 * sum(term * wt)\n}, function (eta) \npmax(exp(eta), .Machine$double.eps), expression({\n    if (any(y < 0)) stop(\"negative values not allowed for the negative binomial family\")\n    n <- rep(1, nobs)\n    mustart <- y + (y == 0)/6\n}), function (mu) \nall(mu > 0), function (eta) \nTRUE, function (object, nsim) \n{\n    ftd <- fitted(object)\n    rnegbin(nsim * length(ftd), ftd, .Theta)\n}))\n\n\nWarning: Something went wrong when calculating the pseudo R-squared. Returning NA\ninstead.\n\n\nRegistered S3 methods overwritten by 'broom':\n  method            from  \n  tidy.glht         jtools\n  tidy.summary.glht jtools\n\n\nLoading required namespace: broom.mixed\n\n\n\n\n\nOne needs to be careful when interpreting these confidence intervals as they were produced parametrically and they may be biased as we discussed in the section on statistical hypothesis testing. Again, this is a strong area of development among quantitative ecologists; but we haven’t yet derived unified views on this issue.\nWe can also plot the coefficients building:\n\ncf <- coefficients(model.bilinear.negBinom)[-1] # removes the intercept\ncfInter <- matrix(t(cf),nrow=ncol(E))\ncolnames(cfInter) <- colnames(T)\nrownames(cfInter) <- colnames(E)\ncfInter\n\n            Height      Spread       Angle        Area        Thick        SLA\nAspect  0.08562528  0.05659044  0.05406318  0.04512061 -0.008000594  0.4512878\nSlope   0.08477186  0.06555292  0.20518548  0.01057746  0.101762893 -0.3083102\nSnow   -0.07428183 -0.37726596 -0.35230429 -0.23652105 -0.174573655 -0.4210057\n            N_mass       Seed\nAspect -0.59525146  0.2977853\nSlope  -0.05644061  0.1137532\nSnow    0.67458707 -0.5011741\n\n# install.packages(\"lattice\")\nlibrary(lattice)\na <- max(abs(cfInter))\ncolort = colorRampPalette(c(\"blue\",\"white\",\"red\"))\nlevelplot(cfInter, xlab=\"traits\",ylab=\"environment\", col.regions=colort(100), at=seq(-a, a, length=100))\n\n\n\n\n\n\n\nConsidering main effects of trait and environment, and their interactions - predicting and explaining species distributions\n\nThe bilinear model estimates and test for the interactions between traits and environmental features in a single model. That allows for partial slopes to be estimated (i.e., in which the effects of one trait-environment interaction is independent of the others as in standard linear regression models and GLMs). Understanding partial slopes is essential for inference and usually covered in Intro statistics for biologists/ecologists under multiple regression. A standard definition is “The partial slope in multiple regression (or GLM) is the slope of the relationship between a predictor variable that is independent of the other predictor variables and the criterion. It is also the regression coefficient for the predictor variable in question.” (https://onlinestatbook.com/2/glossary/partial_slope.html).\nNow, we may want to consider also the main effects of each environmental feature and traits in addition to their interactions. This is the model implemented in Jamil et al. (2013) and Brown et al. (2014). The Brown model et al. model is:\n\\[{ln(Y_{ij}) = \\beta_0\\ +\\beta_1env_i\\ +\\beta_2env_i^2\\ +\\beta_3spp_j\\ +\\beta_4(env \\times\\ trait)_{ij}\\ }\\] where \\(\\beta_0\\) is the overall intercept for the model, \\(\\beta_1env_i\\) contains the slopes for each environmental variable, the model also consider square terms for the environmental variables \\(\\beta_2env_i^2\\), \\(\\beta_3spp_j\\) contains species-specific intercept terms which allows predictions for each species separately, \\(\\beta_4(env \\times\\ trait)_{ij}\\) contains the slopes for each trait by environment interaction (the 4th corner slopes). Whereas Brown et al. treated \\(spp_j\\) as fixed, Jamil et al. (2013) treated as random (more on this later). I’ve followed the formulation notation given in Brown et al. to facilitate understanding their paper; but we could easily change by the notation used in the mixed model selection (following Gelman and Hill 2007).\nBy setting species-specific intercept terms (i.e., \\(\\beta_3spp_j\\)) we can predict species in their appropriate scale of abundance variation. As such, we can use these types of models to predict species distributions. This sort of modelling is becoming the standard for predicting multiple species because it considers multiple types of predictors (trait, environments, non-linearities) and their interactions. Single species models, for instance, can’t consider trait variation and the interactions of traits and environment. As such, stacked models are extremely powerful tools even for modelling single species distributions.\nThe Brown et al. model can be fit using the package mvabund as follows:\n\n# install.packages(\"mvabund\")\nlibrary(mvabund)\nglm.trait.res <- traitglm(as.matrix(aravo$spe),E,T,family=\"negative.binomial\",col.intercepts=TRUE)\nBIC(glm.trait.res)\n\n       l \n8104.818 \n\n\nAll coefficients of the model can be retrieved by:\n\ncoefficients(glm.trait.res)\n\n                         l\n(Intercept)   -1.939671753\nsppAlch.glau  -0.054831143\nsppAlch.pent   0.024330521\nsppAlch.vulg  -0.155929316\nsppAlop.alpi   0.069840352\nsppAndr.brig  -0.144086942\nsppAndr.vita  -0.117405506\nsppAnte.carp  -0.105193294\nsppAnte.dioi  -0.259135963\nsppAnth.alpe  -0.404567216\nsppAnth.nipp  -0.069393787\nsppArni.mont  -0.265964212\nsppAste.alpi  -0.324558310\nsppAven.vers  -0.108786793\nsppBart.alpi  -0.332898371\nsppCamp.sche  -0.008108304\nsppCard.alpi  -0.136528887\nsppCare.foet   0.027908430\nsppCare.parv  -0.174556575\nsppCare.rosa  -0.091371379\nsppCare.rupe  -0.182772040\nsppCare.semp  -0.112095430\nsppCera.cera  -0.171760014\nsppCera.stri  -0.021497460\nsppCirs.acau  -0.188888639\nsppDrab.aizo  -0.118690070\nsppDrya.octo  -0.358279264\nsppErig.unif  -0.059293913\nsppFest.laev  -0.365739677\nsppFest.quad  -0.105113832\nsppFest.viol  -0.058339848\nsppGent.acau  -0.181050369\nsppGent.camp  -0.134520188\nsppGent.vern  -0.044346876\nsppGeum.mont   0.064963289\nsppHeli.sede  -0.217416821\nsppHier.pili  -0.138692435\nsppHomo.alpi  -0.207003373\nsppKobr.myos  -0.028361647\nsppLeon.pyre   0.029558348\nsppLeuc.alpi  -0.003570705\nsppLigu.muto  -0.064014046\nsppLloy.sero  -0.284035626\nsppLotu.alpi  -0.144194010\nsppLuzu.lute  -0.193109208\nsppMinu.sedo   0.021602219\nsppMinu.vern  -0.137286811\nsppMyos.alpe  -0.092884602\nsppOmal.supi   0.033812911\nsppOxyt.camp  -0.233480701\nsppOxyt.lapp  -0.201732614\nsppPhyt.orbi  -0.327727501\nsppPlan.alpi   0.066971460\nsppPoa.alpi    0.095801892\nsppPoa.supi   -0.203286074\nsppPoly.vivi  -0.016324658\nsppPote.aure   0.053779431\nsppPote.cran  -0.126165407\nsppPote.gran  -0.231501706\nsppPuls.vern  -0.096096734\nsppRanu.kuep  -0.020389979\nsppSagi.glab  -0.011366535\nsppSali.herb   0.053863437\nsppSali.reti  -0.286340876\nsppSali.retu  -0.255645078\nsppSali.serp  -0.283448058\nsppSaxi.pani  -0.185168997\nsppScab.luci  -0.331712183\nsppSedu.alpe  -0.065750719\nsppSemp.mont  -0.074418068\nsppSene.inca  -0.162375900\nsppSesl.caer  -0.205203677\nsppSibb.proc   0.019139245\nsppSile.acau  -0.179836450\nsppTara.alpi  -0.250167730\nsppThym.poly  -0.280189375\nsppTrif.alpi  -0.134717635\nsppTrif.badi  -0.322563479\nsppTrif.thal  -0.225527793\nsppVero.alli  -0.293989557\nsppVero.alpi  -0.100779079\nsppVero.bell  -0.011889063\nAspect         0.014997588\nSlope          0.064347175\nSnow          -0.303043451\nAspect.squ     0.013997280\nSlope.squ     -0.047627887\nSnow.squ      -0.274070121\nAspect.Height -0.010645817\nAspect.Spread -0.070752741\nAspect.Angle  -0.083977548\nAspect.Area   -0.040276747\nAspect.Thick  -0.082064700\nAspect.SLA     0.065782084\nAspect.N_mass -0.138709436\nAspect.Seed    0.042758169\nSlope.Height  -0.020464655\nSlope.Spread   0.023128427\nSlope.Angle   -0.018699260\nSlope.Area    -0.027960323\nSlope.Thick   -0.012369639\nSlope.SLA     -0.108694248\nSlope.N_mass   0.021179434\nSlope.Seed     0.014142823\nSnow.Height   -0.209648151\nSnow.Spread    0.049505713\nSnow.Angle    -0.188496204\nSnow.Area     -0.046298194\nSnow.Thick     0.099956790\nSnow.SLA       0.309130472\nSnow.N_mass    0.317287032\nSnow.Seed     -0.169753499\n\n\nThe overall intercept \\(\\beta_0=-1.9397\\), the individual intercept for each species \\(\\beta_3spp_j\\) are the coefficients above starting with spp; for instance, the individual slope for Alch.glau (1st species in the list) is \\(\\beta_3spp_1=-0.055\\). The slopes for each environmental variable appear under the names we gave. For instance, the slope for aspect is \\(\\beta_1env_aspect=0.015\\). Note that .squ in the coefficients are the slopes for the squared environmental terms; for instance \\(\\beta_2env_{snow}^2=-0.274\\). Finally we have the 4th corner slopes. For instance, \\(\\beta_4(env \\times\\ trait)_{snow,seed\\_mass}=-0.1698\\). Remember that snow here refers to the mean snowmelt date. As such, communities with large seed masses (in average) are found in sites that have early snowmelt dates compared to sites with late snowmelt dates; these sites have communities that tend to have small seed masses in average.\nLet’s understand this model by programming it from scratch. That’s really the best way to understand models; and whenever possible I try to demonstrate them from ‘scratch’; not always possible depending on the amount of operations (e.g., mixed models) and our abilities to understand large codes . But this one is simple enough that we can do it:\n\nn.species <- ncol(aravo$spe)\nn.communities <- nrow(aravo$spe)\nn.env.variables <- ncol(E)\n# repeats each trait to make it compatible (vectorized) with the stacked species distributions\ntraitVec <- T[rep(1:n.species,each=n.communities),]\n# repeats each environmental variable to make it compatible  (vectorized) with the stacked species distributions:\nenvVec <- matrix(rep(t(E),n.species),ncol=NCOL(E),byrow=TRUE)\n# creates an intercept for each species:\nspecies.intercepts <- rep(1:n.species,each=n.communities)\nspecies.intercepts <- as.factor(species.intercepts)\nmod <- as.formula(\"~species.intercepts-1\")\nspecies.intercepts <- model.matrix(mod)[,-1]\n# the interaction terms:\nTE <- kronecker(T, E, make.dimnames = TRUE)\n# combining the predictors in a single matrix:\npreds <- cbind(species.intercepts,envVec,envVec^2,TE)\n# running the model\nmodel.bilinear.negBinom.Brown <- glm.nb(Dist.stacked  ~ preds)\n\nAnd as we can see, they are exactly the same model:\n\nBIC(glm.trait.res)\n\n       l \n8104.818 \n\nBIC(model.bilinear.negBinom.Brown)\n\n[1] 8104.818\n\n\nWe can easily adapt the graphical and diagnostic outputs for this model as well, but we won’t for simplicity.\n\nsumm(model.bilinear.negBinom)\n\nError in glm.control(...) : \n  unused argument (family = list(\"Negative Binomial(0.5278)\", \"log\", function (mu) \nlog(mu), function (eta) \npmax(exp(eta), .Machine$double.eps), function (mu) \nmu + mu^2/.Theta, function (y, mu, wt) \n2 * wt * (y * log(pmax(1, y)/mu) - (y + .Theta) * log((y + .Theta)/(mu + .Theta))), function (y, n, mu, wt, dev) \n{\n    term <- (y + .Theta) * log(mu + .Theta) - y * log(mu) + lgamma(y + 1) - .Theta * log(.Theta) + lgamma(.Theta) - lgamma(.Theta + y)\n    2 * sum(term * wt)\n}, function (eta) \npmax(exp(eta), .Machine$double.eps), expression({\n    if (any(y < 0)) stop(\"negative values not allowed for the negative binomial family\")\n    n <- rep(1, nobs)\n    mustart <- y + (y == 0)/6\n}), function (mu) \nall(mu > 0), function (eta) \nTRUE, function (object, nsim) \n{\n    ftd <- fitted(object)\n    rnegbin(nsim * length(ftd), ftd, .Theta)\n}))\n\n\nWarning: Something went wrong when calculating the pseudo R-squared. Returning NA\ninstead.\n\n\n\n\n\n  \n    Observations \n    6150 \n  \n  \n    Dependent variable \n    Dist.stacked \n  \n  \n    Type \n    Generalized linear model \n  \n  \n    Family \n    Negative Binomial(0.5278) \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    𝛘²(NA) \n    NA \n  \n  \n    Pseudo-R² (Cragg-Uhler) \n    NA \n  \n  \n    Pseudo-R² (McFadden) \n    NA \n  \n  \n    AIC \n    8609.80 \n  \n  \n    BIC \n    8784.63 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -1.27 \n    0.03 \n    -40.67 \n    0.00 \n  \n  \n    TEHeight:Aspect \n    0.09 \n    0.11 \n    0.81 \n    0.42 \n  \n  \n    TEHeight:Slope \n    0.08 \n    0.05 \n    1.56 \n    0.12 \n  \n  \n    TEHeight:Snow \n    -0.07 \n    0.11 \n    -0.69 \n    0.49 \n  \n  \n    TESpread:Aspect \n    0.06 \n    0.10 \n    0.56 \n    0.58 \n  \n  \n    TESpread:Slope \n    0.07 \n    0.06 \n    1.18 \n    0.24 \n  \n  \n    TESpread:Snow \n    -0.38 \n    0.10 \n    -3.82 \n    0.00 \n  \n  \n    TEAngle:Aspect \n    0.05 \n    0.11 \n    0.50 \n    0.62 \n  \n  \n    TEAngle:Slope \n    0.21 \n    0.07 \n    2.99 \n    0.00 \n  \n  \n    TEAngle:Snow \n    -0.35 \n    0.10 \n    -3.52 \n    0.00 \n  \n  \n    TEArea:Aspect \n    0.05 \n    0.12 \n    0.36 \n    0.72 \n  \n  \n    TEArea:Slope \n    0.01 \n    0.06 \n    0.16 \n    0.87 \n  \n  \n    TEArea:Snow \n    -0.24 \n    0.12 \n    -1.90 \n    0.06 \n  \n  \n    TEThick:Aspect \n    -0.01 \n    0.09 \n    -0.09 \n    0.93 \n  \n  \n    TEThick:Slope \n    0.10 \n    0.05 \n    2.09 \n    0.04 \n  \n  \n    TEThick:Snow \n    -0.17 \n    0.09 \n    -1.91 \n    0.06 \n  \n  \n    TESLA:Aspect \n    0.45 \n    0.19 \n    2.43 \n    0.02 \n  \n  \n    TESLA:Slope \n    -0.31 \n    0.14 \n    -2.19 \n    0.03 \n  \n  \n    TESLA:Snow \n    -0.42 \n    0.15 \n    -2.81 \n    0.00 \n  \n  \n    TEN_mass:Aspect \n    -0.60 \n    0.19 \n    -3.10 \n    0.00 \n  \n  \n    TEN_mass:Slope \n    -0.06 \n    0.15 \n    -0.38 \n    0.70 \n  \n  \n    TEN_mass:Snow \n    0.67 \n    0.15 \n    4.54 \n    0.00 \n  \n  \n    TESeed:Aspect \n    0.30 \n    0.11 \n    2.80 \n    0.01 \n  \n  \n    TESeed:Slope \n    0.11 \n    0.05 \n    2.21 \n    0.03 \n  \n  \n    TESeed:Snow \n    -0.50 \n    0.11 \n    -4.39 \n    0.00 \n  \n\n\n Standard errors: MLE\n\n\n\nAs mentioned earlier, we can get predicted models per species, making stacked models not only a community model but also considering information from multiple sources for single species distributions as well:\n\nfitted.by.species <- matrix(glm.trait.res$fitted,n.communities,n.species)\n\nPredicted abundance values per species are in columns:\nView(fitted.by.species)\n\n\n\nA very brief way to understand mixed models: the Simpson’s paradox \n\nSpecies and sites are not likely to differ from one another randomly but rather have some sites more similar to others (or more different). We call this a hierarchical structure. For instance, sites that are more close to one another may have more similar values than sites further way. Or some species may be more similar in abundance than others, and so on.\nWhen using linear models and GLMS, researchers often ignore the hierarchical structure of the data (some sites have more similar or differences in total abundances of species, some species have more similar or differences in their total abundances). As such, standard GLMs can generate biased variance estimates and increase the likelihood of committing type I errors (i.e., rejecting the statistical null hypothesis more often than set by alpha, i.e., significance level).\nAlthough this is very interesting ecologically, it does bring some inferential challenges (parameter estimation and statistical hypothesis testing) when fitting statistical models. GLMM (Generalized Linear Mixed Models) are then used to deal with these issues. This paper by Harrison et al. (2018) provides a great Introduction to GLLMs for ecologists: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/.\nOne common feature here is that species may differ in the way they are structured by environmental and/or trait variation. This can be well described by the Simpson’s paradox (Simpson 1951), which is defined “as a phenomenon in probability and statistics in which a trend appears in several groups of data but disappears or reverses when the groups are combined.”\nLet’s understand this paradox by simulating some data (code not show here) and graphing it. This sort of demonstration has become somewhat common when explaining the utility of mixed model.\n\n\n\n\n\n\n\n\nAt first glance, the influence of temperature on abundance is positive if we consider the variation across all data points independent of the sites. However, within species, the influence of the environment is negative. As such, it is obvious that there is variation among species that can’t be explained by temperature alone. As such, we should consider a mixed model with temperature as a fixed factor (measured variable) and species as a random factor. Obviously one question of interest is why do species vary in their effects of temperature. But we don’t have other predictors that could assist in explaining these differences (e.g., physiology). Perhaps considering traits could assist in determining this variation (more on that later).\nThe data was saved in a matrix called data.Simpson. For simplicity, we will treat these data as normally distributed. The data were generated assuming normality any way; the goal here is just a demonstration.\nLet’s analyze these data with a fixed model using a simple regression:\n\nlibrary(jtools)\nlm.mod <- lm(abundance ~ scale(temperature),data=data.Simpson)\nsumm(lm.mod,scale = TRUE)\n\n\n\n\n  \n    Observations \n    100 \n  \n  \n    Dependent variable \n    abundance \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,98) \n    15.13 \n  \n  \n    R² \n    0.13 \n  \n  \n    Adj. R² \n    0.12 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    -0.08 \n    0.18 \n    -0.48 \n    0.63 \n  \n  \n    `scale(temperature)` \n    0.69 \n    0.18 \n    3.89 \n    0.00 \n  \n\n\n Standard errors: OLS; Continuous predictors are mean-centered and scaled by 1 s.d.\n\n\n\nAs we can see, the overal influence of temperature is positive and significant, explaining 12% of the variation in abundance as a function of temperature (i.e., \\(R^2=0.12\\)).\nLet’s consider now a mixed effect model that we estimate the variation in intercepts but still assume a common slope for all species. This is a common procedure in mixed model effects, i.e., starting with the simplest model. This fixed effect is coded as usually and the random effect is coded as (1|species), where 1 means the intercepts (common way to code the intercept in statistical models). As such, one intercept per species is estimated. The scale=TRUE in the function summ below reports the analysis with standardized predictors.\n\n# install.packages(\"lme4\")\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlm.mod.intercept <- lmer(abundance ~ temperature + (1|species),data=data.Simpson)\nsumm(lm.mod.intercept,scale = TRUE)\n\n\n\n\n  \n    Observations \n    100 \n  \n  \n    Dependent variable \n    abundance \n  \n  \n    Type \n    Mixed effects linear regression \n  \n\n \n\n  \n    AIC \n    343.74 \n  \n  \n    BIC \n    354.16 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.30 \n  \n  \n    Pseudo-R² (total) \n    0.95 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n \n\n  \n    (Intercept) \n    -0.08 \n    1.88 \n    -0.04 \n    3.94 \n    0.97 \n  \n  \n    temperature \n    -2.84 \n    0.35 \n    -8.10 \n    97.70 \n    0.00 \n  \n\n\n  p values calculated using Kenward-Roger standard errors and d.f. ; Continuous predictors are mean-centered and scaled by 1 s.d.\n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    species \n    (Intercept) \n    4.20 \n  \n  \n    Residual \n     \n    1.16 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    species \n    5 \n    0.93 \n  \n\n\n\n\nWow, what a change in the interpretation. By considering variation in intercepts across species, the fixed effect influence of temperature is now negative (as we should expect). Note, however, that we had information on a categorical factor, i.e, species, that could be used to estimate random effects related to variation among them. The variation (standard deviation) of intercepts among species is quite large in contrast to residuals; 4.20 against 1.16, respectively. The explanatory power of variation among intercepts make the \\(R^2=0.95\\) increase dramatically in contrast to the fixed model, demonstrating that the species random effect has a huge effect and ability to improve the model predictive power. We also find the ICC (Intra Class Correlation) which measures how similar the abundance is within groups, i.e., species. The ICC is 0.93, indicating that abundance values are more similar within than among species.\nThe variation in intercepts can be plotted as follows:\n\nlibrary(ggplot2)\nintercepts <- coefficients(lm.mod.intercept)$species[,\"(Intercept)\"]\nslopes <- coefficients(lm.mod.intercept)$species[,\"temperature\"]\nlines <- data.frame(intercepts,slopes)\nlines[\"species\"] <-  unique(data.Simpson[,\"species\"])\ndata.Simpson$pred <- predict(lm.mod.intercept)\nggplot() +\ngeom_point(data=data.Simpson,aes(x=temperature,y=abundance,color=species),size=1) + \ngeom_abline(aes(intercept = `(Intercept)`, slope = temperature),size = 1.5,as.data.frame(t(fixef(lm.mod.intercept)))) +\ngeom_abline(data = lines, aes(intercept=intercepts, slope=slopes, color=species)) +\ntheme_classic() + \nxlab(\"Temperature\") + ylab(\"log(Abundance)\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe black line represents the common model. Since only intercepts were allowed to vary, the species per slope are the same as the fixed model (after controlling for variation across species, which made the overall slope to be negative).\nFinally, let’s estimate the random intercept and slope model. Here we estimate variation due to differences in intercepts and slopes across species:\n\nlm.mod.interceptSlope <- lmer(abundance ~ temperature + (1 + temperature|species),data=data.Simpson)\nsumm(lm.mod.interceptSlope,scale = TRUE)\n\n\n\n\n  \n    Observations \n    100 \n  \n  \n    Dependent variable \n    abundance \n  \n  \n    Type \n    Mixed effects linear regression \n  \n\n \n\n  \n    AIC \n    336.10 \n  \n  \n    BIC \n    351.73 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.31 \n  \n  \n    Pseudo-R² (total) \n    0.96 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n \n\n  \n    (Intercept) \n    0.11 \n    1.75 \n    0.06 \n    3.99 \n    0.95 \n  \n  \n    temperature \n    -2.91 \n    0.84 \n    -3.45 \n    3.98 \n    0.03 \n  \n\n\n  p values calculated using Kenward-Roger standard errors and d.f. ; Continuous predictors are mean-centered and scaled by 1 s.d.\n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    species \n    (Intercept) \n    3.84 \n  \n  \n    species \n    temperature \n    1.73 \n  \n  \n    Residual \n     \n    1.05 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    species \n    5 \n    0.93 \n  \n\n\n\n\nThe variation (standard deviation) in intercepts across species in much larger (3.84) than slopes (1.73). Is the predictive power between the two models significant? In order words, does a model that estimate independent slopes for each species explain more variation than one that only considers variation in intercepts? We can simply compare the BIC of both models:\n\nc(BIC(lm.mod),BIC(lm.mod.intercept),BIC(lm.mod.interceptSlope))\n\n[1] 408.7963 356.1764 353.7488\n\n\nThere is more support for the mixed model that considers variation in intercepts and slopes. One can also estimate the p-value that one model fits better than the other:\n\nanova(lm.mod.intercept,lm.mod.interceptSlope)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: data.Simpson\nModels:\nlm.mod.intercept: abundance ~ temperature + (1 | species)\nlm.mod.interceptSlope: abundance ~ temperature + (1 + temperature | species)\n                      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlm.mod.intercept         4 346.41 356.83 -169.21   338.41                     \nlm.mod.interceptSlope    6 340.30 355.93 -164.15   328.30 10.114  2   0.006365\n                        \nlm.mod.intercept        \nlm.mod.interceptSlope **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFinally, we can plot the intercept and slope variation. The common fixed effect slope has been now estimated by pooling the variation among species, hence it become negative in contrast to the original fixed effect slope.\n\nintercepts <- coefficients(lm.mod.interceptSlope)$species[,\"(Intercept)\"]\nslopes <- coefficients(lm.mod.interceptSlope)$species[,\"temperature\"]\nlines <- data.frame(intercepts,slopes)\nlines[\"species\"] <-  unique(data.Simpson[,\"species\"])\ndata.Simpson$pred <- predict(lm.mod.intercept)\nggplot() +\ngeom_point(data=data.Simpson,aes(x=temperature,y=abundance,color=species),size=2) + \ngeom_abline(aes(intercept = `(Intercept)`, slope = temperature),size = 2,as.data.frame(t(fixef(lm.mod.interceptSlope)))) +\ngeom_abline(data = lines, aes(intercept=intercepts, slope=slopes, color=species),size = 2) +\ntheme_classic() + \nxlab(\"Temperature\") + ylab(\"log(Abundance)\")\n\n\n\n\nAn extreme example pf the Simpson’s paradox\nLet’s consider (just visually) an even more extreme case. The fixed effect is very strong but the within species effect is almost zero. Hopefully the “Simpson’s” examples here provide a good intuition on the importance of mixed models.\n\n\n\n\n\n\n\n\n\nOur first GLMM applied to the fourth corner problem treating species as a random effect - the MLML1 model\n\nThere are different ways that we can account for the potential random effects in community ecology data. Here we will review a few of the latest developments.\nMLM stands for multilevel (i.e., hierarchical) models (MLM). The simplest of these models is the one introduced by Pollock et al. (2012) and is often referred in the ecological literature as MLM1 (see Miller et al. 2018). The model has the following form (following the notation of Gelman and Hill 2007; as in Miller et al. 2018). This is a model that considers species as a random effect while estimating variation in both intercepts and slopes as the last model in the previous session (Simpson’s paradox)\n\\[{ln(Y_{i}) = \\alpha\\ +a_{spp[i]}\\ +\\beta_{12}env_{site[i]}\\times trait_{spp[i]} + (\\beta_1+c_{spp[i]})env_{site[i]}+e_i}\\] \\(a,c \\sim Gaussian(0,\\sigma_a^2,\\sigma_c^2,\\rho_{ac})\\) \\(e \\sim Gaussian(0,\\sigma_e^2)\\)\nNote that we changed the notation of Brown et al. \\(ij\\) that served as an index for the \\(i^{th}\\) site and \\(j^{th}\\) species to simply one index \\(i\\) since we are using a stacked model anyway, i.e., only rows for \\(Y_{i}\\) and one column (i.e., stacked species distributions). As such, the functions \\(spp[i]\\) and \\(site[i]\\) map row i onto the corresponding species and sites. \\(\\beta_{12}\\) contains the slopes for the interactions between environment and trait. The fixed effect \\(\\alpha\\) gives the overall average abundance of species among sites (one overall intercept), and the fixed effect \\(\\beta_{1}\\) gives the mean response of species to the different environmental variables. Random effect \\(a_{spp[i]}\\) allows different species to have different overall abundance (i.e., sum of abundances across species; random intercept model across species), and random effect \\(c_{spp[i]}\\) allows different species to have different responses to the environmental variables (i.e., random slope model across species). \\(a_{spp[i]}\\) and \\(c_{spp[i]}\\) have means zero and variances \\(\\sigma_a^2\\) and \\(\\sigma_c^2\\) (referred as to hyperparameters in mixed model lingo), with \\(\\rho_{ac}\\) denoting the correlation between \\(a_{spp[i]}\\) and \\(c_{spp[i]}\\) (i.e., species intercepts and species slopes for the environment can correlate). Finally,random effect \\(e_i\\) gives observation-level variance; this is necessary here to allow for overdispersion (Harisson, 2014). We can also use the negative binomial as we saw earlier. Note that we are assuming hierarchical variation in variance and not covariance (e.g., phylogenetic and spatial autocorrelation). One step at the time.\nLet’s start by setting an appropriate data structure to run the mixed model. To make the coding more manageable we will consider here only one trait and one environment. The code can be easily generalized for multiple traits and environments. We found early a strong interaction between seed mass and snow in which species with greater seed mass tended to be found in sites with small levels of snow (i.e., a negative correlation between snow and seed mass).\nFor simplicity, let’s load the aravo data again:\n\nlibrary(ade4)\ndata(aravo)\nE <- aravo$env[,c(\"Aspect\",\"Slope\",\"Snow\")]\nE <- as.matrix(E)\nT <- as.matrix(aravo$trait)\nDist.stacked <- as.vector(as.matrix(aravo$spe))\nn.species <- ncol(aravo$spe)\nn.communities <- nrow(aravo$spe)\nn.env.variables <- ncol(E)\n\n\n# to code for observation-level variance:\nobs <- 1:(n.species*n.communities)\n# to code for species (as we saw earlier to plot residuals):\nspecies <- rep(row.names(aravo$traits),each=nrow(aravo$spe))\nsites <- rep(row.names(aravo$spe),each=ncol(aravo$spe))\n# standardizing the data:\nseed.mass <- scale(T[rep(1:n.species,each=n.communities),\"Seed\"])\nsnow.melt.days <- scale(matrix(rep(t(E[,\"Snow\"]),n.species),ncol=1,byrow=TRUE))\ndata.df <- data.frame(abundance=Dist.stacked,snow.melt.days,seed.mass,species,sites,obs)\n\nLet’s see the data frame:\nView(data.df)\nLet’s start by fitting a glm:\n\nglm.mod <- glm(abundance ~ snow.melt.days + snow.melt.days:seed.mass,data=data.df, family = \"poisson\")\nsumm(glm.mod,scale = TRUE)\n\n\n\n\n  \n    Observations \n    6150 \n  \n  \n    Dependent variable \n    abundance \n  \n  \n    Type \n    Generalized linear model \n  \n  \n    Family \n    poisson \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    𝛘²(2) \n    56.18 \n  \n  \n    Pseudo-R² (Cragg-Uhler) \n    0.01 \n  \n  \n    Pseudo-R² (McFadden) \n    0.01 \n  \n  \n    AIC \n    9421.91 \n  \n  \n    BIC \n    9442.08 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -1.17 \n    0.02 \n    -50.74 \n    0.00 \n  \n  \n    snow.melt.days \n    -0.10 \n    0.02 \n    -4.20 \n    0.00 \n  \n  \n    snow.melt.days:seed.mass \n    -0.14 \n    0.02 \n    -6.39 \n    0.00 \n  \n\n\n Standard errors: MLE; Continuous predictors are mean-centered and scaled by 1 s.d.\n\n\n\nDespite the very low \\(R^2=0.01\\), the coefficiences are all significant and negative.\nLet’s run the model. Below, both intercepts (coded as 1) and slopes for snow.melt.days (coded as env) are allowed to vary among species (i.e., (1 + env|species)) and we also allow\n\nMLM1.mod <- glmer(abundance ~ snow.melt.days + snow.melt.days:seed.mass + (1 + snow.melt.days|species) + (1 | obs), family = \"poisson\", control=glmerControl(calc.derivs=F), nAGQ = 0, data=data.df)\nsumm(MLM1.mod,scale = TRUE)\n\n\n\n\n  \n    Observations \n    6150 \n  \n  \n    Dependent variable \n    abundance \n  \n  \n    Type \n    Mixed effects generalized linear model \n  \n  \n    Family \n    poisson \n  \n  \n    Link \n    log \n  \n\n \n\n  \n    AIC \n    7335.48 \n  \n  \n    BIC \n    7382.55 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.09 \n  \n  \n    Pseudo-R² (total) \n    0.65 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    z val. \n    p \n  \n \n\n  \n    (Intercept) \n    -2.12 \n    0.17 \n    -12.37 \n    0.00 \n  \n  \n    snow.melt.days \n    -0.72 \n    0.13 \n    -5.51 \n    0.00 \n  \n  \n    snow.melt.days:seed.mass \n    -0.08 \n    0.09 \n    -0.84 \n    0.40 \n  \n\n\n  ; Continuous predictors are mean-centered and scaled by 1 s.d.\n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    obs \n    (Intercept) \n    0.47 \n  \n  \n    species \n    (Intercept) \n    1.46 \n  \n  \n    species \n    snow.melt.days \n    1.08 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    obs \n    6150 \n    0.07 \n  \n  \n    species \n    82 \n    0.63 \n  \n\n\n\n\nThe variation among observations (residuals) is not relevant here; we used it to allow for potential overdispersion in abundance (i.e., variance of abundance much greater than the mean). Note that both the intercept and slopes contribute more or less to the same amount of variation (intercept sd=1.46 and slopes = 1.08), indicating that an intercept and slope mixed model is the most appropriate model. We could have fit the two models and tested as before we saw early in the Simpson’s paradox section. Note the huge increase in \\(R^2=0.65\\), demonstrating that considering a random structure is much better. The coefficient of snow.melt.days remains negative indicating that most species and their variation are negative despite the potential for some species to increase their abundances for large values of snow.melt.days. We will see this below.\nThe residual against the predicted values provide a good indication that the model is appropriate:\n\nlibrary(DHARMa)\nDunnSmyth.res <- simulateResiduals(fittedModel = MLM1.mod, plot = F)\nplot(DunnSmyth.res$scaledResiduals~log(fitted(MLM1.mod)),col=as.numeric(factor(species)),xlab=\"Fitted values [log scale]\",ylab=\"residuals\")\n\n\n\n\nAnd now the Q-Q plot to assess residual normality:\n\nplotQQunif(DunnSmyth.res)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nNote that the Kolmogorov-Smirnov (KS) provides indication that the residuals cannot be assumed to be normal. That said, the distribution of residuals looks somewhat on the expected. The KS becomes quite significant because of the large number of data points. Finally, GLMs and GLMMs tend to be quite robust even when residuals are not normal.\nIntercepts in a poisson model are log of species abundances not explained by the predictors model, i.e., when we set them “manually” to take values of zero. Perhaps some sites are more productive than others and we did not measure trait or environmental variables that could account for the slope and intercept variation variation; but the random effects is telling us that there is something we are potentially missing to explain their variation.\nMoreover, the random effect indicates that species slopes for the environment are strongly and positively correlated with the species intercepts (\\(\\rho_{ac}=0.73\\)). We can plot the intercepts against slopes:\n\nplot(coefficients(MLM1.mod)$species[,\"(Intercept)\"],coefficients(MLM1.mod)$species[,\"snow.melt.days\"],xlab=\"species intercepts\", ylab=\"species environmental slopes\")\n\n\n\n\n\\(\\rho_{ac}\\) does not appear in summ(MLM1.mod) but can be found in summary(MLM1.mod) intead or calculated directly:\n\nsummary(MLM1.mod)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: poisson  ( log )\nFormula: abundance ~ snow.melt.days + snow.melt.days:seed.mass + (1 +  \n    snow.melt.days | species) + (1 | obs)\n   Data: data.df\nControl: glmerControl(calc.derivs = F)\n\n     AIC      BIC   logLik deviance df.resid \n  7335.5   7382.6  -3660.7   7321.5     6143 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6289 -0.4400 -0.2650 -0.0639  7.6016 \n\nRandom effects:\n Groups  Name           Variance Std.Dev. Corr\n obs     (Intercept)    0.2221   0.4712       \n species (Intercept)    2.1201   1.4560       \n         snow.melt.days 1.1671   1.0803   0.73\nNumber of obs: 6150, groups:  obs, 6150; species, 82\n\nFixed effects:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -2.11504    0.17095 -12.372  < 2e-16 ***\nsnow.melt.days           -0.71798    0.13037  -5.507 3.64e-08 ***\nsnow.melt.days:seed.mass -0.07552    0.09024  -0.837    0.403    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) snw.m.\nsnw.mlt.dys 0.730        \nsnw.mlt.d:. 0.010  0.015 \n\ncor(coefficients(MLM1.mod)$species[,\"(Intercept)\"],coefficients(MLM1.mod)$species[,\"snow.melt.days\"])\n\n[1] 0.7163168\n\n\nAlthough the fixed-only effect model run earlier (glm.mod) indicated a significant correlation between snowmelt date and seed mass in driving species distributions, this effect is no longer relevant once the random effects are considered:\n\nsummary(glm.mod)\n\n\nCall:\nglm(formula = abundance ~ snow.melt.days + snow.melt.days:seed.mass, \n    family = \"poisson\", data = data.df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2804  -0.7946  -0.7868  -0.6822   4.3659  \n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -1.16767    0.02301 -50.744  < 2e-16 ***\nsnow.melt.days           -0.09714    0.02313  -4.199 2.68e-05 ***\nsnow.melt.days:seed.mass -0.14286    0.02237  -6.387 1.70e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6552.0  on 6149  degrees of freedom\nResidual deviance: 6495.8  on 6147  degrees of freedom\nAIC: 9421.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nThis indicates that the random variation across species can account for the initial relationship between snow.melt.days and seed mass. Note that the importance of snow.melt.days remains relevant in driving species abundances across sites, but (again) not its interaction with seed mass.\nLet’s plot regression models per species:\n\nintercepts <- coefficients(MLM1.mod)$species[,\"(Intercept)\"]\nslopes <- coefficients(MLM1.mod)$species[,\"snow.melt.days\"]\nlines <- data.frame(intercepts,slopes)\nlines[\"species\"] <-  unique(data.df[,\"species\"])\ndata.df$pred <- predict(MLM1.mod)\nggplot(data=data.df) +\ngeom_point(data=data.df,aes(x=snow.melt.days,y=pred,color=species),size=1) + \ngeom_abline(data = lines, aes(intercept=intercepts, slope=slopes, color=species),size = 2) +\ngeom_abline(aes(intercept = `(Intercept)`, slope = snow.melt.days),size = 2,as.data.frame(t(fixef(MLM1.mod)))) +\ntheme_classic() + \nxlab(\"snow.melt.days\") + ylab(\"log(Abundance)\")\n\n\n\n\nFinally, the GLMM has good predictive power (relative smaller confidence intervals and large \\(R^2=0.51\\):\n\n# install.packages(\"ggeffects\")\nlibrary(ggeffects)\nplot(ggpredict(MLM1.mod, \"snow.melt.days\"))\n\n\n\n\nFinally, as we discussed during the workshop, one needs to be careful while interpreting the significance of interactions between trait and environment. There are bootstrap-based developments to do that for the MLML1 model but they have been show to have inflated type I errors (Miller et al. 2019).\nFunction glmer.nb could have been used to fit the model using the negative binomial family instead.\nHere is a good and simple introduction to mixed models for poisson regression:\nhttps://stats.idre.ucla.edu/r/faq/random-coefficient-poisson-models/\n\nOur second GLMM applied to the fourth corner problem treating species and sites as random effects - the MLML2 model\n\nJamil et al. (2013) implemented a mixed model version in which in contrast to MLM1, it adds a fixed effect term for traits is included in the model (as in Brown et al. 2014) and also an additional random effect (intercepts) for site:\n\nMLM2.mod <- glmer(abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days|species) + (1 | sites) + (1 | obs), family = \"poisson\", control=glmerControl(calc.derivs=F), nAGQ = 0, data=data.df)\nsummary(MLM2.mod,scale = TRUE)\n\nWarning in summary.merMod(MLM2.mod, scale = TRUE): additional arguments ignored\n\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 0) [glmerMod]\n Family: poisson  ( log )\nFormula: abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days |  \n    species) + (1 | sites) + (1 | obs)\n   Data: data.df\nControl: glmerControl(calc.derivs = F)\n\n     AIC      BIC   logLik deviance df.resid \n  7264.1   7324.6  -3623.0   7246.1     6141 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5851 -0.4284 -0.2557 -0.0732  8.9436 \n\nRandom effects:\n Groups  Name           Variance Std.Dev. Corr\n obs     (Intercept)    0.1472   0.3837       \n species (Intercept)    1.6523   1.2854       \n         snow.melt.days 1.0169   1.0084   0.71\n sites   (Intercept)    0.4344   0.6591       \nNumber of obs: 6150, groups:  obs, 6150; species, 82; sites, 75\n\nFixed effects:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)               -2.1239     0.1708 -12.436  < 2e-16 ***\nsnow.melt.days            -0.6805     0.1228  -5.543 2.97e-08 ***\nseed.mass                 -0.2050     0.1634  -1.255    0.210    \nsnow.melt.days:seed.mass  -0.1134     0.1281  -0.885    0.376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) snw.m. sd.mss\nsnw.mlt.dys 0.630               \nseed.mass   0.022  0.029        \nsnw.mlt.d:. 0.023  0.031  0.680 \n\n\nLet’s test the fit difference between MLM1 and MLM2:\n\nanova(MLM1.mod,MLM2.mod)\n\nData: data.df\nModels:\nMLM1.mod: abundance ~ snow.melt.days + snow.melt.days:seed.mass + (1 + snow.melt.days | species) + (1 | obs)\nMLM2.mod: abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days | species) + (1 | sites) + (1 | obs)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nMLM1.mod    7 7335.5 7382.6 -3660.7   7321.5                         \nMLM2.mod    9 7264.1 7324.6 -3623.0   7246.1 75.406  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nc(BIC(MLM1.mod),BIC(MLM2.mod))\n\n[1] 7382.551 7324.594\n\n\nWe won’t diagnostics here for brevity and the code for the MLM1 can be easily adapted here.\nAlthough the fixed effects for the trait (seed mass) and interaction (seed mass and snow.melt.day) were not significant, the random effect standard deviation for site is relative large (0.66) and improved the predictive power for abundances across species.\n\nOur last GLMM applied to the fourth corner problem treating species and sites as random effects - the MLML3 model\n\nter Braak (2019) proposed yet another version that seems to work better than the previous ones.\n\nMLM3.mod <- glmer(abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days|species) + (1 + seed.mass|sites) + (1 | obs), family = \"poisson\", control=glmerControl(calc.derivs=F), nAGQ = 0, data=data.df)\n\nLet’s test the fit difference between MLM2 and MLM3:\n\nanova(MLM2.mod,MLM3.mod)\n\nData: data.df\nModels:\nMLM2.mod: abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days | species) + (1 | sites) + (1 | obs)\nMLM3.mod: abundance ~ snow.melt.days * seed.mass + (1 + snow.melt.days | species) + (1 + seed.mass | sites) + (1 | obs)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nMLM2.mod    9 7264.1 7324.6 -3623.0   7246.1                     \nMLM3.mod   11 7267.0 7341.0 -3622.5   7245.0 1.0759  2     0.5839\n\nc(BIC(MLM2.mod),BIC(MLM3.mod))\n\n[1] 7324.594 7340.967\n\n\nFor these data, the MLM3 does not improve the MLM2.\nThis is not the end - we will keep updating this page after the workshop\nReferences (still being filled):\nAM Brown, DI Warton, NR Andrew, M Binns, G Cassis, H Gibb Methods in Ecology and Evolution 5 (4), 344-352\nCholer, P. (2005) Consistent shifts in Alpine plant traits along a mesotopographical gradient. Arctic, Antarctic, and Alpine Research, 37,444–453.\nDray, S., & Legendre, P. (2008) Testing the species traits-environment relationships : The fourth-corner problem revisited. Ecology, 89, 3400–3412.\nDunn, KP & Smyth GK (1996) Randomized quantile residuals. Journal of Computational and Graphical Statistics, 5, 1-10.\nGabriel, KR. (1998) Generalised bilinear regression. Biometrika, 85, 689-700.\nGelman, A & Hill, J (2007). Data analysis using regression and multi-level/hierarchical models. New York, NY: Cambridge University Press.\nHarrison, XA (2014). Using observation-level random effects to model overdispersion in count data in ecology and evolution. PeerJ, 2, e616.\nJamil, T., Ozinga, W. A., Kleyer, M., & ter Braak, C. J. F. (2013). Selecting traits that explain species-environment relationships: A generalized linear mixed model approach. Journal of Vegetation Science, 24, 988–1000.\nPeres-Neto, P. R., Dray, S., & ter Braak, C. J. F. (2017). Linking trait variation to the environment: Critical issues with community-weighted mean correlation resolved by the fourth-corner approach. Ecography, 40, 806–816.\nPollock, L. J., Morris, W. K., & Vesk, P. A. (2012). The role of functional traits in species distributions revealed through a hierarchical model. Ecography, 35, 716–725.\nSimpson, EH (1951). The Interpretation of Interaction in Contingency Tables. Journal of the Royal Statistical Society, Series B., 13, 238–241.\nter Braak, CJF, Cormont, A. & Dray, S. (2012). Improved testing of species traits–environment relationships in the fourth‐corner problem. Ecology, 93, 1525-1526.\nter Braak, CJF & Looman, CWN. (1986). Weighted averaging, logistic regression and the Gaussian response model. Plant Ecology, 65, 3-11.\nuseful sites for GLMs (will also expand on this later on):\n\nBeyond Multiple Linear Regression: https://bookdown.org/roback/bookdown-BeyondMLR/\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{peres-neto2021,\n  author = {Pedro Peres-Neto},\n  title = {Generalized {Linear} {Models} for {Community} {Ecology}},\n  date = {2021-05-17},\n  url = {https://bios2.github.io/posts/2021-07-19-glm-community-ecology},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPedro Peres-Neto. 2021. “Generalized Linear Models for Community\nEcology.” BIOS2 Education Resources. May 17, 2021. https://bios2.github.io/posts/2021-07-19-glm-community-ecology."
  },
  {
    "objectID": "summer-schools/BiodiversityModelling2022.html",
    "href": "summer-schools/BiodiversityModelling2022.html",
    "title": "Biodiversity Modelling 2022",
    "section": "",
    "text": "The 2022 edition of the Biodiversity Modelling Summer School was presented by Dominique Gravel and Vincent Beauregard (Université de Sherbrooke) as well as several invited experts and partners.\nIt took place from August 22 to August 26 2022 in the form of a retreat at the Jouvence Resort in Orford in the heart of the Eastern Townships.\nThe intensive summer school is supported by Université de Sherbrooke and the NSERC-CREATE Training program in computational biodiversity science and services BIOS².\n\n\nMassive, complex and sometimes real-time data offer new opportunities for biodiversity monitoring. Data science uses a variety of methods to extract knowledge from multi-dimensional information systems to address a wide range of applications and disciplines. The increasing availability of information on biodiversity and its changes also comes with a challenge of communication and representation, not only to scientists but also to policy makers and the general public. Biodiversity science is no exception and quantitative ecologists are called upon to develop communication techniques and tools to assist decision making. This process, often referred to as business intelligence, relies on a set of skills that range from consulting with partners to the production of interactive applications, through the creation of data models and analysis schemes.\nAt the end of this course, students will be able to conceptualize and develop a communication strategy based on biodiversity data to meet the needs of a partner.\nUpon completion of this course, students will be able to:\n\nFormulate an intervention plan based on business intelligence principles\nIdentify the information needs of a partner\nCharacterize the profile of data users\nSet up a data model specific to the communication strategy\nConceptualize an interactive dashboard in the form of a model\n\n\n\n\nThe course is based on a strategic communication mandate for a partner organization. Students will conduct a consultation to identify information needs and develop the appropriate data model. Sessions consist of short lectures on theoretical concepts presented by specialists from different disciplines, interspersed with specific exercises designed to practice the elements taught and apply them to their consulting problem. The projects are based on data organized and distributed by Biodiversité Québec.\n\n\n\n\n\nThe first day of the course was focused on initiating a mandate with partners.\nThe session covered:\n\nIntroduction to the course\nPresentation of partners\nConsultation workshop\n\nParticipants prepared their first deliverable: Schematization of the consultation mandate.\nView the presentation for this session below:\n\n\n\n\n\nThe second day of the course was focused on business intelligence and design, with guest expert Daniel Chamberland-Tremblay from the École de Gestion at Université de Sherbrooke.\nThe session covered:\n\nBusiness Intelligence Principles\nMike2.0 methodology\nElements of design\nDevelopment of persona and user journey\n\nParticipants prepared a deliverable: User characterization grid.\nView the presentation for this session below:\n\n\n\n\n\nThe third day of the course covered data models, biodiversity indicators, and media.\n\nData and information systems architecture\nEssential biodiversity variables\nCase study: Living Planet Index and Biodiversity Distribution Index\nData journalism and digital storytelling\n\nParticipants prepared a deliverable: Conceptualization of the data and indicators model.\nView the presentation for this session below:\n\n\n\n\n\nThe fourth day focused on interactive dashboards and data visualisation principles with guest expert Thomas Hurtut, a professor at Polytechnique Montréal and co-founder of Kashika studio.\nThe session covered:\n\nDashboard design\nFor an effective visualization\nVisualization tools and technologies\nModel design using Figma\n\nParticipants prepated a deliverable: Interactive dashboard model.\nView the presentation “Data visualization design: process and principles” by guest expert Thomas Hurtut here.\n\n\n\nOn the final day, participants completed and presented their projects to the group and to their partners.\nThe session covered:\n\nFigma workshop (continuation and end)\nPresentation of the models\nExchanges with the partners\n\nThere is no presentation for this day.\n\n\n\n\nVisit the course homepage here and learn more about the course content and format here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOS² Education resources",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nBayes for the Busy Ecologist\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nThis workshop presents one idea of a complete workflow for applied Bayesian statistics with real-world models that are actually used by biodiversity scientists.\n\n\n\n\n\n\nOct 3, 2023\n\n\nAndrew MacDonald\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Microbiome Analysis\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nThis workshop will give an overview of the theory and practice of using metabarcoding approaches to study the diversity of microbial communities. The workshop will give participants an understanding of 1) the current methods for microbiome diversity quantification using metabarcoding/amplicon sequencing approaches and 2) the normalization and diversity analysis approaches that can be used to quantify the diversity of microbial communities.\n\n\n\n\n\n\nMay 19, 2022\n\n\nSteven Kembel, Zihui Wang, Salix Dubois\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Generalized Additive Models (GAMs)\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nTo address the increase in both quantity and complexity of available data, ecologists require flexible, robust statistical models, as well as software to perform such analyses. This workshop will focus on how a single tool, the R mgcv package, can be used to fit Generalized Additive Models (GAMs) to data from a wide range of sources.\n\n\n\n\n\n\nNov 2, 2021\n\n\nEric Pedersen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Shiny Apps\n\n\n\n\n\n\n\nTechnical\n\n\nFellow contributed\n\n\nEN\n\n\n\n\nIntroduction to interactive app development with R Shiny.\n\n\n\n\n\n\nJun 22, 2021\n\n\nKatherine Hébert, Andrew MacDonald, Jake Lawlor, Vincent Bellavance\n\n\n\n\n\n\n  \n\n\n\n\nGeneralized Linear Models for Community Ecology\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nIn this workshop we will explore, discuss, and apply generalized linear models to combine information on species distributions, traits, phylogenies, environmental and landscape variation. We will also discuss inference under spatial and phylogenetic autocorrelation under fixed and random effects implementations. We will discuss technical elements and cover implementations using R.\n\n\n\n\n\n\nMay 17, 2021\n\n\nPedro Peres-Neto\n\n\n\n\n\n\n  \n\n\n\n\nBuilding R packages\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nThis practical training will cover the basics of modern package development in R with a focus on the following three aspects: (1) how to turn your code into functions, (2) how to write tests and documentation, and (3) how to share your R package on GitHub..\n\n\n\n\n\n\nMay 4, 2021\n\n\nAndrew MacDonald\n\n\n\n\n\n\n  \n\n\n\n\nPoint-count Data Analysis\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nAnalysis of point-count data in the presence of variable survey methodologies and detection error offered by Péter Sólymos to BIOS2 Fellows in March 2021.\n\n\n\n\n\n\nMar 25, 2021\n\n\nPéter Sólymos\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to EDI Concepts in a Scientific Context\n\n\n\n\n\n\n\nTransversal competencies\n\n\nFR\n\n\nEN\n\n\n\n\nA short introduction to EDI concepts in a scientific context.\n\n\n\n\n\n\nJan 22, 2021\n\n\nAgathe Riallan, Marie-José Naud\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Statistics in Ecology\n\n\n\n\n\n\n\nFR\n\n\nEN\n\n\nTechnical\n\n\n\n\nTraining session about statistical analysis of spatial data in ecology, hosted by Philippe Marchand (UQAT). | Session de formation sur l’analyse statistique des données spatiales en écologie, animée par Pr. Philippe Marchand (UQAT).\n\n\n\n\n\n\nJan 12, 2021\n\n\nPhilippe Marchand\n\n\n\n\n\n\n  \n\n\n\n\nMaking Websites with HUGO\n\n\n\n\n\n\n\nTechnical\n\n\nTransversal competencies\n\n\nEN\n\n\n\n\nThis workshop provides a general introduction to HUGO, a popular open source framework for building websites without requiring a knowledge of HTML/CSS or web programming.\n\n\n\n\n\n\nDec 7, 2020\n\n\nDominique Gravel, Guillaume Larocque\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization\n\n\n\n\n\n\n\nTechnical\n\n\nFellow contributed\n\n\nEN\n\n\n\n\nGeneral principles of visualization and graphic design, and techniques of tailored visualization. This training was developed and delivered by Alex Arkilanian and Katherine Hébert on September 21st and 22nd, 2020.\n\n\n\n\n\n\nSep 21, 2020\n\n\nAlex Arkilanian, Katherine Hébert\n\n\n\n\n\n\n  \n\n\n\n\nScience Communication\n\n\n\n\n\n\n\nCareer\n\n\nFellow contributed\n\n\nEN\n\n\n\n\nRecordings, content and handouts from a 6-hour Science Communication workshop held over two days on 15 and 16 June 2020.\n\n\n\n\n\n\nJun 15, 2020\n\n\nGracielle Higino, Katherine Hébert\n\n\n\n\n\n\n  \n\n\n\n\nSensibilisation aux réalités autochtones et recherche collaborative\n\n\n\n\n\n\n\nTransversal competencies\n\n\nFR\n\n\n\n\nSérie de deux webinaires sur la sensibilisation aux réalités autochtones et la recherche en collaboration avec les Autochtones, offert du 28 au 30 avril 2020 par Catherine-Alexandra Gagnon, PhD.\n\n\n\n\n\n\nApr 28, 2020\n\n\nCatherine-Alexandra Gagnon\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Modeling in Ecology and Evolution\n\n\n\n\n\n\n\nTechnical\n\n\nEN\n\n\n\n\nThis workshop will introduce participants to the logic behind modeling in biology, focusing on developing equations, finding equilibria, analyzing stability, and running simulations.Techniques will be illustrated with the software tools, Mathematica and Maxima. This workshop was held in two parts: January 14 and January 16, 2020.\n\n\n\n\n\n\nJan 14, 2020\n\n\nSarah P. Otto\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\nBIOS² Education resources\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayes for the Busy Ecologist\n\n\n\n\nThis workshop presents one idea of a complete workflow for applied Bayesian statistics with real-world models that are actually used by biodiversity scientists.\n\n\n\n\n\n\nOct 3, 2023\n\n\nAndrew MacDonald\n\n\n\n\n\n\n  \n\n\n\n\nBiodiversity Modelling 2022\n\n\n\n\n The 2022 edition of the Biodiversity Modelling Summer School was on the theme: Biodiversity changes and data visualization. The course took the form of a workshop during which the students, in collaboration with local organizations involved in biodiversity monitoring, developed a web platform for visualizing biodiversity changes.\n\n\n\n\n\n\nAug 22, 2022\n\n\nDominique Gravel, Vincent Beauregard\n\n\n\n\n\n\n  \n\n\n\n\nBuilding R packages\n\n\n\n\nThis practical training will cover the basics of modern package development in R with a focus on the following three aspects: (1) how to turn your code into functions, (2) how to write tests and documentation, and (3) how to share your R package on GitHub..\n\n\n\n\n\n\nMay 4, 2021\n\n\nAndrew MacDonald\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization\n\n\n\n\nGeneral principles of visualization and graphic design, and techniques of tailored visualization. This training was developed and delivered by Alex Arkilanian and Katherine Hébert on September 21st and 22nd, 2020.\n\n\n\n\n\n\nSep 21, 2020\n\n\nAlex Arkilanian, Katherine Hébert\n\n\n\n\n\n\n  \n\n\n\n\nGeneralized Linear Models for Community Ecology\n\n\n\n\nIn this workshop we will explore, discuss, and apply generalized linear models to combine information on species distributions, traits, phylogenies, environmental and landscape variation. We will also discuss inference under spatial and phylogenetic autocorrelation under fixed and random effects implementations. We will discuss technical elements and cover implementations using R.\n\n\n\n\n\n\nMay 17, 2021\n\n\nPedro Peres-Neto\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to EDI Concepts in a Scientific Context\n\n\n\n\nA short introduction to EDI concepts in a scientific context.\n\n\n\n\n\n\nJan 22, 2021\n\n\nAgathe Riallan, Marie-José Naud\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Generalized Additive Models (GAMs)\n\n\n\n\nTo address the increase in both quantity and complexity of available data, ecologists require flexible, robust statistical models, as well as software to perform such analyses. This workshop will focus on how a single tool, the R mgcv package, can be used to fit Generalized Additive Models (GAMs) to data from a wide range of sources.\n\n\n\n\n\n\nNov 2, 2021\n\n\nEric Pedersen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Microbiome Analysis\n\n\n\n\nThis workshop will give an overview of the theory and practice of using metabarcoding approaches to study the diversity of microbial communities. The workshop will give participants an understanding of 1) the current methods for microbiome diversity quantification using metabarcoding/amplicon sequencing approaches and 2) the normalization and diversity analysis approaches that can be used to quantify the diversity of microbial communities.\n\n\n\n\n\n\nMay 19, 2022\n\n\nSteven Kembel, Zihui Wang, Salix Dubois\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Shiny Apps\n\n\n\n\nIntroduction to interactive app development with R Shiny.\n\n\n\n\n\n\nJun 22, 2021\n\n\nKatherine Hébert, Andrew MacDonald, Jake Lawlor, Vincent Bellavance\n\n\n\n\n\n\n  \n\n\n\n\nMaking Websites with HUGO\n\n\n\n\nThis workshop provides a general introduction to HUGO, a popular open source framework for building websites without requiring a knowledge of HTML/CSS or web programming.\n\n\n\n\n\n\nDec 7, 2020\n\n\nDominique Gravel, Guillaume Larocque\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Modeling in Ecology and Evolution\n\n\n\n\nThis workshop will introduce participants to the logic behind modeling in biology, focusing on developing equations, finding equilibria, analyzing stability, and running simulations.Techniques will be illustrated with the software tools, Mathematica and Maxima. This workshop was held in two parts: January 14 and January 16, 2020.\n\n\n\n\n\n\nJan 14, 2020\n\n\nSarah P. Otto\n\n\n\n\n\n\n  \n\n\n\n\nPoint-count Data Analysis\n\n\n\n\nAnalysis of point-count data in the presence of variable survey methodologies and detection error offered by Péter Sólymos to BIOS2 Fellows in March 2021.\n\n\n\n\n\n\nMar 25, 2021\n\n\nPéter Sólymos\n\n\n\n\n\n\n  \n\n\n\n\nScience Communication\n\n\n\n\nRecordings, content and handouts from a 6-hour Science Communication workshop held over two days on 15 and 16 June 2020.\n\n\n\n\n\n\nJun 15, 2020\n\n\nGracielle Higino, Katherine Hébert\n\n\n\n\n\n\n  \n\n\n\n\nSensibilisation aux réalités autochtones et recherche collaborative\n\n\n\n\nSérie de deux webinaires sur la sensibilisation aux réalités autochtones et la recherche en collaboration avec les Autochtones, offert du 28 au 30 avril 2020 par Catherine-Alexandra Gagnon, PhD.\n\n\n\n\n\n\nApr 28, 2020\n\n\nCatherine-Alexandra Gagnon\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Statistics in Ecology\n\n\n\n\nTraining session about statistical analysis of spatial data in ecology, hosted by Philippe Marchand (UQAT). | Session de formation sur l’analyse statistique des données spatiales en écologie, animée par Pr. Philippe Marchand (UQAT).\n\n\n\n\n\n\nJan 12, 2021\n\n\nPhilippe Marchand\n\n\n\n\n\n\nNo matching items"
  }
]