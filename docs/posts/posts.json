[
  {
    "path": "posts/2021-03-25-point-count-data-analysis/",
    "title": "Point-count Data Analysis",
    "description": "Analysis of point-count data in the presence of variable survey methodologies and detection error offered by Peter Solymos to BIOS2 Fellows in March 2021.",
    "author": [
      {
        "name": "Peter Solymos",
        "url": {}
      }
    ],
    "date": "2021-03-25",
    "categories": [
      "Training",
      "Twelve hour"
    ],
    "contents": "\n\nContents\nInstructor\nOutline\nGet course materials\nInstall required software\nGet the notes\n\nUseful resources\nReferences\nLicense\n\n\nThis course is aimed towards researchers analyzing field observations, who are often faced by data heterogeneities due to field sampling protocols changing from one project to another, or through time over the lifespan of projects, or trying to combine ‘legacy’ data sets with new data collected by recording units.\nSuch heterogeneities can bias analyses when data sets are integrated inadequately, or can lead to information loss when filtered and standardized to common standards. Accounting for these issues is important for better inference regarding status and trend of species and communities.\nAnalysts of such ‘messy’ data sets need to feel comfortable with manipulating the data, need a full understanding the mechanics of the models being used (i.e. critically interpreting the results and acknowledging assumptions and limitations), and should be able to make informed choices when faced with methodological challenges.\nThe course emphasizes critical thinking and active learning through hands on programming exercises. We will use publicly available data sets to demonstrate the data manipulation and analysis. We will use freely available and open-source R packages.\nThe expected outcome of the course is a solid foundation for further professional development via increased confidence in applying these methods for field observations.\nInstructor\nDr. Peter SolymosBoreal Avian Modelling Project and the Alberta Biodiversity Monitoring InstituteDepartment of Biological Sciences, University of Alberta\nOutline\nEach day will consist of 3 sessions, roughly one hour each, with short breaks in between.\n\nThe video recordings from the workshop can be found on YouTube.\n\nSession\nTopic\nFiles\nVideos\nDay 1\nNaive techniques\n\n\n\n1. Introductions\nSlides\nVideo\n\n2. Organizing point count data\nNotes\nPart 1, Part 2\n\n3. Regression techniques\nNotes\nPart 1, Part 2\nDay 2\nBehavioral complexities\n\n\n\n1. Statistical assumptions and nuisance variables\nSlides\nVideo\n\n2. Behavioral complexities\nNotes\nbSims, Video\n\n3. Removal modeling techniques\nNotes\nVideo\n\n4. Finite mixture models and testing assumptions\nNotes\nMixtures, Testing\nDay 3\nThe detection process\n\n\n\n1. The detection process\nSlides\nVideo\n\n2. Distance sampling and density\nNotes\nVideo\n\n3. Estimating population density\nNotes\nVideo\n\n4. Assumptions\nNotes\nVideo\nDay 4\nComing full circle\n\n\n\n1. QPAD overview\nSlides\nVideo\n\n2. Models with detectability offsets\nNotes\nOffsets, Models\n\n3. Model validation and error propagation\nNotes\nValidation, Error\n\n4. Recordings, roadsides, closing remarks\nNotes\nVideo\nGet course materials\nInstall required software\nFollow the instructions at the R website to download and install the most up-to-date base R version suitable for your operating system (the latest R version at the time of writing these instructions is 4.0.4).\nThen run the following script in R:\nsource(\"https://raw.githubusercontent.com/psolymos/qpad-workshop/main/src/install.R\")\nHaving RStudio is not absolutely necessary, but it will make life easier. RStudio is also available for different operating systems. Pick the open source desktop edition from here (the latest RStudio Desktop version at the time of writing these instructions is 1.4.1106).\nPrior exposure to R programming is not necessary, but knowledge of basic R object types and their manipulation (arrays, data frames, indexing) is useful for following hands-on exercises. Software Carpentry’s Data types and structures in R is a good resource to brush up your R skills.\nGet the notes\nIf you don’t want to use git:\nDownload the workshop archive release into a folder\nExtract the zip archive\nOpen the workshop.Rproj file in RStudio (or open any other R GUI/console and setwd() to the directory where you downloaded the file)\n(You can delete the archive)\nIf you want to use git: fork or clone the repository\ncd into/your/dir\ngit clone https://github.com/psolymos/qpad-workshop.git\nUseful resources\nUsing the QPAD package to get offsets based on estimates from the Boreal Avian Modelling Project’s database\nNA-POPS: Point count Offsets for Population Sizes of North America landbirds\nReferences\nSólymos, P., Toms, J. D., Matsuoka, S. M., Cumming, S. G., Barker, N. K. S., Thogmartin, W. E., Stralberg, D., Crosby, A. D., Dénes, F. V., Haché, S., Mahon, C. L., Schmiegelow, F. K. A., and Bayne, E. M., 2020. Lessons learned from comparing spatially explicit models and the Partners in Flight approach to estimate population sizes of boreal birds in Alberta, Canada. Condor, 122: 1-22. PDF\nSólymos, P., Matsuoka, S. M., Cumming, S. G., Stralberg, D., Fontaine, P., Schmiegelow, F. K. A., Song, S. J., and Bayne, E. M., 2018. Evaluating time-removal models for estimating availability of boreal birds during point-count surveys: sample size requirements and model complexity. Condor, 120: 765-786. PDF\nSólymos, P., Matsuoka, S. M., Stralberg, D., Barker, N. K. S., and Bayne, E. M., 2018. Phylogeny and species traits predict bird detectability. Ecography, 41: 1595-1603. PDF\nVan Wilgenburg, S. L., Sólymos, P., Kardynal, K. J. and Frey, M. D., 2017. Paired sampling standardizes point count data from humans and acoustic recorders. Avian Conservation and Ecology, 12(1):13. PDF\nYip, D. A., Leston, L., Bayne, E. M., Sólymos, P. and Grover, A., 2017. Experimentally derived detection distances from audio recordings and human observers enable integrated analysis of point count data. Avian Conservation and Ecology, 12(1):11. PDF\nSólymos, P., and Lele, S. R., 2016. Revisiting resource selection probability functions and single-visit methods: clarification and extensions. Methods in Ecology and Evolution, 7:196-205. PDF\nMatsuoka, S. M., Mahon, C. L., Handel, C. M., Sólymos, P., Bayne, E. M., Fontaine, P. C., and Ralph, C. J., 2014. Reviving common standards in point-count surveys for broad inference across studies. Condor 116:599-608. PDF\nSólymos, P., Matsuoka, S. M., Bayne, E. M., Lele, S. R., Fontaine, P., Cumming, S. G., Stralberg, D., Schmiegelow, F. K. A. & Song, S. J., 2013. Calibrating indices of avian density from non-standardized survey data: making the most of a messy situation. Methods in Ecology and Evolution 4:1047-1058. PDF\nMatsuoka, S. M., Bayne, E. M., Sólymos, P., Fontaine, P., Cumming, S. G., Schmiegelow, F. K. A., & Song, S. A., 2012. Using binomial distance-sampling models to estimate the effective detection radius of point-counts surveys across boreal Canada. Auk 129:268-282. PDF\nLicense\nThe course material is licensed under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Source code is under MIT license.\n\n\n\n",
    "preview": "posts/2021-03-25-point-count-data-analysis/thumb.jpg",
    "last_modified": "2021-04-19T18:58:25+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-21-spatial-statistics-in-ecology-part-4/",
    "title": "Spatial Statistics in Ecology, Part 4",
    "description": "Introduction to spatial statistics offered by Philippe Marchand to BIOS2 Fellows in January 2021. In the previous parts, we saw how to account for spatial dependence in linear regression models with either geostatistical models (also called Gaussian processes) or spatial autocorrelation models (CAR/SAR). In this last part, we will see how to combine these features with more complex regression models, in particular generalized linear mixed models (GLMM).",
    "author": [
      {
        "name": "Philippe Marchand",
        "url": {}
      }
    ],
    "date": "2021-01-21",
    "categories": [
      "Technical",
      "English"
    ],
    "contents": "\n\nContents\nGLMM with spatial Gaussian process\nData\nNon-spatial GLMM\nSpatial GLMM with spaMM\nGaussian process models vs. smoothing splines\nBayesian methods for GLMMs with Gaussian processes\n\nGLMM with spatial autoregression\nReference\n\nGLMM with spatial Gaussian process\nData\nThe gambia dataset found in the geoR package presents the results of a study of malaria prevalence among children of 65 villages in The Gambia. We will use a slightly transformed version of the data found in the file gambia.csv.\n\n\nlibrary(geoR)\n\ngambia <- read.csv(\"data/gambia.csv\")\nhead(gambia)\n\n\n  id_village        x        y pos  age netuse treated green phc\n1          1 349.6313 1458.055   1 1783      0       0 40.85   1\n2          1 349.6313 1458.055   0  404      1       0 40.85   1\n3          1 349.6313 1458.055   0  452      1       0 40.85   1\n4          1 349.6313 1458.055   1  566      1       0 40.85   1\n5          1 349.6313 1458.055   0  598      1       0 40.85   1\n6          1 349.6313 1458.055   1  590      1       0 40.85   1\n\nHere are the fields in that dataset:\nid_village: Identifier of the village.\nx and y: Spatial coordinates of the village (in kilometers, based on UTM coordinates).\npos: Binary response, whether the child tested positive for malaria.\nage: Age of the child in days.\nnetuse: Whether or not the child sleeps under a bed net.\ntreated: Whether or not the bed net is treated.\ngreen: Remote sensing based measure of greenness of vegetation (measured at the village level).\nphc: Presence or absence of a public health centre for the village.\nWe can count the number of positive cases and total children tested by village to map the fraction of positive cases (or prevalence, prev).\n\n\n# Create village-level dataset\ngambia_agg <- group_by(gambia, id_village, x, y, green, phc) %>%\n    summarize(pos = sum(pos), total = n()) %>%\n    mutate(prev = pos / total) %>%\n    ungroup()\nhead(gambia_agg)\n\n\n# A tibble: 6 x 8\n  id_village     x     y green   phc   pos total  prev\n       <int> <dbl> <dbl> <dbl> <int> <int> <int> <dbl>\n1          1  350. 1458.  40.8     1    17    33 0.515\n2          2  359. 1460.  40.8     1    19    63 0.302\n3          3  360. 1460.  40.1     0     7    17 0.412\n4          4  364. 1497.  40.8     0     8    24 0.333\n5          5  366. 1460.  40.8     0    10    26 0.385\n6          6  367. 1463.  40.8     0     7    18 0.389\n\n\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nWe use the gambia.borders dataset from the geoR package to trace the country boundaries with geom_path. Since those boundaries are in meters, we divide by 1000 to get the same scale as our points. We also use coord_fixed to ensure a 1:1 aspect ratio between the axes and use the viridis color scale, which makes it easier to visualize a continuous variable compared with the default gradient scale in ggplot2.\nBased on this map, there seems to be spatial correlation in malaria prevalence, with the eastern cluster of villages showing more high prevalence values (yellow-green) and the middle cluster showing more low prevalence values (purple).\nNon-spatial GLMM\nFor this first example, we will ignore the spatial aspect of the data and model the presence of malaria (pos) as a function of the use of a bed net (netuse) and the presence of a public health centre (phc). Since we have a binary response, we need to use a logistic regression model (a GLM). Since we have predictors at both the individual and village level, and we expect that children of the same village have more similar probabilities of having malaria even after accounting for those predictors, we need to add a random effect of the village. The result is a GLMM that we fit using the glmer function in the lme4 package.\n\n\nlibrary(lme4)\n\nmod_glmm <- glmer(pos ~ netuse + phc + (1 | id_village), \n                  data = gambia, family = binomial)\nsummary(mod_glmm)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: pos ~ netuse + phc + (1 | id_village)\n   Data: gambia\n\n     AIC      BIC   logLik deviance df.resid \n  2428.0   2450.5  -1210.0   2420.0     2031 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1286 -0.7120 -0.4142  0.8474  3.3434 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n id_village (Intercept) 0.8149   0.9027  \nNumber of obs: 2035, groups:  id_village, 65\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.1491     0.2297   0.649   0.5164    \nnetuse       -0.6044     0.1442  -4.190 2.79e-05 ***\nphc          -0.4985     0.2604  -1.914   0.0556 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) netuse\nnetuse -0.422       \nphc    -0.715 -0.025\n\nAccording to these results, both netuse and phc result in a decrease of malaria prevalence, although the effect of phc is not significant at a threshold \\(\\alpha = 0.05\\). The intercept (0.149) is the logit of the probability of malaria presence for a child with no bednet and no public health centre, but it is the mean intercept across all villages, and there is a lot of variation between villages, based on the random effect standard deviation of 0.90. We can get the estimated intercept for each village with the function coef:\n\n\nhead(coef(mod_glmm)$id_village)\n\n\n  (Intercept)     netuse        phc\n1  0.93727515 -0.6043602 -0.4984835\n2  0.09204843 -0.6043602 -0.4984835\n3  0.22500620 -0.6043602 -0.4984835\n4 -0.46271089 -0.6043602 -0.4984835\n5  0.13680037 -0.6043602 -0.4984835\n6 -0.03723346 -0.6043602 -0.4984835\n\nSo for example, the intercept for village 1 is around 0.94, equivalent to a probability of 72%:\n\n\nplogis(0.937)\n\n\n[1] 0.7184933\n\nwhile the intercept in village 2 is equivalent to a probability of 52%:\n\n\nplogis(0.092)\n\n\n[1] 0.5229838\n\nThe DHARMa package provides a general method for checking whether the residuals of a GLMM are distributed according to the specified model and whether there is any residual trend. The package works by simulating replicates of each observation according to the fitted model and then determining a “standardized residual”, which is the relative position of the observed value with respect to the simulated values, e.g. 0 if the observation is smaller than all the simulations, 0.5 if it is in the middle, etc. If the model represents the data well, each value of the standardized residual between 0 and 1 should be equally likely, so the standardized residuals should produce a uniform distribution between 0 and 1.\nThe simulateResiduals function performs the calculation of the standardized residuals, then the plot function plots the diagnostic graphs with the results of certain tests.\n\n\nlibrary(DHARMa)\nres_glmm <- simulateResiduals(mod_glmm)\nplot(res_glmm)\n\n\n\n\nThe graph on the left is a quantile-quantile plot of standardized residuals. The results of three statistical tests also also shown: a Kolmogorov-Smirnov (KS) test which checks whether there is a deviation from the theoretical distribution, a dispersion test that checks whether there is underdispersion or overdispersion, and an outlier test based on the number of residuals that are more extreme than all the simulations. Here, we get a significant result for the outliers, though the message indicates that this result might have an inflated type I error rate in this case.\nOn the right, we generally get a graph of standardized residuals (in y) as a function of the rank of the predicted values, in order to check for any leftover trend in the residual. Here, the predictions are binned by quartile, so it might be better to instead aggregate the predictions and residuals by village, which we can do with the recalculateResiduals function.\n\n\nplot(recalculateResiduals(res_glmm, group = gambia$id_village))\n\n\n\n\nThe plot to the right now shows individual points, along with a quantile regression for the 1st quartile, the median and the 3rd quartile. In theory, these three curves should be horizontal straight lines (no leftover trend in the residuals vs. predictions). The curve for the 3rd quartile (in red) is significantly different from a horizontal line, which could indicate some systematic effect that is missing from the model.\nSpatial GLMM with spaMM\nThe spaMM (spatial mixed models) package is a relatively new R package that can perform approximate maximum likelihood estimation of parameters for GLMM with spatial dependence, modelled either as a Gaussian process or with a CAR (we will see the latter in the last section). The package implements different algorithms, but there is a single fitme function that chooses the appropriate algorithm for each model type. For example, here is the same (non-spatial) model as above fit with spaMM.\n\n\nlibrary(spaMM)\n\nmod_spamm_glmm <- fitme(pos ~ netuse + phc + (1 | id_village),\n                        data = gambia, family = binomial)\nsummary(mod_spamm_glmm)\n\n\nformula: pos ~ netuse + phc + (1 | id_village)\nEstimation of lambda by Laplace ML approximation (p_v).\nEstimation of fixed effects by Laplace ML approximation (p_v).\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)   0.1491   0.2287  0.6519\nnetuse       -0.6045   0.1420 -4.2567\nphc          -0.4986   0.2593 -1.9231\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   id_village  :  0.8151  \n             --- Coefficients for log(lambda):\n      Group        Term Estimate Cond.SE\n id_village (Intercept)  -0.2045  0.2008\n# of obs: 2035; # of groups: id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\np_v(h) (marginal L): -1210.016\n\nNote that the estimates of the fixed effects as well as the variance of random effects are nearly identical to those obtained by glmer above.\nWe can now use spaMM to fit the same model with the addition of spatial correlations between villages. In the formula of the model, this is represented as a random effect Matern(1 | x + y), which means that the intercepts are spatially correlated between villages following a Matérn correlation function of coordinates (x, y). The Matérn function is a flexible function for spatial correlation that includes a shape parameter \\(\\nu\\) (nu), so that when \\(\\nu = 0.5\\) it is equivalent to the exponential correlation but as \\(\\nu\\) grows to large values, it approaches a Gaussian correlation. We could let the function estimate \\(\\nu\\), but here we will fix it to 0.5 with the fixed argument of fitme.\n\n\nmod_spamm <- fitme(pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village),\n                   data = gambia, family = binomial, fixed = list(nu = 0.5))\nsummary(mod_spamm)\n\n\nformula: pos ~ netuse + phc + Matern(1 | x + y) + (1 | id_village)\nEstimation of lambda and corrPars by Laplace ML approximation (p_v).\nEstimation of fixed effects by Laplace ML approximation (p_v).\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3351  0.2047\nnetuse      -0.51719   0.1407 -3.6758\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu      1.rho \n0.50000000 0.05128915 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.6421 \n   id_village  :  0.1978  \n             --- Coefficients for log(lambda):\n      Group        Term Estimate Cond.SE\n      x + y (Intercept)   -0.443  0.2919\n id_village (Intercept)    -1.62  0.3166\n# of obs: 2035; # of groups: x + y, 65; id_village, 65 \n ------------- Likelihood values  -------------\n                        logLik\np_v(h) (marginal L): -1197.968\n\nLet’s first check the random effects of the model. The spatial correlation function has a parameter rho equal to 0.0513. This parameter in spaMM is the inverse of the range, so here the range of exponential correlation is 1/0.0513 or around 19.5 km. There are now two variance prameters, the one identified as x + y is the long-range variance (i.e. sill) for the exponential correlation model whereas the one identified as id_village shows the non-spatially correlated portion of the variation between villages.\nIn fact, while we left the random effects (1 | id_village) in the formula to represent the non-spatial portion of variation between villages, we could also represent this with a nugget effect in the geostatistical model. In both cases, it would represent the idea that even two villages very close to each other would have different baseline prevalences in the model.\nBy default, the Matern function has no nugget effect, but we can add one by specifying a non-zero Nugget in the initial parameter list init.\n\n\nmod_spamm2 <- fitme(pos ~ netuse + phc + Matern(1 | x + y),\n                    data = gambia, family = binomial, fixed = list(nu = 0.5),\n                    init = list(Nugget = 0.1))\nsummary(mod_spamm2)\n\n\nformula: pos ~ netuse + phc + Matern(1 | x + y)\nEstimation of lambda and corrPars by Laplace ML approximation (p_v).\nEstimation of fixed effects by Laplace ML approximation (p_v).\nfamily: binomial( link = logit ) \n ------------ Fixed effects (beta) ------------\n            Estimate Cond. SE t-value\n(Intercept)  0.06861   0.3352  0.2047\nnetuse      -0.51719   0.1407 -3.6758\nphc         -0.44416   0.2052 -2.1648\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n      1.nu   1.Nugget      1.rho \n0.50000000 0.23551424 0.05128739 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   x + y  :  0.8399  \n             --- Coefficients for log(lambda):\n Group        Term Estimate Cond.SE\n x + y (Intercept)  -0.1744  0.2146\n# of obs: 2035; # of groups: x + y, 65 \n ------------- Likelihood values  -------------\n                        logLik\np_v(h) (marginal L): -1197.968\n\nAs you can see, all estimates are the same, except that the variance of the spatial portion (sill) is now 0.84 and the nugget is equal to a fraction 0.235 of that sill, so a variance of 0.197, which is the same as the id_village random effect in the version above. Thus the two formulations are equivalent.\nNow, recall the coefficients we obtained for the non-spatial GLMM:\n\n\nsummary(mod_glmm)$coefficients\n\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept)  0.1490596  0.2297164  0.6488855 5.164124e-01\nnetuse      -0.6043602  0.1442451 -4.1898129 2.791846e-05\nphc         -0.4984835  0.2604289 -1.9140866 5.560909e-02\n\nIn the spatial version, both fixed effects have moved slightly towards zero, but the standard error of the effect of phc has decreased. It is interesting that the inclusion of spatial dependence has allowed us to estimate more precisely the effect of having a public health centre in the village. This would not always be the case: for a predictor that is also strongly correlated in space, spatial correlation in the response makes it harder to estimate the effect of this predictor, since it is confounded with the spatial effect. However, for a predictor that is not correlated in space, including the spatial effect reduces the residual (non-spatial) variance and may thus increase the precision of the predictor’s effect.\nThe spaMM package is also compatible with DHARMa for residual diagnostics. (You can in fact ignore the warning that it is not in the class of supported models, this is due to using the fitme function rather than a specific algorithm function in spaMM.)\n\n\nres_spamm <- simulateResiduals(mod_spamm2)\nplot(res_spamm)\n\n\n\nplot(recalculateResiduals(res_spamm, group = gambia$id_village))\n\n\n\n\nFinally, while we will show how to make and visualize spatial predictions below, we can produce a quick map of the estimated spatial effects in a spaMM model with the filled.mapMM function.\n\n\nfilled.mapMM(mod_spamm2)\n\n\n\n\nGaussian process models vs. smoothing splines\nIf you are familiar with generalized additive models (GAM), you might think that the spatial variation in malaria prevalence (as shown in the map above) could be represented by a 2D smoothing spline (as a function of \\(x\\) and \\(y\\)) within a GAM.\nThe code below fits the GAM equivalent of our Gaussian process GLMM above with the gam function in the mgcv package. The spatial effect is represented by the 2D spline s(x, y) whereas the non-spatial random effect of village is represented by s(id_village, bs = \"re\"), which is the same as (1 | id_village) in the previous models. Note that for the gam function, categorical variables must be explicitly converted to factors.\n\n\nlibrary(mgcv)\ngambia$id_village <- as.factor(gambia$id_village)\nmod_gam <- gam(pos ~ netuse + phc + s(id_village, bs = \"re\") + s(x, y), \n               data = gambia, family = binomial)\n\n\n\nTo visualize the 2D spline, we will use the gratia package.\n\n\nlibrary(gratia)\ndraw(mod_gam)\n\n\n\n\nNote that the plot of the spline s(x, y) (top right) does not extend too far from the locations of the data (other areas are blank). In this graph, we can also see that the village random effects follow the expected Gaussian distribution (top left).\nNext, we will use both the spatial GLMM from the previous section and this GAMM to predict the mean prevalence on a spatial grid of points contained in the file gambia_pred.csv. The graph below adds those prediction points (in black) on the previous map of the data points.\n\n\ngambia_pred <- read.csv(\"data/gambia_pred.csv\")\n\nggplot(gambia_agg, aes(x = x, y = y)) +\n    geom_point(data = gambia_pred) +\n    geom_point(aes(color = prev)) +\n    geom_path(data = gambia.borders, aes(x = x / 1000, y = y / 1000)) +\n    coord_fixed() +\n    theme_minimal() +\n    scale_color_viridis_c()\n\n\n\n\nTo make predictions from the GAMM model at those points, the code below goes through the following steps:\nAll predictors in the model must be in the prediction data frame, so we add constant values of netuse and phc (both equal to 1) for all points. Thus, we will make predictions of malaria prevalence in the case where a net is used and a public health centre is present. We also add a constant id_village, although it will not be used in predictions (see below).\nWe call the predict function on the output of gam to produce predictions at the new data points (argument newdata), including standard errors (se.fit = TRUE) and excluding the village random effects, so the prediction is made for an “average village”. The resulting object gam_pred will have columns fit (mean prediction) and se.fit (standard error). Those predictions and standard errors are on the link (logit) scale.\nWe add the original prediction data frame to gam_pred with cbind.\nWe add columns for the mean prediction and 50% confidence interval boundaries (mean \\(\\pm\\) 0.674 standard error), converted from the logit scale to the probability scale with plogis. We choose a 50% interval since a 95% interval may be too wide here to contrast the different predictions on the map at the end of this section.\n\n\ngambia_pred <- mutate(gambia_pred, netuse = 1, phc = 1, id_village = 1)\n\ngam_pred <- predict(mod_gam, newdata = gambia_pred, se.fit = TRUE, \n                    exclude = \"s(id_village)\")\ngam_pred <- cbind(gambia_pred, as.data.frame(gam_pred))\ngam_pred <- mutate(gam_pred, pred = plogis(fit), \n                   lo = plogis(fit - 0.674 * se.fit), # 50% CI\n                   hi = plogis(fit + 0.674 * se.fit))\n\n\n\nNote: The reason we do not make predictions directly on the probability (response) scale is that the normal formula for confidence intervals applies more accurately on the logit scale. Adding a certain number of standard errors around the mean on the probability scale would lead to less accurate intervals and maybe even confidence intervals outside the possible range (0, 1) for a probability.\nWe apply the same strategy to make predictions from the spaMM spatial GLMM model. There are a few differences in the predict method compared with the GAMM case.\nThe argument binding = \"fit\" means that mean predictions (fit column) will be attached to the prediction dataset and returned as spamm_pred.\nThe variances = list(linPred = TRUE) tells predict to calculate the variance of the linear predictor (so the square of the standard error). However, it appears as an attribute predVar in the output data frame rather than a se.fit column, so we move it to a column on the next line.\n\n\nspamm_pred <- predict(mod_spamm, newdata = gambia_pred, type = \"link\",\n                      binding = \"fit\", variances = list(linPred = TRUE))\nspamm_pred$se.fit <- sqrt(attr(spamm_pred, \"predVar\"))\nspamm_pred <- mutate(spamm_pred, pred = plogis(fit), \n                     lo = plogis(fit - 0.674 * se.fit),\n                     hi = plogis(fit + 0.674 * se.fit))\n\n\n\nFinally, we combine both sets of predictions as different rows of a pred_all dataset with bind_rows. The name of the dataset each prediction originates from (gam or spamm) will appear in the “model” column (argument .id). To simplify production of the next plot, we then use pivot_longer in the tidyr package to change the three columns “pred”, “lo” and “hi” to two columns, “stat” and “value” (pred_tall has thus three rows for every row in pred_all).\n\n\npred_all <- bind_rows(gam = gam_pred, spamm = spamm_pred, .id = \"model\")\n\nlibrary(tidyr)\npred_tall <- pivot_longer(pred_all, c(pred, lo, hi), names_to = \"stat\",\n                          values_to = \"value\")\n\n\n\nHaving done these steps, we can finally look at the prediction maps (mean, lower and upper bounds of the 50% confidence interval) with ggplot. The original data points are shown in red.\n\n\nggplot(pred_tall, aes(x = x, y = y)) +\n    geom_point(aes(color = value)) +\n    geom_point(data = gambia_agg, color = \"red\", size = 0) +\n    coord_fixed() +\n    facet_grid(stat~model) +\n    scale_color_viridis_c() +\n    theme_minimal()\n\n\n\n\nWhile both models agree that there is a higher prevalence near the eastern cluster of villages, the GAMM also estimates a higher prevalence at a few points (western edge and around the center) where there is no data. This is an artifact of the shape of the spline fit around the data points, since a spline is meant to fit a global, although nonlinear, trend. In contrast, the geostatistical model represents the spatial effect as local correlations and reverts to the overall mean prevalence when far from any data points, which is a safer assumption. This is one reason to choose a geostatistical / Gaussian process model in this case.\nBayesian methods for GLMMs with Gaussian processes\nBayesian models provide a flexible framework to express models with complex dependence structure among the data, including spatial dependence. However, fitting a Gaussian process model with a fully Bayesian approach can be slow, due the need to compute a spatial covariance matrix between all point pairs at each iteration.\nThe INLA (integrated nested Laplace approximation) method performs an approximate calculation of the Bayesian posterior distribution, which makes it suitable for spatial regression problems. We do not cover it in this course, but I recommend the textbook by Paula Moraga (in the references section below) that provides worked examples of using INLA for various geostatistical and areal data models, in the context of epidemiology, including models with both space and time dependence. The book presents the same Gambia malaria data as an example of a geostatistical dataset, which inspired its use in this course.\nGLMM with spatial autoregression\nWe return to the last example of the previous part, where we modelled the rate of COVID-19 cases (cases / 1000) for administrative health network divisions (RLS) in Quebec as a function of their population density. The rate is given by the “taux_1k” column in the rls_covid shapefile.\n\n\nlibrary(sf)\nrls_covid <- read_sf(\"data/rls_covid.shp\")\nrls_covid <- rls_covid[!is.na(rls_covid$dens_pop), ]\nplot(rls_covid[\"taux_1k\"])\n\n\n\n\nPreviously, we modelled the logarithm of this rate as a linear function of the logarithm of population density, with the residual variance correlated among neighbouring units via a CAR (conditional autoregression) structure, as shown in the code below.\n\n\nlibrary(spdep)\nlibrary(spatialreg)\n\nrls_nb <- poly2nb(rls_covid)\nrls_w <- nb2listw(rls_nb, style = \"B\")\n\ncar_lm <- spautolm(log(taux_1k) ~ log(dens_pop), data = rls_covid,\n                   listw = rls_w, family = \"CAR\")\nsummary(car_lm)\n\n\n\nCall: \nspautolm(formula = log(taux_1k) ~ log(dens_pop), data = rls_covid, \n    listw = rls_w, family = \"CAR\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1.201858 -0.254084 -0.053348  0.281482  1.427053 \n\nCoefficients: \n              Estimate Std. Error z value  Pr(>|z|)\n(Intercept)   1.702068   0.168463 10.1035 < 2.2e-16\nlog(dens_pop) 0.206623   0.032848  6.2903 3.169e-10\n\nLambda: 0.15762 LR test value: 23.991 p-value: 9.6771e-07 \nNumerical Hessian standard error of lambda: 0.0050486 \n\nLog likelihood: -80.68953 \nML residual variance (sigma squared): 0.2814, (sigma: 0.53048)\nNumber of observations: 95 \nNumber of parameters estimated: 4 \nAIC: 169.38\n\nAs a reminder, the poly2nb function in the spdep package creates a list of neighbours based on bordering polygons in a shapefile, then the nb2listw converts it to a list of weights, here binary weights (style = \"B\") so that each bordering region receives the same weight of 1 in the autoregressive model.\nInstead of using the rates, it would be possible to model the cases directly (column “cas” in the dataset) with a Poisson regression, which is appropriate for count data. To account for the fact that if the risk per person were equal, cases would be proportional to population, we can add the unit’s population pop as an offset in the Poisson regression. Therefore, the model would look like: cas ~ log(dens_pop) + offset(log(pop)). Note that since the Poisson regression uses a logarithmic link, that model with log(pop) as an offset assumes that log(cas / pop) (so the log rate) is proportional to log(dens_pop), just like the linear model above, but it has the advantage of modelling the stochasticity of the raw data (the number of cases) directly with a Poisson distribution.\nWe do not have the population in this data, but we can estimate it from the cases and the rate (cases / 1000) as follows:\n\n\nrls_covid$pop <- rls_covid$cas / rls_covid$taux_1k * 1000\n\n\n\nTo define a CAR model in spaMM, we need a weights matrix rather than a list of weights as in the spatialreg package. Fortunately, the spdep package also includes a function nb2mat to convert the neighbours list to a matrix of weights, here again using binary weights. To avoid a warning, we specify the row and column names of that matrix to be equal to the IDs associated with each unit (RLS_code). Then, we add a term adjacency(1 | RLS_code) to the model to specify that the residual variation between different groups defined by RLS_code is spatially correlated with a CAR structure (here, each group has only one observation since we have one data point by RLS unit).\n\n\nlibrary(spaMM)\n\nrls_mat <- nb2mat(rls_nb, style = \"B\")\nrownames(rls_mat) <- rls_covid$RLS_code\ncolnames(rls_mat) <- rls_covid$RLS_code\n\nrls_spamm <- fitme(cas ~ log(dens_pop) + offset(log(pop)) + adjacency(1 | RLS_code),\n                   data = rls_covid, adjMatrix = rls_mat, family = poisson)\nsummary(rls_spamm)\n\n\nformula: cas ~ log(dens_pop) + offset(log(pop)) + adjacency(1 | RLS_code)\nEstimation of lambda and corrPars by Laplace ML approximation (p_v).\nEstimation of fixed effects by Laplace ML approximation (p_v).\nfamily: poisson( link = log ) \n ------------ Fixed effects (beta) ------------\n              Estimate Cond. SE t-value\n(Intercept)    -5.1620  0.16858 -30.621\nlog(dens_pop)   0.1999  0.03267   6.118\n --------------- Random effects ---------------\nFamily: gaussian( link = identity ) \n                   --- Correlation parameters:\n    1.rho \n0.1576786 \n           --- Variance parameters ('lambda'):\nlambda = var(u) for u ~ Gaussian; \n   RLS_code  :  0.266  \n             --- Coefficients for log(lambda):\n    Group        Term Estimate Cond.SE\n RLS_code (Intercept)   -1.324  0.1473\n# of obs: 95; # of groups: RLS_code, 95 \n ------------- Likelihood values  -------------\n                        logLik\np_v(h) (marginal L): -709.3234\n\nNote that the spatial correlation coefficient rho (0.158) is similar to the equivalent quantity in the spautolm model above, where it was called Lambda. The effect of log(dens_pop) is also approximately 0.2 in both models.\nReference\nMoraga, Paula (2019) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny. Chapman & Hall/CRC Biostatistics Series. Available online at https://www.paulamoraga.com/book-geospatial/.\n\n\n\n",
    "preview": "posts/2021-01-21-spatial-statistics-in-ecology-part-4/spatial-statistics-in-ecology-part-4_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-20T21:58:12+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-19-spatial-statistics-in-ecology/",
    "title": "Spatial Statistics in Ecology, Part 3",
    "description": "Third part of the introduction to spatial statistics offered by Philippe Marchand to BIOS2 Fellows in January 2021, focused on aerial data analysis in R.",
    "author": [
      {
        "name": "Philippe Marchand",
        "url": {}
      }
    ],
    "date": "2021-01-19",
    "categories": [],
    "contents": "\nAreal data\nAreal data are variables measured for regions of space, defined by polygons. This type of data is more common in the social sciences, human geography and epidemiology, where data is often available at the scale of administrative divisions.\nThis type of data also appears frequently in natural resource management. For example, the following map shows the forest management units of the Ministère de la Forêt, de la Faune et des Parcs du Québec.\n\nSuppose that a variable is available at the level of these management units. How can we model the spatial correlation between units that are spatially close together?\nOne option would be to apply the geostatistical methods seen before, for example by calculating the distance between the centers of the polygons.\nAnother option, which is more adapted for areal data, is to define a network where each region is connected to neighbouring regions by a link. It is then assumed that the variables are directly correlated between neighbouring regions only. (Note, however, that direct correlations between immediate neighbours also generate indirect correlations for a chain of neighbours).\nIn this type of model, the correlation is not necessarily the same from one link to another. In this case, each link in the network can be associated with a weight representing its importance for the spatial correlation. We represent these weights by a matrix \\(W\\) where \\(w_{ij}\\) is the weight of the link between regions \\(i\\) and \\(j\\). A region has no link with itself, so \\(w_{ii} = 0\\).\nA simple choice for \\(W\\) is to assign a weight equal to 1 if the regions are neighbours, otherwise 0 (binary weight).\nIn addition to land divisions represented by polygons, another example of areal data consists of a grid where the variable is calculated for each cell of the grid. In this case, a cell generally has 4 or 8 neighbouring cells, depending on whether diagonals are included or not.\nMoran’s I\nBefore discussing spatial autocorrelation models, we present Moran’s \\(I\\) statistic, which allows us to test whether a significant correlation is present between neighbouring regions.\nMoran’s \\(I\\) is a spatial autocorrelation coefficient of \\(z\\), weighted by the \\(w_{ij}\\). It therefore takes values between -1 and 1.\n\\[I = \\frac{N}{\\sum_i \\sum_j w_{ij}} \\frac{\\sum_i \\sum_j w_{ij} (z_i - \\bar{z}) (z_j - \\bar{z})}{\\sum_i (z_i - \\bar{z})^2}\\]\nIn this equation, we recognize the expression of a correlation, which is the product of the deviations from the mean for two variables \\(z_i\\) and \\(z_j\\), divided by the product of their standard deviations (it is the same variable here, so we get the variance). The contribution of each pair \\((i, j)\\) is multiplied by its weight \\(w_{ij}\\) and the term on the left (the number of regions \\(N\\) divided by the sum of the weights) ensures that the result is bounded between -1 and 1.\nSince the distribution of \\(I\\) is known in the absence of spatial autocorrelation, this statistic serves to test the null hypothesis that there is no spatial correlation between neighbouring regions.\nAlthough we will not see an example in this course, Moran’s \\(I\\) can also be applied to point data. In this case, we divide the pairs of points into distance classes and calculate \\(I\\) for each distance class; the weight \\(w_{ij} = 1\\) if the distance between \\(i\\) and \\(j\\) is in the desired distance class, otherwise 0.\nSpatial autoregression models\nLet us recall the formula for a linear regression with spatial dependence:\n\\[v = \\beta_0 + \\sum_i \\beta_i u_i + z + \\epsilon\\]\nwhere \\(z\\) is the portion of the residual variance that is spatially correlated.\nThere are two main types of autoregressive models to represent the spatial dependence of \\(z\\): conditional autoregression (CAR) and simultaneous autoregressive (SAR).\nConditional autoregressive (CAR) model\nIn the conditional autoregressive model, the value of \\(z_i\\) for the region \\(i\\) follows a normal distribution: its mean depends on the value \\(z_j\\) of neighbouring regions, multiplied by the weight \\(w_{ij}\\) and a correlation coefficient \\(\\rho\\); its standard deviation \\(\\sigma_{z_i}\\) may vary from one region to another.\n\\[z_i \\sim \\text{N}\\left(\\sum_j \\rho w_{ij} z_j,\\sigma_{z_i} \\right)\\]\nIn this model, if \\(w_{ij}\\) is a binary matrix (0 for non-neighbours, 1 for neighbours), then \\(\\rho\\) is the coefficient of partial correlation between neighbouring regions. This is similar to a first-order autoregressive model in the context of time series, where the autoregression coefficient indicates the partial correlation.\nSimultaneous autoregressive (SAR) model\nIn the simultaneous autoregressive model, the value of \\(z_i\\) is given directly by the sum of contributions from neighbouring values \\(z_j\\), multiplied by \\(\\rho w_{ij}\\), with an independent residual \\(\\nu_i\\) of standard deviation \\(\\sigma_z\\).\n\\[z_i = \\sum_j \\rho w_{ij} z_j + \\nu_i\\]\nAt first glance, this looks like a temporal autoregressive model. However, there is an important conceptual difference. For temporal models, the causal influence is directed in only one direction: \\(v(t-2)\\) affects \\(v(t-1)\\) which then affects \\(v(t)\\). For a spatial model, each \\(z_j\\) that affects \\(z_i\\) depends in turn on \\(z_i\\). Thus, to determine the joint distribution of \\(z\\), a system of equations must be solved simultaneously (hence the name of the model).\nFor this reason, although this model resembles the formula of CAR model, the solutions of the two models differ and in the case of SAR, the coefficient \\(\\rho\\) is not directly equal to the partial correlation due to each neighbouring region.\nFor more details on the mathematical aspects of these models, see the article by Ver Hoef et al. (2018) suggested in reference.\nFor the moment, we will consider SAR and CAR as two types of possible models to represent a spatial correlation on a network. We can always fit several models and compare them with the AIC to choose the best form of correlation or the best weight matrix.\nThe CAR and SAR models share an advantage over geostatistical models in terms of efficiency. In a geostatistical model, spatial correlations are defined between each pair of points, although they become negligible as distance increases. For a CAR or SAR model, only neighbouring regions contribute and most weights are equal to 0, making these models faster to fit than a geostatistical model when the data are massive.\nAnalysis of areal data in R\nTo illustrate the analysis of areal data in R, we load the packages sf (to read geospatial data), spdep (to define spatial networks and calculate Moran’s \\(I\\)) and spatialreg (for SAR and CAR models).\n\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(spatialreg)\n\n\n\nAs an example, we will use a dataset that presents some of the results of the 2018 provincial election in Quebec, with population characteristics of each riding. This data is included in a shapefile (.shp) file type, which we can read with the read_sf function of the sf package.\n\n\nelect2018 <- read_sf(\"data/elect2018.shp\")\nhead(elect2018)\n\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97879.03 ymin: 174515.3 xmax: 694261.1 ymax: 599757.1\nProjected CRS: LambertAQ\n# A tibble: 6 x 10\n  circ   age_moy pct_frn pct_prp rev_med propCAQ propPQ propPLQ propQS\n  <chr>    <dbl>   <dbl>   <dbl>   <int>   <dbl>  <dbl>   <dbl>  <dbl>\n1 Abiti…    40.8   0.963   0.644   34518    42.7   19.5    18.8   15.7\n2 Abiti…    42.2   0.987   0.735   33234    34.1   33.3    11.3   16.6\n3 Acadie    40.3   0.573   0.403   25391    16.5    9      53.8   13.8\n4 Anjou…    43.5   0.821   0.416   31275    28.9   14.7    39.1   14.5\n5 Argen…    43.3   0.858   0.766   31097    38.9   21.1    17.4   12.2\n6 Artha…    43.4   0.989   0.679   30082    61.8    9.4    11.4   12.6\n# … with 1 more variable: geometry <MULTIPOLYGON [m]>\n\nNote: The dataset is actually composed of 4 files with the extensions .dbf, .prj, .shp and .shx, but it is sufficient to write the name of the .shp file in read_sf.\nThe columns of the dataset are, in order:\nthe name of the electoral riding (circ);\nfour characteristics of the population (age_moy = mean age, pct_frn = fraction of the population that speaks mainly French at home, pct_prp = fraction of households that own their home, rev_med = median income);\nfour columns showing the fraction of votes obtained by the main parties (CAQ, PQ, PLQ, QS);\na geometry column that contains the geometric object (multipolygon) corresponding to the riding.\nTo illustrate one of the variables on a map, we call the plot function with the name of the column in square brackets and quotation marks.\n\n\nplot(elect2018[\"rev_med\"])\n\n\n\n\nIn this example, we want to model the fraction of votes obtained by the CAQ based on the characteristics of the population in each riding and taking into account the spatial correlations between neighbouring ridings.\nDefinition of the neighbourhood network\nThe poly2nb function of the spdep package defines a neighbourhood network from polygons. The result vois is a list of 125 elements where each element contains the indices of the neighbouring (bordering) polygons of a given polygon.\n\n\nvois <- poly2nb(elect2018)\nvois[[1]]\n\n\n[1]   2  37  63  88 101 117\n\nThus, the first riding (Abitibi-Est) has 6 neighbouring ridings, for which the names can be found as follows:\n\n\nelect2018$circ[vois[[1]]]\n\n\n[1] \"Abitibi-Ouest\"               \"Gatineau\"                   \n[3] \"Laviolette-Saint-Maurice\"    \"Pontiac\"                    \n[5] \"Rouyn-Noranda-Témiscamingue\" \"Ungava\"                     \n\nWe can illustrate this network by extracting the coordinates of the center of each district, creating a blank map with plot(elect2018[\"geometry\"]), then adding the network as an additional layer with plot(vois, add = TRUE, coords = coords).\n\n\ncoords <- st_centroid(elect2018) %>%\n    st_coordinates()\nplot(elect2018[\"geometry\"])\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nWe can “zoom” on southern Québec by choosing the limits xlim and ylim.\n\n\nplot(elect2018[\"geometry\"], \n     xlim = c(400000, 800000), ylim = c(100000, 500000))\nplot(vois, add = TRUE, col = \"red\", coords = coords)\n\n\n\n\nWe still have to add weights to each network link with the nb2listw function. The style of weights “B” corresponds to binary weights, i.e. 1 for the presence of link and 0 for the absence of link between two ridings.\nOnce these weights are defined, we can verify with Moran’s test whether there is a significant autocorrelation of votes obtained by the CAQ between neighbouring ridings.\n\n\npoids <- nb2listw(vois, style = \"B\")\n\nmoran.test(elect2018$propCAQ, poids)\n\n\n\n    Moran I test under randomisation\n\ndata:  elect2018$propCAQ  \nweights: poids    \n\nMoran I statistic standard deviate = 13.148, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.680607768      -0.008064516       0.002743472 \n\nThe value \\(I = 0.68\\) is very significant judging by the \\(p\\)-value of the test.\nLet’s verify if the spatial correlation persists after taking into account the four characteristics of the population, therefore by inspecting the residuals of a linear model including these four predictors.\n\n\nelect_lm <- lm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, data = elect2018)\nsummary(elect_lm)\n\n\n\nCall:\nlm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.9890  -4.4878   0.0562   6.2653  25.8146 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.354e+01  1.836e+01   0.737    0.463    \nage_moy     -9.170e-01  3.855e-01  -2.378    0.019 *  \npct_frn      4.588e+01  5.202e+00   8.820 1.09e-14 ***\npct_prp      3.582e+01  6.527e+00   5.488 2.31e-07 ***\nrev_med     -2.624e-05  2.465e-04  -0.106    0.915    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.409 on 120 degrees of freedom\nMultiple R-squared:  0.6096,    Adjusted R-squared:  0.5965 \nF-statistic: 46.84 on 4 and 120 DF,  p-value: < 2.2e-16\n\nmoran.test(residuals(elect_lm), poids)\n\n\n\n    Moran I test under randomisation\n\ndata:  residuals(elect_lm)  \nweights: poids    \n\nMoran I statistic standard deviate = 6.7047, p-value =\n1.009e-11\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.340083290      -0.008064516       0.002696300 \n\nMoran’s \\(I\\) has decreased but remains significant, so some of the previous correlation was induced by these predictors, but there remains a spatial correlation due to other factors.\nSpatial autoregression models\nFinally, we fit SAR and CAR models to these data with the spautolm (spatial autoregressive linear model) function of spatialreg. Here is the code for a SAR model including the effect of the same four predictors.\n\n\nelect_sar <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids)\nsummary(elect_sar)\n\n\n\nCall: \nspautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-23.08342  -4.10573   0.24274   4.29941  23.08245 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 15.09421119 16.52357745  0.9135   0.36098\nage_moy     -0.70481703  0.32204139 -2.1886   0.02863\npct_frn     39.09375061  5.43653962  7.1909 6.435e-13\npct_prp     14.32329345  6.96492611  2.0565   0.03974\nrev_med      0.00016730  0.00023209  0.7208   0.47101\n\nLambda: 0.12887 LR test value: 42.274 p-value: 7.9339e-11 \nNumerical Hessian standard error of lambda: 0.01207 \n\nLog likelihood: -433.8862 \nML residual variance (sigma squared): 53.028, (sigma: 7.282)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 881.77\n\nThe value given by Lambda in the summary corresponds to the coefficient \\(\\rho\\) in our description of the model. The likelihood-ratio test (LR test) confirms that this residual spatial correlation (after controlling for the effect of predictors) is significant.\nThe estimated effects for the predictors are similar to those of the linear model without spatial correlation. The effects of mean age, fraction of francophones and fraction of homeowners remain significant, although their magnitude has decreased somewhat.\nTo fit a CAR rather than SAR model, we must specify family = \"CAR\".\n\n\nelect_car <- spautolm(propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n                      data = elect2018, listw = poids, family = \"CAR\")\nsummary(elect_car)\n\n\n\nCall: \nspautolm(formula = propCAQ ~ age_moy + pct_frn + pct_prp + rev_med, \n    data = elect2018, listw = poids, family = \"CAR\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-21.73315  -4.24623  -0.24369   3.44228  23.43749 \n\nCoefficients: \n               Estimate  Std. Error z value  Pr(>|z|)\n(Intercept) 16.57164696 16.84155327  0.9840  0.325128\nage_moy     -0.79072151  0.32972225 -2.3981  0.016478\npct_frn     38.99116707  5.43667482  7.1719 7.399e-13\npct_prp     17.98557474  6.80333470  2.6436  0.008202\nrev_med      0.00012639  0.00023106  0.5470  0.584364\n\nLambda: 0.15517 LR test value: 40.532 p-value: 1.9344e-10 \nNumerical Hessian standard error of lambda: 0.0026868 \n\nLog likelihood: -434.7573 \nML residual variance (sigma squared): 53.9, (sigma: 7.3416)\nNumber of observations: 125 \nNumber of parameters estimated: 7 \nAIC: 883.51\n\nFor a CAR model with binary weights, the value of Lambda (which we called \\(\\rho\\)) directly gives the partial correlation coefficient between neighbouring districts. Note that the AIC here is slightly higher than the SAR model, so the latter gave a better fit.\nExercise\nThe rls_covid dataset, in shapefile format, contains data on detected COVID-19 cases (cas), number of cases per 1000 people (taux_1k) and the population density (dens_pop) in each of Quebec’s local health service networks (RLS) (Source: Data downloaded from the Institut national de santé publique du Québec as of January 17, 2021).\n\n\nrls_covid <- read_sf(\"data/rls_covid.shp\")\nhead(rls_covid)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 785111.2 ymin: 341057.8 xmax: 979941.5 ymax: 541112.7\nProjected CRS: Conique_conforme_de_Lambert_du_MTQ_utilis_e_pour_Adresse_Qu_be\n# A tibble: 6 x 6\n  RLS_code RLS_nom    cas taux_1k dens_pop                    geometry\n  <chr>    <chr>    <dbl>   <dbl>    <dbl>          <MULTIPOLYGON [m]>\n1 0111     RLS de …   152    7.34     6.76 (((827028.3 412772.4, 8270…\n2 0112     RLS de …   256    7.34    19.6  (((855905 452116.9, 855784…\n3 0113     RLS de …    81    4.26     4.69 (((911829.4 441311.2, 9121…\n4 0114     RLS des…    28    3.3      5.35 (((879249.6 471975.6, 8792…\n5 0115     RLS de …   576    9.96    15.5  (((917748.1 503148.7, 9179…\n6 0116     RLS de …    76    4.24     5.53 (((951316 523499.3, 952553…\n\nFit a linear model of the number of cases per 1000 as a function of population density (it is suggested to apply a logarithmic transform to the latter). Check whether the model residuals are correlated between bordering RLS with a Moran’s test and then model the same data with a conditional autoregressive model.\nReference\nVer Hoef, J.M., Peterson, E.E., Hooten, M.B., Hanks, E.M. and Fortin, M.-J. (2018) Spatial autoregressive models for statistical inference from ecological data. Ecological Monographs 88: 36-59.\n\n\n\n",
    "preview": "posts/2021-01-19-spatial-statistics-in-ecology/spatial-statistics-in-ecology_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-19T21:21:03+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-13-spatial-statistics-in-ecology/",
    "title": "Spatial Statistics in Ecology, Part 2",
    "description": "Introduction to spatial statistics offered by Philippe Marchand to BIOS2 Fellows in January 2021. This second part is focused on spatial correlations and geostatistical models.",
    "author": [
      {
        "name": "Philippe Marchand",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [
      "Technical",
      "English"
    ],
    "contents": "\n\nContents\nSpatial correlation of a variable\nIntrinsic or induced dependence\nDifferent ways to model spatial effects\n\nGeostatistical models\nVariogram\nTheoretical models for the variogram\nEmpirical variogram\nRegression model with spatial correlation\n\nGeostatistical models in R\nRegression with spatial correlation\nExercise\n\nKriging\nSolutions\n\nSpatial correlation of a variable\nCorrelation between measurements of a variable taken at nearby points often occurs in environmental data. This principle is sometimes referred to as the “first law of geography” and is expressed in the following quote from Waldo Tobler: “Everything is related to everything else, but near things are more related than distant things”.\nIn statistics, we often refer to autocorrelation as the correlation between measurements of the same variable taken at different times (temporal autocorrelation) or places (spatial autocorrelation).\nIntrinsic or induced dependence\nThere are two basic types of spatial dependence on a measured variable \\(y\\): an intrinsic dependence on \\(y\\), or a dependence induced by external variables influencing \\(y\\), which are themselves spatially correlated.\nFor example, suppose that the abundance of a species is correlated between two sites located near each other:\nthis spatial dependence can be induced if it is due to a spatial correlation of habitat factors that are favorable or unfavorable to the species;\nor it can be intrinsic if it is due to the dispersion of individuals to nearby sites.\nIn many cases, both types of dependence affect a given variable.\nIf the dependence is simply induced and the external variables that cause it are included in the model explaining \\(y\\), then the model residuals will be independent and we can use all the methods already seen that ignore spatial correlation.\nHowever, if the dependence is intrinsic or due to unmeasured external factors, then the spatial correlation of the residuals in the model will have to be taken into account.\nDifferent ways to model spatial effects\nIn this training, we will directly model the spatial correlations of our data. It is useful to compare this approach to other ways of including spatial aspects in a statistical model.\nFirst, we could include predictors in the model that represent position (e.g., longitude, latitude). Such predictors may be useful for detecting a systematic large-scale trend or gradient, whether or not the trend is linear (e.g., with a generalized additive model).\nIn contrast to this approach, the models we will see now serve to model a spatial correlation in the random fluctuations of a variable (i.e., in the residuals after removing any systematic effect).\nMixed models use random effects to represent the non-independence of data on the basis of their grouping, i.e., after accounting for systematic fixed effects, data from the same group are more similar (their residual variation is correlated) than data from different groups. These groups were sometimes defined according to spatial criteria (observations grouped into sites).\nHowever, in the context of a random group effect, all groups are as different from each other, e.g., two sites within 100 km of each other are no more or less similar than two sites 2 km apart.\nThe methods we will see here and in the next parts of the training therefore allow us to model non-independence on a continuous scale (closer = more correlated) rather than just discrete (hierarchy of groups).\nGeostatistical models\nGeostatistics refers to a group of techniques that originated in the earth sciences. Geostatistics is concerned with variables that are continuously distributed in space and where a number of points are sampled to estimate this distribution. A classic example of these techniques comes from the mining field, where the aim was to create a map of the concentration of ore at a site from samples taken at different points on the site.\nFor these models, we will assume that \\(z(x, y)\\) is a stationary spatial variable measured at points with coordinates \\(x\\) and \\(y\\).\nVariogram\nA central aspect of geostatistics is the estimation of the variogram \\(\\gamma_z\\) . The variogram is equal to half the mean square difference between the values of \\(z\\) for two points \\((x_i, y_i)\\) and \\((x_j, y_j)\\) separated by a distance \\(h\\).\n\\[\\gamma_z(h) = \\frac{1}{2} \\text{E} \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h}\\]\nIn this equation, the \\(\\text{E}\\) function with the index \\(d_{ij}=h\\) designates the statistical expectation (i.e., the mean) of the squared deviation between the values of \\(z\\) for points separated by a distance \\(h\\).\nIf we want instead to express the autocorrelation \\(\\rho_z(h)\\) between measures of \\(z\\) separated by a distance \\(h\\), it is related to the variogram by the equation:\n\\[\\gamma_z = \\sigma_z^2(1 - \\rho_z)\\] ,\nwhere \\(\\sigma_z^2\\) is the global variance of \\(z\\).\nNote that \\(\\gamma_z = \\sigma_z^2\\) when we reach a distance where the measurements of \\(z\\) are independent, so \\(\\rho_z = 0\\). In this case, we can see that \\(\\gamma_z\\) is similar to a variance, although it is sometimes called “semivariogram” or “semivariance” because of the 1/2 factor in the above equation.\nTheoretical models for the variogram\nSeveral parametric models have been proposed to represent the spatial correlation as a function of the distance between sampling points. Let us first consider a correlation that decreases exponentially:\n\\[\\rho_z(h) = e^{-h/r}\\]\nHere, \\(\\rho_z = 1\\) for \\(h = 0\\) and the correlation is multiplied by \\(1/e \\approx 0.37\\) each time the distance increases by \\(r\\). In this context, \\(r\\) is called the range of the correlation.\nFrom the above equation, we can calculate the corresponding variogram.\n\\[\\gamma_z(h) = \\sigma_z^2 (1 - e^{-h/r})\\]\nHere is a graphical representation of this variogram.\n\n\n\nBecause of the exponential function, the value of \\(\\gamma\\) at large distances approaches the global variance \\(\\sigma_z^2\\) without exactly reaching it. This asymptote is called a sill in the geostatistical context and is represented by the symbol \\(s\\).\nFinally, it is sometimes unrealistic to assume a perfect correlation when the distance tends towards 0, because of a possible variation of \\(z\\) at a very small scale. A nugget effect, denoted \\(n\\), can be added to the model so that \\(\\gamma\\) approaches \\(n\\) (rather than 0) if \\(h\\) tends towards 0. The term nugget comes from the mining origin of these techniques, where a nugget could be the source of a sudden small-scale variation in the concentration of a mineral.\nBy adding the nugget effect, the remainder of the variogram is “compressed” to keep the same sill, resulting in the following equation.\n\\[\\gamma_z(h) = n + (s - n) (1 - e^{-h/r})\\]\nIn the gstat package that we use below, the term \\((s-n)\\) is called a partial sill or psill for the exponential portion of the variogram.\n\n\n\nIn addition to the exponential model, two other common theoretical models for the variogram are the Gaussian model (where the correlation follows a half-normal curve), and the spherical model (where the variogram increases linearly at the start and then curves and reaches the plateau at a distance equal to its range \\(r\\)). The spherical model thus allows the correlation to be exactly 0 at large distances, rather than gradually approaching zero in the case of the other models.\nModel\n\\(\\rho(h)\\)\n\\(\\gamma(h)\\)\nExponential\n\\(\\exp\\left(-\\frac{h}{r}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h}{r}\\right)\\right)\\)\nGaussian\n\\(\\exp\\left(-\\frac{h^2}{r^2}\\right)\\)\n\\(s \\left(1 - \\exp\\left(-\\frac{h^2}{r^2}\\right)\\right)\\)\nSpherical \\((h < r)\\) *\n\\(1 - \\frac{3}{2}\\frac{h}{r} + \\frac{1}{2}\\frac{h^3}{r^3}\\)\n\\(s \\left(\\frac{3}{2}\\frac{h}{r} - \\frac{1}{2}\\frac{h^3}{r^3} \\right)\\)\n* For the spherical model, \\(\\rho = 0\\) and \\(\\gamma = s\\) if \\(h \\ge r\\).\n\n\n\nEmpirical variogram\nTo estimate \\(\\gamma_z(h)\\) from empirical data, we need to define distance classes, thus grouping different distances within a margin of \\(\\pm \\delta\\) around a distance \\(h\\), then calculating the mean square deviation for the pairs of points in that distance class.\n\\[\\hat{\\gamma_z}(h) = \\frac{1}{2 N_{\\text{paires}}} \\sum \\left[ \\left( z(x_i, y_i) - z(x_j, y_j) \\right)^2 \\right]_{d_{ij} = h \\pm \\delta}\\]\nWe will see in the next section how to estimate a variogram in R.\nRegression model with spatial correlation\nThe following equation represents a multiple linear regression including residual spatial correlation:\n\\[v = \\beta_0 + \\sum_i \\beta_i u_i + z + \\epsilon\\]\nHere, \\(v\\) designates the response variable and \\(u\\) the predictors, to avoid confusion with the spatial coordinates \\(x\\) and \\(y\\).\nIn addition to the residual \\(\\epsilon\\) that is independent between observations, the model includes a term \\(z\\) that represents the spatially correlated portion of the residual variance.\nHere are suggested steps to apply this type of model:\nFit the regression model without spatial correlation.\nVerify the presence of spatial correlation from the empirical variogram of the residuals.\nFit one or more regression models with spatial correlation and select the one that shows the best fit to the data.\nGeostatistical models in R\nThe gstat package contains functions related to geostatistics. For this example, we will use the oxford dataset from this package, which contains measurements of physical and chemical properties for 126 soil samples from a site, along with their coordinates XCOORD and YCOORD.\n\n\nlibrary(gstat)\n\ndata(oxford)\nstr(oxford)\n\n\n'data.frame':   126 obs. of  22 variables:\n $ PROFILE  : num  1 2 3 4 5 6 7 8 9 10 ...\n $ XCOORD   : num  100 100 100 100 100 100 100 100 100 100 ...\n $ YCOORD   : num  2100 2000 1900 1800 1700 1600 1500 1400 1300 1200 ...\n $ ELEV     : num  598 597 610 615 610 595 580 590 598 588 ...\n $ PROFCLASS: Factor w/ 3 levels \"Cr\",\"Ct\",\"Ia\": 2 2 2 3 3 2 3 2 3 3 ...\n $ MAPCLASS : Factor w/ 3 levels \"Cr\",\"Ct\",\"Ia\": 2 3 3 3 3 2 2 3 3 3 ...\n $ VAL1     : num  3 3 4 4 3 3 4 4 4 3 ...\n $ CHR1     : num  3 3 3 3 3 2 2 3 3 3 ...\n $ LIME1    : num  4 4 4 4 4 0 2 1 0 4 ...\n $ VAL2     : num  4 4 5 8 8 4 8 4 8 8 ...\n $ CHR2     : num  4 4 4 2 2 4 2 4 2 2 ...\n $ LIME2    : num  4 4 4 5 5 4 5 4 5 5 ...\n $ DEPTHCM  : num  61 91 46 20 20 91 30 61 38 25 ...\n $ DEP2LIME : num  20 20 20 20 20 20 20 20 40 20 ...\n $ PCLAY1   : num  15 25 20 20 18 25 25 35 35 12 ...\n $ PCLAY2   : num  10 10 20 10 10 20 10 20 10 10 ...\n $ MG1      : num  63 58 55 60 88 168 99 59 233 87 ...\n $ OM1      : num  5.7 5.6 5.8 6.2 8.4 6.4 7.1 3.8 5 9.2 ...\n $ CEC1     : num  20 22 17 23 27 27 21 14 27 20 ...\n $ PH1      : num  7.7 7.7 7.5 7.6 7.6 7 7.5 7.6 6.6 7.5 ...\n $ PHOS1    : num  13 9.2 10.5 8.8 13 9.3 10 9 15 12.6 ...\n $ POT1     : num  196 157 115 172 238 164 312 184 123 282 ...\n\nSuppose that we want to model the magnesium concentration (MG1), represented as a function of the spatial position in the following graph.\n\n\nlibrary(ggplot2)\nggplot(oxford, aes(x = YCOORD, y = XCOORD, size = MG1)) +\n    geom_point() +\n    coord_fixed()\n\n\n\n\nNote that the \\(x\\) and \\(y\\) axes have been inverted to save space. The coord_fixed() function of ggplot2 ensures that the scale is the same on both axes, which is useful for representing spatial data.\nWe can immediately see that these measurements were taken on a 100 m grid. It seems that the magnesium concentration is spatially correlated, although it may be a correlation induced by another variable. In particular, we know that the concentration of magnesium is negatively related to the soil pH (PH1).\n\n\nggplot(oxford, aes(x = PH1, y = MG1)) +\n    geom_point()\n\n\n\n\nThe variogram function of gstat is used to estimate a variogram from empirical data. Here is the result obtained for the variable MG1.\n\n\nvar_mg <- variogram(MG1 ~ 1, locations = ~ XCOORD + YCOORD, data = oxford)\nvar_mg\n\n\n    np     dist    gamma dir.hor dir.ver   id\n1  225 100.0000 1601.404       0       0 var1\n2  200 141.4214 1950.805       0       0 var1\n3  548 215.0773 2171.231       0       0 var1\n4  623 303.6283 2422.245       0       0 var1\n5  258 360.5551 2704.366       0       0 var1\n6  144 400.0000 2948.774       0       0 var1\n7  570 427.5569 2994.621       0       0 var1\n8  291 500.0000 3402.058       0       0 var1\n9  366 522.8801 3844.165       0       0 var1\n10 200 577.1759 3603.060       0       0 var1\n11 458 619.8400 3816.595       0       0 var1\n12  90 670.8204 3345.739       0       0 var1\n\nThe formula MG1 ~ 1 indicates that no linear predictor is included in this model, while the argument locations indicates which variables in the data frame correspond to the spatial coordinates.\nIn the resulting table, gamma is the value of the variogram for the distance class centered on dist, while np is the number of pairs of points in that class. Here, since the points are located on a grid, we obtain regular distance classes (e.g.: 100 m for neighboring points on the grid, 141 m for diagonal neighbors, etc.).\nHere, we limit ourselves to the estimation of isotropic variograms, i.e. the variogram depends only on the distance between the two points and not on the direction. Although we do not have time to see it today, it is possible with gstat to estimate the variogram separately in different directions.\nWe can illustrate the variogram with plot.\n\n\nplot(var_mg, col = \"black\")\n\n\n\n\nIf we want to estimate the residual spatial correlation of MG1 after including the effect of PH1, we can add that predictor to the formula.\n\n\nvar_mg <- variogram(MG1 ~ PH1, locations = ~ XCOORD + YCOORD, data = oxford)\nplot(var_mg, col = \"black\")\n\n\n\n\nIncluding the effect of pH, the range of the spatial correlation seems to decrease, while the plateau is reached around 300 m. It even seems that the variogram decreases beyond 400 m. In general, we assume that the variance between two points does not decrease with distance, unless there is a periodic spatial pattern.\nThe function fit.variogram accepts as arguments a variogram estimated from the data, as well as a theoretical model described in a vgm function, and then estimates the parameters of that model according to the data. The fitting is done by the method of least squares.\nFor example, vgm(\"Exp\") means we want to fit an exponential model.\n\n\nvfit <- fit.variogram(var_mg, vgm(\"Exp\"))\nvfit\n\n\n  model    psill    range\n1   Nug    0.000  0.00000\n2   Exp 1951.496 95.11235\n\nThere is no nugget effect, because psill = 0 for the Nug (nugget) part of the model. The exponential part has a sill at 1951 and a range of 95 m.\nTo compare different models, a vector of model names can be given to vgm. In the following example, we include the exponential, gaussian (“Gau”) and spherical (“Sph”) models.\n\n\nvfit <- fit.variogram(var_mg, vgm(c(\"Exp\", \"Gau\", \"Sph\")))\nvfit\n\n\n  model    psill    range\n1   Nug    0.000  0.00000\n2   Exp 1951.496 95.11235\n\nThe function gives us the result of the model with the best fit (lowest sum of squared deviations), which here is the same exponential model.\nFinally, we can superimpose the theoretical model and the empirical variogram on the same graph.\n\n\nplot(var_mg, vfit, col = \"black\")\n\n\n\n\nRegression with spatial correlation\nWe have seen above that the gstat package allows us to estimate the variogram of the residuals of a linear model. In our example, the magnesium concentration was modeled as a function of pH, with spatially correlated residuals.\nAnother tool to fit this same type of model is the gls function of the nlme package, which is included with the installation of R.\nThis function applies the generalized least squares method to fit linear regression models when the residuals are not independent or when the residual variance is not the same for all observations. Since the estimates of the coefficients depend on the estimated correlations between the residuals and the residuals themselves depend on the coefficients, the model is fitted by an iterative algorithm:\nA classical linear regression model (without correlation) is fitted to obtain residuals.\nThe spatial correlation model (variogram) is fitted with those residuals.\nThe regression coefficients are re-estimated, now taking into account the correlations.\nSteps 2 and 3 are repeated until the estimates are stable at a desired precision.\nHere is the application of this method to the same model for the magnesium concentration in the oxford dataset. In the correlation argument of gls, we specify an exponential correlation model as a function of our spatial coordinates and we include a possible nugget effect.\nIn addition to the exponential correlation corExp, the gls function can also estimate a Gaussian (corGaus) or spherical (corSpher) model.\n\n\nlibrary(nlme)\ngls_mg <- gls(MG1 ~ PH1, oxford, \n              correlation = corExp(form = ~ XCOORD + YCOORD, nugget = TRUE))\nsummary(gls_mg)\n\n\nGeneralized least squares fit by REML\n  Model: MG1 ~ PH1 \n  Data: oxford \n      AIC      BIC   logLik\n  1278.65 1292.751 -634.325\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~XCOORD + YCOORD \n Parameter estimate(s):\n      range      nugget \n478.0322959   0.2944753 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 391.1387  50.42343  7.757084       0\nPH1         -41.0836   6.15662 -6.673079       0\n\n Correlation: \n    (Intr)\nPH1 -0.891\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.1846957 -0.6684520 -0.3687813  0.4627580  3.1918604 \n\nResidual standard error: 53.8233 \nDegrees of freedom: 126 total; 124 residual\n\nTo compare this result with the adjusted variogram above, the parameters given by gls must be transformed. The range has the same meaning in both cases and corresponds to 478 m for the result of gls. The global variance of the residuals is the square of Residual standard error. The nugget effect here (0.294) is expressed as a fraction of that variance. Finally, to obtain the partial sill of the exponential part, the nugget effect must be subtracted from the total variance.\nAfter performing these calculations, we can give these parameters to the vgm function of gstat to superimpose this variogram estimated by gls on our variogram of the residuals of the classical linear model.\n\n\ngls_range <- 478\ngls_var <- 53.823^2\ngls_nugget <- 0.294 * gls_var\ngls_psill <- gls_var - gls_nugget\n\ngls_vgm <- vgm(\"Exp\", psill = gls_psill, range = gls_range, nugget = gls_nugget)\n\nplot(var_mg, gls_vgm, col = \"black\", ylim = c(0, 4000))\n\n\n\n\nDoes the model fit the data less well here? In fact, this empirical variogram represented by the points was obtained from the residuals of the linear model ignoring the spatial correlation, so it is a biased estimate of the actual spatial correlations. The method is still adequate to quickly check if spatial correlations are present. However, to simultaneously fit the regression coefficients and the spatial correlation parameters, the generalized least squares (GLS) approach is preferable and will produce more accurate estimates.\nFinally, note that the result of the gls model also gives the AIC, which we can use to compare the fit of different models (with different predictors or different forms of spatial correlation).\nExercise\nThe bryo_belg.csv dataset is adapted from the data of this study:\n\nNeyens, T., Diggle, P.J., Faes, C., Beenaerts, N., Artois, T. et Giorgi, E. (2019) Mapping species richness using opportunistic samples: a case study on ground-floor bryophyte species richness in the Belgian province of Limburg. Scientific Reports 9, 19122. https://doi.org/10.1038/s41598-019-55593-x\n\nThis data frame shows the specific richness of ground bryophytes (richness) for different sampling points in the Belgian province of Limburg, with their position (x, y) in km, in addition to information on the proportion of forest (forest) and wetlands (wetland) in a 1 km^2$ cell containing the sampling point.\n\n\nbryo_belg <- read.csv(\"data/bryo_belg.csv\")\nhead(bryo_belg)\n\n\n  richness    forest   wetland        x        y\n1        9 0.2556721 0.5036614 228.9516 220.8869\n2        6 0.6449114 0.1172068 227.6714 219.8613\n3        5 0.5039905 0.6327003 228.8252 220.1073\n4        3 0.5987329 0.2432942 229.2775 218.9035\n5        2 0.7600775 0.1163538 209.2435 215.2414\n6       10 0.6865434 0.0000000 210.4142 216.5579\n\nFor this exercise, we will use the square root of the specific richness as the response variable. The square root transformation often allows to homogenize the variance of the count data in order to apply a linear regression.\nFit a linear model of the transformed species richness to the proportion of forest and wetlands, without taking into account spatial correlations. What is the effect of the two predictors in this model?\nCalculate the empirical variogram of the model residuals in (a). Does there appear to be a spatial correlation between the points?\nNote: The cutoff argument to the variogram function specifies the maximum distance at which the variogram is calculated. You can manually adjust this value to get a good view of the sill.\nRe-fit the linear model in (a) with the gls function in the nlme package, trying different types of spatial correlations (exponential, Gaussian, spherical). Compare the models (including the one without spatial correlation) with the AIC.\nWhat is the effect of the proportion of forests and wetlands according to the model in (c)? Explain the differences between the conclusions of this model and the model in (a).\nKriging\nAs mentioned before, a common application of geostatistical models is to predict the value of the response variable at unsampled locations, a form of spatial interpolation called kriging (pronounced with a hard “g”).\nThere are three basic types of kriging based on the assumptions made about the response variable:\nOrdinary kriging: Stationary variable with an unknown mean.\nSimple kriging: Stationary variable with a known mean.\nUniversal kriging: Variable with a trend given by a linear or non-linear model.\nFor all kriging methods, the predictions at a new point are a weighted mean of the values at known points. These weights are chosen so that kriging provides the best linear unbiased prediction of the response variable, if the model assumptions (in particular the variogram) are correct. That is, among all possible unbiased predictions, the weights are chosen to give the minimum mean square error. Kriging also provides an estimate of the uncertainty of each prediction.\nWhile we will not present the detailed kriging equations here, the weights depend on both the correlations (estimated by the variogram) between the sampled points and the new point, as well of the correlations between the sampled points themselves. In other words, sampled points near the new point are given more weight, but isolated sampled points are also given more weight, because sample points close to each other provide redundant information.\nKriging is an interpolation method, so the prediction at a sampled point will always be equal to the measured value (the measurement is supposed to have no error, just spatial variation). However, in the presence of a nugget effect, any small displacement from the sampled location will show variability according to the nugget.\nIn the example below, we generate a new dataset composed of randomly-generated (x, y) coordinates within the study area as well as randomly-generated pH values based on the oxford data. We then apply the function krige to predict the magnesium values at these new points. Note that we specify the variogram derived from the GLS results in the model argument to krige.\n\n\nset.seed(14)\nnew_points <- data.frame(\n    XCOORD = runif(100, min(oxford$XCOORD), max(oxford$XCOORD)),\n    YCOORD = runif(100, min(oxford$YCOORD), max(oxford$YCOORD)),\n    PH1 = rnorm(100, mean(oxford$PH1), sd(oxford$PH1))\n)\n\npred <- krige(MG1 ~ PH1, locations = ~ XCOORD + YCOORD, data = oxford,\n              newdata = new_points, model = gls_vgm)\n\n\n[using universal kriging]\n\nhead(pred)\n\n\n    XCOORD    YCOORD var1.pred var1.var\n1 227.0169  162.1185  47.13065 1269.002\n2 418.9136  465.9013  79.68437 1427.269\n3 578.5943 2032.7477  60.30539 1264.471\n4 376.2734 1530.7193 127.22366 1412.875\n5 591.5336  421.6290 105.88124 1375.485\n6 355.7369  404.3378 127.73055 1250.114\n\nThe result of krige includes the new point coordinates, the prediction of the variable var1.pred along with its estimated variance var1.var. In the graph below, we show the mean MG1 predictions from kriging (triangles) along with the measurements (circles).\n\n\npred$MG1 <- pred$var1.pred\n\nggplot(oxford, aes(x = YCOORD, y = XCOORD, color = MG1)) +\n    geom_point() +\n    geom_point(data = pred, shape = 17, size = 2) +\n    coord_fixed()\n\n\n\n\nThe estimated mean and variance from kriging can be used to simulate possible values of the variable at each new point, conditional on the sampled values. In the example below, we performed 4 conditional simulations by adding the argument nsim = 4 to the same krige instruction.\n\n\nsim_mg <- krige(MG1 ~ PH1, locations = ~ XCOORD + YCOORD, data = oxford,\n                newdata = new_points, model = gls_vgm, nsim = 4)\n\n\ndrawing 4 GLS realisations of beta...\n[using conditional Gaussian simulation]\n\nhead(sim_mg)\n\n\n    XCOORD    YCOORD      sim1      sim2      sim3      sim4\n1 227.0169  162.1185  13.22592  32.43060  42.81847  79.60594\n2 418.9136  465.9013  67.94216  15.53717  69.25356  63.42233\n3 578.5943 2032.7477  99.93083  77.98291  74.28468  58.98483\n4 376.2734 1530.7193 104.86240 155.50774  85.82552 143.07373\n5 591.5336  421.6290  78.14221  68.62827 147.33052 130.14264\n6 355.7369  404.3378 164.46754 117.26160 131.85158 143.58951\n\n\n\nlibrary(tidyr)\nsim_mg <- pivot_longer(sim_mg, cols = c(sim1, sim2, sim3, sim4), \n                       names_to = \"sim\", values_to = \"MG1\")\nggplot(sim_mg, aes(x = YCOORD, y = XCOORD, color = MG1)) +\n    geom_point() +\n    coord_fixed() +\n    facet_wrap(~ sim)\n\n\n\n\nSolutions\n\n\nbryo_lm <- lm(sqrt(richness) ~ forest + wetland, data = bryo_belg)\nsummary(bryo_lm)\n\n\n\nCall:\nlm(formula = sqrt(richness) ~ forest + wetland, data = bryo_belg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8847 -0.4622  0.0545  0.4974  2.3116 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.34159    0.08369  27.981  < 2e-16 ***\nforest       1.11883    0.13925   8.034 9.74e-15 ***\nwetland     -0.59264    0.17216  -3.442 0.000635 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7095 on 417 degrees of freedom\nMultiple R-squared:  0.2231,    Adjusted R-squared:  0.2193 \nF-statistic: 59.86 on 2 and 417 DF,  p-value: < 2.2e-16\n\nThe proportion of forest has a significant positive effect and the proportion of wetlands has a significant negative effect on bryophyte richness.\n\n\nplot(variogram(sqrt(richness) ~ forest + wetland, locations = ~ x + y,\n               data = bryo_belg, cutoff = 50), col = \"black\")\n\n\n\n\nThe variogram is increasing from 0 to at least 40 km, so there appears to be spatial correlations in the model residuals.\n\n\nbryo_exp <- gls(sqrt(richness) ~ forest + wetland, data = bryo_belg,\n                correlation = corExp(form = ~ x + y, nugget = TRUE))\nbryo_gaus <- gls(sqrt(richness) ~ forest + wetland, data = bryo_belg,\n                correlation = corGaus(form = ~ x + y, nugget = TRUE))\nbryo_spher <- gls(sqrt(richness) ~ forest + wetland, data = bryo_belg,\n                  correlation = corSpher(form = ~ x + y, nugget = TRUE))\n\n\n\n\n\nAIC(bryo_lm)\n\n\n[1] 908.6358\n\nAIC(bryo_exp)\n\n\n[1] 867.822\n\nAIC(bryo_gaus)\n\n\n[1] 870.9592\n\nAIC(bryo_spher)\n\n\n[1] 866.9117\n\nThe spherical model has the smallest AIC.\n\n\nsummary(bryo_spher)\n\n\nGeneralized least squares fit by REML\n  Model: sqrt(richness) ~ forest + wetland \n  Data: bryo_belg \n       AIC      BIC    logLik\n  866.9117 891.1102 -427.4558\n\nCorrelation Structure: Spherical spatial correlation\n Formula: ~x + y \n Parameter estimate(s):\n     range     nugget \n43.1725704  0.6063077 \n\nCoefficients:\n                 Value Std.Error   t-value p-value\n(Intercept)  2.0368754 0.2481673  8.207671   0.000\nforest       0.6989805 0.1481691  4.717450   0.000\nwetland     -0.2441117 0.1809121 -1.349339   0.178\n\n Correlation: \n        (Intr) forest\nforest  -0.251       \nwetland -0.235  0.241\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.75202529 -0.06568241  0.61415377  1.15239953  3.23320744 \n\nResidual standard error: 0.799832 \nDegrees of freedom: 420 total; 417 residual\n\nBoth effects are less important in magnitude and the effect of wetlands is not significant anymore. As is the case for other types of non-independent residuals, the “effective sample size” here is less than the number of points, since points close to each other provide redundant information. Therefore, the relationship between predictors and response is less clear than given by the model assuming all these points were independent.\nNote that the results for all three gls models are quite similar, so the choice to include spatial correlations was more important than the exact shape assumed for the variogram.\n\n\n\n",
    "preview": "posts/2021-01-13-spatial-statistics-in-ecology/spatial-statistics-in-ecology_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-20T21:46:39+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-12-spatial-statistics-in-ecology/",
    "title": "Spatial Statistics in Ecology, Part 1",
    "description": "Introduction to spatial statistics offered by Philippe Marchand to BIOS2 Fellows in January 2021. This first part focuses on concepts and initial analyses.",
    "author": [
      {
        "name": "Philippe Marchand",
        "url": {}
      }
    ],
    "date": "2021-01-12",
    "categories": [
      "Technical",
      "English"
    ],
    "contents": "\n\nContents\nIntroduction to spatial statistics\nTypes of spatial analyses\nStationarity and isotropy\nGeoreferenced data\n\nPoint pattern analysis\nPoint pattern and point process\nComplete spatial randomness\nExploratory or inferential analysis for a point pattern\nRipley’s K function\nEdge effects\nExample\nExercise 1\n\nEffect of heterogeneity\nExercise 2\n\nRelationship between two point patterns\nQuestions\n\nMarked point patterns\nReferences\n\nSolutions\nExercise 1\nExercise 2\n\n\nIntroduction to spatial statistics\nTypes of spatial analyses\nIn this training, we will discuss three types of spatial analyses: point pattern analysis, geostatistical models and models for areal data.\nIn point pattern analysis, we have point data representing the position of individuals or events in a study area and we assume that all individuals or events have been identified in that area. That analysis focuses on the distribution of the positions of the points themselves. Here are some typical questions for the analysis of point patterns:\nAre the points randomly arranged or clustered?\nAre two types of points arranged independently?\nGeostatistical models represent the spatial distribution of continuous variables that are measured at certain sampling points. They assume that measurements of those variables at different points are correlated as a function of the distance between the points. Applications of geostatistical models include the smoothing of spatial data (e.g., producing a map of a variable over an entire region based on point measurements) and the prediction of those variables for non-sampled points.\nAreal data are measurements taken not at points, but for regions of space represented by polygons (e.g. administrative divisions, grid cells). Models representing these types of data define a network linking each region to its neighbours and include correlations in the variable of interest between neighbouring regions.\nStationarity and isotropy\nSeveral spatial analyses assume that the variables are stationary in space. As with stationarity in the time domain, this property means that summary statistics (mean, variance and correlations between measures of a variable) do not vary with translation in space. For example, the spatial correlation between two points may depend on the distance between them, but not on their absolute position.\nIn particular, there cannot be a large-scale trend (often called gradient in a spatial context), or this trend must be taken into account before modelling the spatial correlation of residuals.\nIn the case of point pattern analysis, stationarity (also called homogeneity) means that point density does not follow a large-scale trend.\nIn a isotropic statistical model, the spatial correlations between measurements at two points depend only on the distance between the points, not on the direction. In this case, the summary statistics do not change under a spatial rotation of the data.\nGeoreferenced data\nEnvironmental studies increasingly use data from geospatial data sources, i.e. variables measured over a large part of the globe (e.g. climate, remote sensing). The processing of these data requires concepts related to Geographic Information Systems (GIS), which are not covered in this workshop, where we focus on the statistical aspects of spatially varying data.\nThe use of geospatial data does not necessarily mean that spatial statistics are required. For example, we will often extract values of geographic variables at study points to explain a biological response observed in the field. In this case, the use of spatial statistics is only necessary when there is a spatial correlation in the residuals, after controlling for the effect of the predictors.\nPoint pattern analysis\nPoint pattern and point process\nA point pattern describes the spatial position (most often in 2D) of individuals or events, represented by points, in a given study area, often called the observation “window”.\nIt is assumed that each point has a negligible spatial extent relative to the distances between the points. More complex methods exist to deal with spatial patterns of objects that have a non-negligible width, but this topic is beyond the scope of this workshop.\nA point process is a statistical model that can be used to simulate point patterns or explain an observed point pattern.\nComplete spatial randomness\nComplete spatial randomness (CSR) is one of the simplest point patterns, which serves as a null model for evaluating the characteristics of real point patterns. In this pattern, the presence of a point at a given position is independent of the presence of points in a neighbourhood.\nThe process creating this pattern is a homogeneous Poisson process. According to this model, the number of points in any area \\(A\\) follows a Poisson distribution: \\(N(A) \\sim \\text{Pois}(\\lambda A)\\), where \\(\\lambda\\) is the intensity of the process (i.e. the density of points per unit area). \\(N\\) is independent between two disjoint regions, no matter how those regions are defined.\nIn the graph below, only the pattern on the right is completely random. The pattern on the left shows point aggregation (higher probability of observing a point close to another point), while the pattern in the center shows repulsion (low probability of observing a point very close to another).\n\n\n\nExploratory or inferential analysis for a point pattern\nSeveral summary statistics are used to describe the characteristics of a point pattern. The simplest is the intensity \\(\\lambda\\), which as mentioned above represents the density of points per unit area. If the point pattern is heterogeneous, the intensity is not constant, but depends on the position: \\(\\lambda(x, y)\\).\nCompared to intensity, which is a first-order statistic, second-order statistics describe how the probability of the presence of a point in a region depends on the presence of other points. The Ripley’s \\(K\\) function presented in the next section is an example of a second-order summary statistic.\nStatistical inferences on point patterns usually consist of testing the hypothesis that the point pattern corresponds to a given null model, such as CSR or a more complex null model. Even for the simplest null models, we rarely know the theoretical distribution for a summary statistic of the point pattern under the null model. Hypothesis tests on point patterns are therefore performed by simulation: a large number of point patterns are simulated from the null model and the distribution of the summary statistics of interest for these simulations is compared to their values for the observed point pattern.\nRipley’s K function\nRipley’s K function \\(K(r)\\) is defined as the mean number of points within a circle of radius \\(r\\) around a point in the pattern, standardized by the intensity \\(\\lambda\\).\nUnder the CSR null model, the mean number of points in any circle of radius \\(r\\) is \\(\\lambda \\pi r^2\\), thus in theory \\(K(r) = \\pi r^2\\) for that model. A higher value of \\(K(r)\\) means that there is an aggregation of points at the scale \\(r\\), whereas a lower value means that there is repulsion.\nIn practice, \\(K(r)\\) is estimated for a specific point pattern by the equation:\n\\[ K(r) = \\frac{A}{n(n-1)} \\sum_i \\sum_{j > i} I \\left( d_{ij} \\le r \\right) w_{ij}\\]\nwhere \\(A\\) is the area of the observation window and \\(n\\) is the number of points in the pattern, so \\(n(n-1)\\) is the number of distinct pairs of points. We take the sum for all pairs of points of the indicator function \\(I\\), which takes a value of 1 if the distance between points \\(i\\) and \\(j\\) is less than or equal to \\(r\\). Finally, the term \\(w_{ij}\\) is used to give extra weight to certain pairs of points to account for edge effects, as discussed in the next section.\nFor example, the graphs below show the estimated \\(K(r)\\) function for the patterns shown above, for values of \\(r\\) up to 1/4 of the window width. The red dashed curve shows the theoretical value for CSR and the gray area is an “envelope” produced by 99 simulations of that null pattern. The aggregated pattern shows an excess of neighbours up to \\(r = 0.25\\) and the pattern with repulsion shows a significant deficit of neighbours for small values of \\(r\\).\n\n\n\nIn addition to \\(K\\), there are other statistics to describe the second-order properties of point patterns, such as the mean distance between a point and its nearest \\(N\\) neighbours. You can refer to the Wiegand and Moloney (2013) textbook in the references to learn more about different summary statistics for point patterns.\nEdge effects\nIn the context of point pattern analysis, edge effects are due to the fact that we have incomplete knowledge of the neighbourhood of points near the edge of the observation window, which can induce a bias in the calculation of statistics such as Ripley’s \\(K\\).\nDifferent methods have been developed to correct the bias due to edge effects. In Ripley’s edge correction method, the contribution of a neighbour \\(j\\) located at a distance \\(r\\) from a point \\(i\\) receives a weight \\(w_{ij} = 1/\\phi_i(r)\\), where \\(\\phi_i(r)\\) is the fraction of the circle of radius \\(r\\) around \\(i\\) contained in the observation window. For example, if 2/3 of the circle is in the window, this neighbour counts as 3/2 neighbours in the calculation of a statistic like \\(K\\).\n\nRipley’s method is one of the simplest to correct for edge effects, but is not necessarily the most efficient; in particular, larger weights given to certain pairs of points tend to increase the variance of the calculated statistic. Other correction methods are presented in specialized textbooks, such as Wiegand and Moloney (2013).\nExample\nFor this example, we use the dataset semis_xy.csv, which represents the \\((x, y)\\) coordinates for seedlings of two species (sp, B = birch and P = poplar) in a 15 x 15 m plot.\n\n\nsemis <- read.csv(\"data/semis_xy.csv\")\nhead(semis)\n\n\n      x    y sp\n1 14.73 0.05  P\n2 14.72 1.71  P\n3 14.31 2.06  P\n4 14.16 2.64  P\n5 14.12 4.15  B\n6  9.88 4.08  B\n\nThe spatstat package provides tools for point pattern analysis in R. The first step consists in transforming our data frame into a ppp object (point pattern) with the function of the same name. In this function, we specify which columns contain the coordinates x and y as well as the marks, which here will be the species codes. We also need to specify an observation window (window) using the owin function, where we provide the plot limits in x and y.\n\n\nlibrary(spatstat)\n\nsemis <- ppp(x = semis$x, y = semis$y, marks = as.factor(semis$sp),\n             window = owin(xrange = c(0, 15), yrange = c(0, 15)))\nsemis\n\n\nMarked planar point pattern: 281 points\nMultitype, with levels = B, P \nwindow: rectangle = [0, 15] x [0, 15] units\n\nMarks can be numeric or categorical. Note that for categorical marks as is the case here, the variable must be explicitly converted to a factor.\nThe plot function applied to a point pattern shows a diagram of the pattern.\n\n\nplot(semis)\n\n\n\n\nThe intensity function calculates the density of points of each species by unit area (here, by \\(m^2\\)).\n\n\nintensity(semis)\n\n\n        B         P \n0.6666667 0.5822222 \n\nTo first analyze the distribution of each species separately, we split the pattern with split. Since the pattern contains categorical marks, it is automatically split according to the values of those marks. The result is a list of two point patterns.\n\n\nsemis_split <- split(semis)\nplot(semis_split)\n\n\n\n\nThe Kest function calculates Ripley’s \\(K\\) for a series of distances up to (by default) 1/4 of the width of the window. Here we apply it to the first pattern (birch) by choosing semis_split[[1]]. Note that double square brackets are necessary to choose an item from a list in R.\nThe argument correction = \"iso\" tells the function to apply Ripley’s correction for edge effects.\n\n\nk <- Kest(semis_split[[1]], correction = \"iso\")\nplot(k)\n\n\n\n\nAccording to this graph, there seems to be an excess of neighbours for distances of 1 m and above. To check if this is a significant difference, we produce a simulation envelope with the envelope function. The first argument of envelope is a point pattern to which the simulations will be compared, the second one is a function to be computed (here, Kest) for each simulated pattern, then we add the arguments of the Kest function (here, only correction).\n\n\nplot(envelope(semis_split[[1]], Kest, correction = \"iso\"))\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\nAs indicated by the message, by default the function performs 99 simulations of the null model corresponding to complete spatial randomness (CSR).\nThe observed curve falls outside the envelope of the 99 simulations near \\(r = 2\\). We must be careful not to interpret too quickly a result that is outside the envelope. Although there is about a 1% probability of obtaining a more extreme result under the null hypothesis at a given distance, the envelope is calculated for a large number of values of \\(r\\) and is not corrected for multiple comparisons. Thus, a significant difference for a very small range of values of \\(r\\) may be simply due to chance.\nExercise 1\nLooking at the graph of the second point pattern (poplar seedlings), can you predict where Ripley’s \\(K\\) will be in relation to the null hypothesis of complete spatial randomness? Verify your prediction by calculating Ripley’s \\(K\\) for this point pattern in R.\nEffect of heterogeneity\nThe graph below illustrates a heterogeneous point pattern, i.e. it shows an density gradient (more points on the left than on the right).\n\n\n\nA density gradient can be confused with an aggregation of points, as can be seen on the graph of the corresponding Ripley’s \\(K\\). In theory, these are two different processes:\nHeterogeneity: The density of points varies in the study area, for example due to the fact that certain local conditions are more favorable to the presence of the species of interest.\nAggregation: The mean density of points is homogeneous, but the presence of one point increases the presence of other points in its vicinity, for example due to positive interactions between individuals.\nHowever, it may be difficult to differentiate between the two in practice, especially since some patterns may be both heterogeneous and aggregated.\nLet’s take the example of the poplar seedlings from the previous exercise. The density function applied to a point pattern performs a kernel density estimation of the density of the seedlings across the plot. By default, this function uses a Gaussian kernel with a standard deviation sigma specified in the function, which determines the scale at which density fluctuations are “smoothed”. Here, we use a value of 2 m for sigma and we first represent the estimated density with plot, before overlaying the points (add = TRUE means that the points are added to the existing plot rather than creating a new plot).\n\n\ndens_p <- density(semis_split[[2]], sigma = 2)\nplot(dens_p)\nplot(semis_split[[2]], add = TRUE)\n\n\n\n\nTo measure the aggregation or repulsion of points in a heterogeneous pattern, we must use the inhomogeneous version of the \\(K\\) statistic (Kinhom in spatstat). This statistic is still equal to the mean number of neighbours within a radius \\(r\\) of a point in the pattern, but rather than standardizing this number by the overall intensity of the pattern, it is standardized by the local estimated density. As above, we specify sigma = 2 to control the level of smoothing for the varying density estimate.\n\n\nplot(Kinhom(semis_split[[2]], sigma = 2, correction = \"iso\"))\n\n\n\n\nTaking into account the heterogeneity of the pattern at a scale sigma of 2 m, there seems to be a deficit of neighbours starting at a radius of about 1.5 m. We can now check whether this deviation is significant.\nAs before, we use envelope to simulate the Kinhom statistic under the null model. However, the null model here is not a homogeneous Poisson process (CSR). It is instead a heterogeneous Poisson process simulated by the function rpoispp(dens_p), i.e. the points are independent of each other, but their density is heterogeneous and given by dens_p. The simulate argument of the envelope function specifies the function used for simulations under the null model; this function must have one argument, here x, even if it is not used.\nFinally, in addition to the arguments needed for Kinhom, i.e. sigma and correction, we also specify nsim = 199 to perform 199 simulations and nrank = 5 to eliminate the 5 most extreme results on each side of the envelope, i.e. the 10 most extreme results out of 199, to achieve an interval containing about 95% of the probability under the null hypothesis.\n\n\nkhet_p <- envelope(semis_split[[2]], Kinhom, sigma = 2,  correction = \"iso\",\n                   nsim = 199, nrank = 5, simulate = function(x) rpoispp(dens_p))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(khet_p)\n\n\n\n\nNote: For a hypothesis test based on simulations of a null hypothesis, the \\(p\\)-value is estimated by \\((m + 1)/(n + 1)\\), where \\(n\\) is the number of simulations and \\(m\\) is the number of simulations where the value of the statistic is more extreme than that of the observed data. This is why the number of simulations is often chosen to be 99, 199, etc.\nExercise 2\nRepeat the heterogeneous density estimation and Kinhom calculation with a standard deviation sigma of 5 rather than 2. How does the smoothing level for the density estimation influence the conclusions?\nTo differentiate between a variation in the density of points from an interaction (aggregation or repulsion) between these points with this type of analysis, it is generally assumed that the two processes operate at different scales. Typically, we can test whether the points are aggregated at a small scale after accounting for a variation in density at a larger scale.\nRelationship between two point patterns\nLet’s consider a case where we have two point patterns, for example the position of trees of two species in a plot (orange and green points in the graph below). Each of the two patterns may or may not present an aggregation of points.\n\n\n\nRegardless of whether points are aggregated at the species level, we want to determine whether the two species are arranged independently. In other words, does the probability of observing a tree of one species depend on the presence of a tree of the other species at a given distance?\nThe bivariate version of Ripley’s \\(K\\) allows us to answer this question. For two patterns noted 1 and 2, the function \\(K_{12}(r)\\) calculates the mean number of points in pattern 2 within a radius \\(r\\) from a point in pattern 1, standardized by the density of pattern 2.\nIn theory, this function is symmetrical, so \\(K_{12}(r) = K_{21}(r)\\) and the result would be the same whether the points of pattern 1 or 2 are chosen as “focal” points for the analysis. However, the estimation of the two quantities for an observed pattern may differ, in particular because of edge effects. The variance of \\(K_{12}\\) and \\(K_{21}\\) between simulations of a null model may also differ, so the null hypothesis test may have more or less power depending on the choice of the focal species.\nThe choice of an appropriate null model is important here. In order to determine whether there is a significant attraction or repulsion between the two patterns, the position of one of the patterns must be randomly moved relative to that of the other pattern, while keeping the spatial structure of each pattern taken in isolation.\nOne way to do this randomization is to shift one of the two patterns horizontally and/or vertically by a random distance. The part of the pattern that “comes out” on one side of the window is attached to the other side. This method is called a toroidal shift, because by connecting the top and bottom as well as the left and right of a rectangular surface, we obtain the shape of a torus (a three-dimensional “donut”).\n\n\n\nThe graph above shows a translation of the green pattern to the right, while the orange pattern remains in the same place. The green points in the shaded area are brought back on the other side. Note that while this method generally preserves the structure of each pattern while randomizing their relative position, it can have some drawbacks, such as dividing point clusters that are near the cutoff point.\nLet’s now check whether the position of the two species (birch and poplar) is independent in our plot. The function Kcross calculates the bivariate \\(K_{ij}\\), we must specify which type of point (mark) is considered as the focal species \\(i\\) and the neighbouring species \\(j\\).\n\n\nplot(Kcross(semis, i = \"P\", j = \"B\", correction = \"iso\"))\n\n\n\n\nHere, the observed \\(K\\) is lower than the theoretical value, indicating a possible repulsion between the two patterns.\nTo determine the envelope of the \\(K\\) under the null hypothesis of independence of the two patterns, we must specify that the simulations are based on a translation of the patterns. We indicate that the simulations use the function rshift (random translation) with the argument simulate = function(x) rshift(x, which = \"B\"); here, the x argument in simulate corresponds to the original point pattern and the which argument indicates which of the patterns is translated. As in the previous case, the arguments needed for Kcross, i.e. i, j and correction, must be repeated in the envelope function.\n\n\nplot(envelope(semis, Kcross, i = \"P\", j = \"B\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = function(x) rshift(x, which = \"B\")))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\nHere, the observed curve is totally within the envelope, so we do not reject the null hypothesis of independence of the two patterns.\nQuestions\nWhat would be one reason for our choice to translate the points of the birch rather than poplar?\nWould the simulations generated by random translation be a good null model if the two patterns were heterogeneous?\nMarked point patterns\nThe fir.csv dataset contains the \\((x, y)\\) coordinates of 822 fir trees in a 1 hectare plot and their status (A = alive, D = dead) following a spruce budworm outbreak.\n\n\nfir <- read.csv(\"data/fir.csv\")\nhead(fir)\n\n\n      x     y status\n1 31.50  1.00      A\n2 85.25 30.75      D\n3 83.50 38.50      A\n4 84.00 37.75      A\n5 83.00 33.25      A\n6 33.25  0.25      A\n\n\n\nfir <- ppp(x = fir$x, y = fir$y, marks = as.factor(fir$status),\n           window = owin(xrange = c(0, 100), yrange = c(0, 100)))\nplot(fir)\n\n\n\n\nSuppose that we want to check whether fir mortality is independent or correlated between neighbouring trees. How does this question differ from the previous example, where we wanted to know if the position of the points of two species was independent?\nIn the previous example, the independence or interaction between the species referred to the formation of the pattern itself (whether or not seedlings of one species establish near those of the other species). Here, the characteristic of interest (survival) occurs after the establishment of the pattern, assuming that all those trees were alive at first and that some died as a result of the outbreak. So we take the position of the trees as fixed and we want to know whether the distribution of status (dead, alive) among those trees is random or shows a spatial pattern.\nIn Wiegand and Moloney’s textbook, the first situation (establishment of seedlings of two species) is called a bivariate pattern, so it is really two interacting patterns, while the second is a single pattern with a qualitative mark. The spatstat package in R does not differentiate between the two in terms of pattern definition (types of points are always represented by the marks argument), but the analysis methods applied to the two questions differ.\nIn the case of a pattern with a qualitative mark, we can define a mark connection function \\(p_{ij}(r)\\). For two points separated by a distance \\(r\\), this function gives the probability that the first point has the mark \\(i\\) and the second the mark \\(j\\). Under the null hypothesis where the marks are independent, this probability is equal to the product of the proportions of each mark in the entire pattern, \\(p_{ij}(r) = p_i p_j\\) independently of \\(r\\).\nIn spatstat, the mark connection function is computed with the markconnect function, where the marks \\(i\\) and \\(j\\) and the type of edge correction must be specified. In our example, we see that two closely spaced points are less likely to have a different status (A and D) than expected under the assumption of random and independent distribution of marks (red dotted line).\n\n\nplot(markconnect(fir, i = \"A\", j = \"D\", correction = \"iso\"))\n\n\n\n\nIn this graph, the fluctuations in the function are due to the estimation error of a continuous \\(r\\) function from a limited number of discrete point pairs.\nTo simulate the null model in this case, we use the rlabel function, which randomly reassigns the marks among the points of the pattern, keeping the points’ positions fixed.\n\n\nplot(envelope(fir, markconnect, i = \"A\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\nNote that since the rlabel function has only one required argument corresponding to the original point pattern, it was not necessary to specify: simulate = function(x) rlabel(x).\nHere are the results for tree pairs of the same status A or D:\n\n\npar(mfrow = c(1, 2))\nplot(envelope(fir, markconnect, i = \"A\", j = \"A\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(envelope(fir, markconnect, i = \"D\", j = \"D\", correction = \"iso\", \n              nsim = 199, nrank = 5, simulate = rlabel))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\n\nIt therefore appears that fir mortality due to this outbreak is spatially aggregated, since trees located in close proximity to each other have a greater probability of sharing the same status than predicted by the null hypothesis.\nReferences\nFortin, M.-J. and Dale, M.R.T. (2005) Spatial Analysis: A Guide for Ecologists. Cambridge University Press: Cambridge, UK.\nWiegand, T. and Moloney, K.A. (2013) Handbook of Spatial Point-Pattern Analysis in Ecology, CRC Press.\nThe dataset in the last example is a subet of the Lake Duparquet Research and Teaching Forest (LDRTF) data, available on Dryad here.\nSolutions\nExercise 1\n\n\nplot(envelope(semis_split[[2]], Kest, correction = \"iso\"))\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\nPoplar seedlings seem to be significantly aggregated according to the \\(K\\) function.\nExercise 2\n\n\ndens_p <- density(semis_split[[2]], sigma = 5)\nplot(dens_p)\nplot(semis_split[[2]], add = TRUE)\n\n\n\nkhet_p <- envelope(semis_split[[2]], Kinhom, sigma = 5, correction = \"iso\",\n                   nsim = 199, nrank = 5, simulate = function(x) rpoispp(dens_p))\n\n\nGenerating 199 simulations by evaluating function  ...\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34.36\n.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72\n.74.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108\n.110.112.114.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144\n.146.148.150.152.154.156.158.160.162.164.166.168.170.172.174.176.178.180\n.182.184.186.188.190.192.194.196.198 199.\n\nDone.\n\nplot(khet_p)\n\n\n\n\nHere, as we estimate density variations at a larger scale, even after accounting for this variation, the poplar seedlings seem to be aggregated at a small scale.\n\n\n\n",
    "preview": "posts/2021-01-12-spatial-statistics-in-ecology/spatial-statistics-in-ecology_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-19T21:35:27+00:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 576
  },
  {
    "path": "posts/2020-06-15-science-communication/",
    "title": "Science Communication",
    "description": "Recordings, content and handouts from a 6-hour Science Communication workshop held over two days on 15 and 16 June 2020.",
    "author": [
      {
        "name": "Gracielle Higino",
        "url": {}
      },
      {
        "name": "Katherine Hébert",
        "url": {}
      }
    ],
    "date": "2020-06-15",
    "categories": [
      "Training",
      "Fellow contributed",
      "Six hour"
    ],
    "contents": "\nThe objective of this training is to share and discuss the concepts and tools that contribute to effective science communication. The training is split into two sessions, which cover the basic concepts of effective science communication and how social media tools can be used to boost the signal of your research and extend your research network. Each training takes the form of a presentation interspersed with several short activity modules, where participants are invited to use the tools we will be discussing to kickstart their own science communication.\nThis training was given on June 1 and 2, 2020. You can view recordings of each session here:\nDay 1\n\n\nDay 2\n\n\nTraining materials can be downloaded as “Supporting Data Files” at the top of the page.\nSession 1: The basics of science communication\nObjectives:\nDiscuss what science communication (or SciComm) can be, and its potential role in boosting the signal of your research\nMake an overview of basic concepts and tools that you can use in any medium (blog posts, presentations, conversations, twitter, etc.) to do effective science communication\nDuring this session, we:\nDiscuss the potential pitfalls of science communication (notably, diversity and inclusivity problems).\nCover the basic concepts of science communication, including the Golden Circle method, the creation of personas, and storytelling techniques.\nHave short activities where participants can try to use some of the techniques we will be covering, such as filling in their own Golden Circle and explaining a blog post as a storyboard.\nSession 2: Social media as a science communication tool\nObjectives:\nRethink the way we write about science by exploring the world of blog posts\nClarify the mechanics of Twitter and how it can be used effectively for science communication\nDuring this session, we:\nDiscuss how to create a story structure using titles and the flow of ideas in blog posts, especially when we are used to writing scientific articles\nCover the basics of how Twitter works (retweets, threads, replies, hashtags, photo captions, etc.) and how to find helpful connections\nHave short activities where participants will be invited to write their own Twitter biographies and to create a Twitter thread explaining a project of their choice.\n\n\n\n",
    "preview": "posts/2020-06-15-science-communication/scicomm_training.png",
    "last_modified": "2021-04-15T21:20:32+00:00",
    "input_file": {},
    "preview_width": 1548,
    "preview_height": 800
  },
  {
    "path": "posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution/",
    "title": "Mathematical Modeling in Ecology and Evolution",
    "description": "This workshop will introduce participants to the logic behind modeling in biology, focusing on developing equations, finding equilibria, analyzing stability , and running simulations.  Techniques will be illustrated with the software tools, Mathematica and Maxima.  This workshop was held in two parts : January 14 and January 16, 2020.",
    "author": [
      {
        "name": "Dr Sarah P. Otto",
        "url": {}
      }
    ],
    "date": "2020-01-14",
    "categories": [
      "Training",
      "co-PI contributed",
      "Six hour"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-15T21:20:32+00:00",
    "input_file": {}
  }
]
