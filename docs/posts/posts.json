[
  {
    "path": "posts/2021-06-22-introduction-to-shiny-apps/",
    "title": "Introduction to Shiny Apps",
    "description": "Shiny is so great.",
    "author": [
      {
        "name": "Katherine Hébert",
        "url": {}
      },
      {
        "name": "Andrew MacDonald",
        "url": {}
      },
      {
        "name": "Jake Lawlor",
        "url": {}
      },
      {
        "name": "Vincent Bellevance",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\nPlots\nShiny is an excellent tool for visual exploration - it is at its most useful when a user can see something change before their eyes according to some selections. This is a great way to allow users to explore a dataset, explore the results of some analyses according to different parameters, and so on!\nLet’s now add a plot to our Shiny app, to visualize the distribution of a variable depending on user input. We’ll be adding the ggplot2 and ggridges packages in the set-up step at the top of our app.R to allow us to make a plot.\n\n\n# load packages\nlibrary(shiny)\nlibrary(ggridges)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(readr)\n\n\n\nUser interface\nTo add a plot in our Shiny, we need to indicate where the plot should appear in the app. We can do this with plotOutput(), a similar function to tableOutput() in the previous section that is meant for plot outputs, as the name suggests.\n\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  )\n)\n\n\n\nNow our Shiny app knows where we want to place our plot.\nServer\nWe now need to create the plot we want to show in our app. This plot will change depending on one or several reactive values that the user can input or select in our UI.\nWe link the UI and server together with IDs that are assigned to each object. Above, we told the UI to expect a plot output with the ID \"ridgePlot\". In the server, we will create a plot and render it as a plot object using renderPlot(), and we will assign this plot output to the ID we call in the UI (as output$ridgePlot).\n\n\n# Define server logic required to make your output(s)\nserver <- function(input, output) {\n\n  \n  # prepare the data\n  # ----------------------------------------------------------\n  \n  # read the dataset\n  eruptions <- readr::read_rds(here::here(\"data\", \"eruptions.rds\"))\n  \n  # filter the dataset to avoid overloading the plot \n  eruptions <- eruptions[which(eruptions$volcano_name %in% names(which(table(eruptions$volcano_name) > 30))),]\n  # this subsets to volcanoes that have erupted more than 30 times\n  \n  \n  # make reactive dataset\n  # ----------------------------------------------------------\n  \n  # subset volcano data with input year range\n  eruptions_filtered <- reactive({\n    subset(eruptions, start_year >= input$years[1] & end_year <= input$years[2])\n  })\n  \n    \n  # create and render the outputs\n  # ----------------------------------------------------------\n  \n  # create the table of volcanoes\n  output$erupt_table <- renderTable({\n    head(eruptions_filtered())\n  })\n  \n  # render the plot output\n  output$ridgePlot <- renderPlot({\n    \n    # create the plot\n    ggplot(data = eruptions_filtered(),\n           aes(x = vei,\n               y = volcano_name,\n               fill = volcano_name)) +\n      # we are using a ridgeplot geom here, from the ggridges package\n      geom_density_ridges( size = .5) + # line width\n      \n      # label the axes\n      labs(x = \"Volcano Explosivity Index\", y = \"\") +\n      \n      # adjust the ggplot theme to make the plot \"prettier\"\n      theme_classic() + \n      theme(legend.position = \"none\",\n            axis.text = element_text(size = 12, face = \"bold\"),\n            axis.title = element_text(size = 14, face = \"bold\"))\n  })\n}\n\n\n\nThe Shiny app\nNow, if we run the Shiny app, we have a plot above the table we made previously. They are positioned in this way because the plotOutput() comes before the tableOutput() in the UI.\n\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\nReview: How a Shiny app works\nBuilding blocks\nWe’ve now seen the basic building blocks of a Shiny app:\nThe user interface, which determines how the app “looks”. This is how we tell Shiny where to ask for user inputs, and where to put any outputs we create.\nReactive values, which are values that change according to user inputs. These are values that affect the outputs we create in the Shiny app, such as tables or plots.\nThe server, where we use reactive values to generate some outputs.\nIDs\nThe user interface and server communicate through IDs that we assign to inputs from the user and outputs from the server.\n\nWe use an ID (in orange) to link the user input in the UI to the reactive values used in the server:\n\nWe use another ID (in blue) to link the output created in the server to the output shown in the user interface:\n\nOrganisation\nThese elements can all be placed in one script named app.R or separately in scripts named ui.R and server.R. The choice is up to you, although it becomes easier to work in separate ui.R and server.R scripts when the Shiny app becomes more complex.\nExample 1: Everything in app.R\nExample 2: Split things into ui.R and server.R\n\nCustomising the theme\nIf you’d like to go one step further, you can also customize the appearance of your Shiny app using built-in themes, or creating your own themes.\nUsing built-in themes\nThere are several built-in themes in Shiny, which allow you to quickly change the appearance of your app. You can browse a gallery of available themes here here, or test themes out interactively here.\nLet’s try the darkly theme on our Shiny app. To do this, we will need the shinythemes package.\n\n\nlibrary(shinythemes)\n\n\n\nWe can change the theme of our previous app with one line of code:\n\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  ),\n  \n  # Customize the theme ----------------------\n  \n  # Use the darkly theme\n  theme = shinythemes::shinytheme(\"darkly\")\n)\n\n\n\nNow, if we run the app, it looks a little different:\n\nUsing a custom theme\nYou can also go beyond the built-in themes, and create your own custom theme with the fonts and colours of your choice. You can also apply this theme to the outputs rendered in the app, to bring all the visuals together for a more cohesive look.\nCustomizing a theme\nTo create a custom theme, we will be using the bs_theme() function from the bslib package.\n\n\nlibrary(bslib)\n\n\n\n\n\n# Create a custom theme \ncute_theme <- bslib::bs_theme(\n  \n  bg = \"#36393B\", # background colour\n  fg = \"#FFD166\", # most of the text on your app\n  primary = \"#F26430\", # buttons, ...\n  \n  # you can also choose fonts\n  base_font = font_google(\"Open Sans\"),\n  heading_font = font_google(\"Open Sans\")\n)\n\n\n\nTo apply this theme to our Shiny app (and the outputs), we will be using the thematic package.\n\n\nlibrary(thematic)\n\n\n\nThere are two essential steps to apply a custom theme to a Shiny app:\nActivating thematic.\nSetting the user interface’s theme to the custom theme (cute_theme).\n\n\n# Activate thematic\n# so your R outputs will be changed to match up with your chosen styling\nthematic::thematic_shiny()\n\n# Define UI for application that makes a table andplots the Volcano Explosivity \n# Index for the most eruptive volcanoes within a selected range of years\n\nui <- fluidPage(\n  \n  # Application title ----\n  \n  titlePanel(\"Exploring volcano explosivity\"),\n  \n  # Input interface ----\n  \n  sidebarLayout(\n    sidebarPanel(\n      \n      # Sidebar with a slider range input\n      sliderInput(\"years\", # the id your server needs to use the selected value\n                  label = h3(\"Years\"),\n                  min = 1900, max = 2020, # maximum range that can be selected\n                  value = c(2010, 2020) # this is the default slider position\n      )\n    )\n  ),\n  \n  # Show the outputs from the server ---------------\n  mainPanel(\n    \n    # Show a ridgeplot of explosivity index for selected volcanoes\n    plotOutput(\"ridgePlot\"),\n    \n    # then, show the table we made in the previous step\n    tableOutput(\"erupt_table\")\n    \n  ),\n  \n  # Customize the theme ----------------------\n  \n  # Use our custom theme\n  theme = cute_theme\n)\n\n\n\nNow, if we run the app, the user interface and plot theme is set to the colours and fonts we set in cute_theme:\n\nHere, thematic is not changing the colours used to represent a variable in our plot, because this is an informative colour scale (unlike the colour of axis labels, lines, and the plot background). However, if we remove this colour variable in our ridgeplot in the server, thematic will change the plot colours as well. Here is a simplified example of our server to see what these changes would look like:\n\n\n# Define server logic required to make your output(s)\nserver <- function(input, output) {\n  \n  #... (all the good stuff we wrote above)\n  \n  # render the plot output\n  output$ridgePlot <- renderPlot({\n    \n    # create the plot\n    ggplot(data = eruptions_filtered(),\n           aes(x = vei,\n               y = volcano_name)) + # we are no longer setting \n             # the fill argument to a variable\n             \n             # we are using a ridgeplot geom here, from the ggridges package\n             geom_density_ridges(size = .5) + \n             \n             # label the axes\n             labs(x = \"Volcano Explosivity Index\", y = \"\") +\n             \n             # remove the \"classic\" ggplot2 so it doesn't override thematic's changes\n             # theme_classic() + \n             theme(legend.position = \"none\",\n                   axis.text = element_text(size = 12, face = \"bold\"),\n                   axis.title = element_text(size = 14, face = \"bold\"))\n           })\n    }\n\n\n\nNow, our plot’s theme follows the app’s custom theme as well:\n\n\nConstructing an App\nTaking advantage of good defaults\nHere, we will use shiny extension shinyDashboards and leaflet to construct a custom Shiny App to map volcanoes of the world. First, we need a few additional packages.\nNote: All Source code for this app can be found here on the BIOS2 Github.\n\n\n# load packages\nlibrary(shiny)\nlibrary(shinydashboard)  # dashboard layout package\nlibrary(shinyWidgets)  # fancy widgets package\nlibrary(leaflet)  # interactive maps package\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nUsing ShinyDashboard\nWe will create our app using defaults from the ShinyDashboard package, which always includes three main components: a header, using dashboardHeader(), a sidebar, using dashboardSidebar(), and a body, using dashboardBody(). These are then added together using the dashboardPage() function.\nBuilding these elements is less like usual R coding, and more like web design, since we are, in fact, designing a unser interface for a web app. Here, we’ll make a basic layout before populating it.\n\n\n# create the header of our app\nheader <- dashboardHeader(\n    title = \"Exploring Volcanoes of the World\",\n    titleWidth = 350 # since we have a long title, we need to extend width element in pixels\n)\n\n\n# create dashboard body - this is the major UI element\nbody <- dashboardBody(\n\n    # make first row of elements (actually, this will be the only row)\n    fluidRow(\n        \n        # make first column, 25% of page - width = 3 of 12 columns\n        column(width = 3,\n               \n               \n               # Box 1: text explaining what this app is\n               #-----------------------------------------------\n               box( width = NULL,\n                    status=\"primary\", # this line can change the automatic color of the box.\n                    title = NULL,\n                    p(\"here, we'll include some info about this app\")\n\n                 \n               ), # end box 1\n               \n               \n               # box 2 : input for selecting volcano type\n               #-----------------------------------------------\n               box(width = NULL, status = \"primary\",\n                   title  = \"Selection Criteria\", solidHeader = T, \n                   \n                   p(\"here, we'll add a UI element for selecting volcano types\"),\n\n               ), # end box 2\n               \n               \n               \n               # box 3: ggplot of selected volcanoes by continent\n               #------------------------------------------------\n               box(width = NULL, status = \"primary\",\n                   solidHeader = TRUE, collapsible = T,\n                   title = \"Volcanoes by Continent\",\n                   p(\"here, we'll add a bar plot of volcanoes in each continent\")\n               ) # end box 3\n               \n        ), # end column 1\n         \n        # second column - 75% of page (9 of 12 columns)\n        #--------------------------------------------------\n        column(width = 9,\n               # Box 4: leaflet map\n               box(width = NULL, background = \"light-blue\", height = 850,\n                   p(\"here, we'll show volcanoes on a map\"),\n               ) # end box with map\n        ) # end second column\n        \n    ) # end fluidrow\n) # end body\n\n\n# add elements together\ndashboardPage(\n    skin = \"blue\",\n    header = header,\n    sidebar = dashboardSidebar(disable = TRUE), # here, we only have one tab of our app, so we don't need a sidebar\n    body = body\n)\n\n\n\n\nPopulating the Layout\nNow, we are going to fill out app with elements. In this app, we will only have one user input: a selection of the volcano type to show. We will use this input (input$volcano_type), which will be used to filter data in the server (i.e. make a smaller dataset using only volcanoes of the selected types), then use this filtered dataset to create output elements (plots and maps).\nBelow, we show the necessary code to include in both the UI and the Server to create each plot element. Notice that after the reactive value selected_volcanoes is created in the selection box, this is the only object that is used to create the other elements in the app.\nLocation\nElement\nUI\nServer\nBox 1\nIntro Textbox\nMarkdown/HTML text code\n\nBox 2\nSelection Wigets\ncheckboxGroupButtons( inputID = \"volcano_type\")\nselected_volcanoes <- reactive({ volcano_df %>% filter(type %in% input$volcano_type)}) to create a filtered dataset that will react to user input\nBox 3\nBar Graph\nplotOutput(\"bargraph\")\noutput$continentplot <- renderPlot(...)) which will plot from the selectied_volcanoes reactive object\nBox 4\nLeaflet Map\nleafletOutput(\"volcanomap\")\noutput$volcanomap <- renderLeaflet(...) to map points from the selectied_volcanoes reactive object\n\nChallenge!\nUse the code provided to add your own additional user input to the Shiny App. The code (which you can access here leaves a space for an additional UI input inside box 2). Then, you’ll need to use your new input element to the reactive value in the Server, as noted in the server code.\nUse the Default Shiny Widgets or shinyWidgets extended package galleries to explore the types of elements you can add.\n\nSee our completed app HERE\n\nWhy do you want to use Shiny?\nThere are many reasons to consider using Shiny for a project:\nsharing results from a paper with your readers\nhelping you explore a model, mathematics, simulations\nletting non R users use R.\nHello Shiny\n\n\n# Define UI for app that draws a histogram ----\nui <- fluidPage(\n\n  # App title ----\n  titlePanel(\"Hello Shiny!\"),\n\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n\n      # Input: Slider for the number of bins ----\n      sliderInput(inputId = \"bins\",\n                  label = \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n\n    ),\n\n    # Main panel for displaying outputs ----\n    mainPanel(\n\n      # Output: Histogram ----\n      plotOutput(outputId = \"distPlot\")\n\n    )\n  )\n)\n\n\n\nGolem\nFirst step is creating your own golem project:\n\ngolem provides you with some very helpful “workflow” scripts:\nEdit the Description by filling in this function in 01_dev.R\nThen, add all the dependencies\nTogether, this edits the DESCRIPTION of your R package to look something like this:\nNow we can start building the app.\n\n\napp_ui <- function(request) {\n  tagList(\n    # Leave this function for adding external resources\n    golem_add_external_resources(),\n    # Your application UI logic \n    shinydashboard::dashboardPage(\n      header = shinydashboard::dashboardHeader(\n        title = \"Exploring Volcanoes of the World\",\n        titleWidth = 350 # since we have a long title, we need to extend width element in pixels\n      ),\n      sidebar = shinydashboard::dashboardSidebar(disable = TRUE), # here, we only have one tab, so we don't need a sidebar\n      body = shinydashboard::dashboardBody(\n        # make first row of elements (actually, this will be the only row)\n        fluidRow(\n          # make first column, 25% of page - width = 3 of 12 colums\n          column(width = 3,\n                 # box 1 : input for selecting volcano type\n                 #-----------------------------------------------\n                 shinydashboard::box(width = NULL, status = \"primary\",\n                     title  = \"Selection Criteria\", solidHeader = T\n                     \n                     ## CHECKBOX HERE\n                     \n                 ), # end box 1\n                 # box 2: ggplot of selected volcanoes by continent\n                 #------------------------------------------------\n                 shinydashboard::box(width = NULL, status = \"primary\",\n                     solidHeader = TRUE, collapsible = T,\n                     title = \"Volcanoes by Continent\"\n                     \n                     ## PLOT HERE\n\n                 ) # end box 2\n          ), # end column 1\n          \n          # second column - 75% of page (9 of 12 columns)\n          column(width = 9,\n                 \n                 # Box 3: leaflet map\n                 shinydashboard::box(width = NULL, background = \"light-blue\"\n                                     \n                                     ## MAP HERE \n                                     \n                 ) # end box with map\n          ) # end second column\n        ) # end fluidrow\n      ) # end body\n    )\n  )\n}\n\n\n\nThis indicates where each of the three components of the app would go.\nAt this point we can run the app to get a very empty-looking UI:\n\nGolem modules\nWe could split this app into three different sections, corresponding to each of the three boxes:\nFilter the type of volcano we see\ntake the filtered volcanoes and plot a stacked bar chart\ntake the filtered volcanoes and plot a map\nSelecting the volcanoes\n\n\n#' volcano_select UI Function\n#'\n#' @description A shiny Module.\n#'\n#' @param id,input,output,session Internal parameters for {shiny}.\n#'\n#' @noRd \n#'\n#' @importFrom shiny NS tagList \nmod_volcano_select_ui <- function(id){\n  ns <- NS(id)\n  tagList(\n    # Widget specifying the species to be included on the plot\n    shinyWidgets::checkboxGroupButtons(\n      inputId = ns(\"volcano_type\"),\n      label = \"Volcano Type\",\n      choices = c(\"Stratovolcano\" , \"Shield\" ,\"Cone\" ,   \"Caldera\" ,    \"Volcanic Field\",\n                  \"Complex\" , \"Other\",   \"Lava Dome\"  , \"Submarine\"    ),\n      checkIcon = list(\n        yes = tags$i(class = \"fa fa-check-square\", \n                     style = \"color: steelblue\"),\n        no = tags$i(class = \"fa fa-square-o\", \n                    style = \"color: steelblue\"))\n    ) # end checkboxGroupButtons\n  )\n}\n    \n#' volcano_select Server Functions\n#'\n#' @noRd \nmod_volcano_select_server <- function(id, volcano){\n  moduleServer( id, function(input, output, session){\n    ns <- session$ns\n    \n    # make reactive dataset\n    # ------------------------------------------------\n    # Make a subset of the data as a reactive value\n    # this subset pulls volcano rows only in the selected types of volcano\n    selected_volcanoes <- reactive({\n      \n      req(input$volcano_type)\n      \n      volcano %>%\n        \n        # select only volcanoes in the selected volcano type (by checkboxes in the UI)\n        dplyr::filter(volcano_type_consolidated %in% input$volcano_type) %>%\n        # Space to add your suggested filter here!! \n        # --- --- --- --- --- --- --- --- --- --- --- --- ---\n        # filter() %>%\n        # --- --- --- --- --- --- --- --- --- --- --- --- ---\n        # change volcano type into factor (this makes plotting it more consistent)\n        dplyr::mutate(volcano_type_consolidated = factor(volcano_type_consolidated,\n                                                  levels = c(\"Stratovolcano\" , \"Shield\",  \"Cone\",   \"Caldera\", \"Volcanic Field\",\n                                                             \"Complex\" ,  \"Other\" ,  \"Lava Dome\" , \"Submarine\" ) ) )\n    })\n    \n  })\n}\n    \n## To be copied in the UI\n# mod_volcano_select_ui(\"volcano_select_1\")\n    \n## To be copied in the server\n# mod_volcano_select_server(\"volcano_select_1\")\n\n\n\nI also like to test my modules by using them to create a toy Shiny app. The best place to do this is by using a testthat directory. This is another great advantage of using a package workflow. You can set this up easily with usethis::use_test(): just run usethis::use_test from the R console when you have the module open.\nThen write a simple test like this\n\n\ntest_that(\"volcano selection module works\", {\n  \n  testthat::skip_if_not(interactive())\n  \n  \n  ui <- fluidPage(\n  ## To be copied in the UI\n  mod_volcano_select_ui(\"volcano_select_1\"),\n  tableOutput(\"table\")\n  )\n  \n  server <- function(input, output) {\n    ## To be copied in the server\n    volcano_data <- readRDS(\"data/volcanoes.rds\")\n    selected_data <- mod_volcano_select_server(\"volcano_select_1\",\n                                               volcano = volcano_data)\n    \n    output$table <- renderTable(selected_data())\n  }\n  \n  shinyApp(ui = ui, server = server)\n  \n})\n\n\n\nWhich generates the following simple app:\n\nBarplot of continents\n\n\n#' continentplot UI Function\n#'\n#' @description A shiny Module.\n#'\n#' @param id,input,output,session Internal parameters for {shiny}.\n#'\n#' @noRd \n#'\n#' @importFrom shiny NS tagList \nmod_continentplot_ui <- function(id){\n  ns <- NS(id)\n  tagList(\n    plotOutput(ns(\"barplot\"), # this calls to object continentplot that is made in the server page\n               height = 350)\n  )\n}\n    \n#' continentplot Server Functions\n#'\n#' @noRd \nmod_continentplot_server <- function(id, volcano, selected_volcanoes){\n  \n  # kind of helpful\n  stopifnot(is.reactive(selected_volcanoes))\n  \n  moduleServer( id, function(input, output, session){\n    ns <- session$ns\n \n    output$barplot <- renderPlot({\n      \n      # create basic barplot\n      barplot <- ggplot2::ggplot(data = volcano,\n                                 ggplot2::aes(x=continent,\n                            fill = volcano_type_consolidated))+\n        # update theme and axis labels:\n        ggplot2::theme_bw()+\n        ggplot2::theme(plot.background  = ggplot2::element_rect(color=\"transparent\",fill = \"transparent\"),\n                       panel.background = ggplot2::element_rect(color=\"transparent\",fill=\"transparent\"),\n                       panel.border     = ggplot2::element_rect(color=\"transparent\",fill=\"transparent\"))+\n        ggplot2::labs(x=NULL, y=NULL, title = NULL) +\n        ggplot2::theme(axis.text.x = ggplot2::element_text(angle=45,hjust=1))\n      \n      \n      # IF a selected_volcanoes() object exists, update the blank ggplot. \n      # basically this makes it not mess up when nothing is selected\n      \n      barplot <- barplot +\n        ggplot2::geom_bar(data = selected_volcanoes(), show.legend = F) +\n        ggplot2::scale_fill_manual(values = RColorBrewer::brewer.pal(9,\"Set1\"), drop=F) +\n        ggplot2::scale_x_discrete(drop=F)\n      \n      \n      # print the plot\n      barplot\n      \n    }) # end renderplot command\n    \n    \n  })\n}\n    \n## To be copied in the UI\n# mod_continentplot_ui(\"continentplot_1\")\n    \n## To be copied in the server\n# mod_continentplot_server(\"continentplot_1\")\n\n\n\nthe test\n\n\ntest_that(\"volano barplot works\", {\n  \n  testthat::skip_if_not(interactive())\n  \n  \n  ui <- fluidPage(\n    ## To be copied in the UI\n    mod_continentplot_ui(\"continentplot_1\"),\n    tableOutput(\"table\")\n  )\n  \n  server <- function(input, output) {\n    ## To be copied in the server\n    volcano_data <- readRDS(\"data/volcanoes.rds\")\n    volcano_recent <- subset(volcano_data, last_eruption_year > 2000)\n    mod_continentplot_server(\"continentplot_1\",\n                                              volcano = volcano_data,\n                                              selected_volcanoes = reactive(volcano_recent))\n  }\n  \n  shinyApp(ui = ui, server = server)\n})\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-18T09:35:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-04-building-r-packages/",
    "title": "Building R packages",
    "description": "This practical training will cover the basics of modern package development in R with a focus on the following three aspects: (1) how to turn your code into functions, (2) how to write tests and documentation, and (3) how to share your R package on GitHub..",
    "author": [
      {
        "name": "Andrew MacDonald",
        "url": {}
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\n\n\n\nvia GIPHY\n\nR packages! they are kind of like cookies:\nAlmost everyone enjoys them\ndelicious when homemade\nLots of storebought options available\nGreat skill to have\nnot necessary to sell them!\n\nhere “selling” cookies is a metaphor for taking your package public, i.e. by submitting to CRAN or even simply by releasing it on a public-facing GitHub page.\n\nBut most of all: cookies are delicious for what they contain: chocolate chunks, candy, oats, cocoa. However, all cookies share some fundamental ingredients and nearly identical structure. Flour, saturated with fat and sugar hydrated only with an egg, flavoured with vanilla and salt. The basic formula is invariant and admits only slight deviation – otherwise, it becomes something other than a cookie.\nThis workshop is devoted to the study of cookie dough.\nMise en place : development environment\nWe’ll explore a few useful packages in this workshop. The first two in particular are very popular tools for modern-day R package development:\ninstall.packages(\"devtools\")\ninstall.packages(\"usethis\")\ninstall.packages(\"testthat\")\ninstall.packages(\"assertthat\")\nBuilding an R package also requires specific tools for compiling the finished package. Run the following line to make sure you have the development environment:\ndevtools::has_devel()\nIf you do not have the software to build R packages, you should see a message which will help you find the correct links to download what you need!\nWindows will need RTools. First do the check above to see if you are already set up. If not then download the software here.\nand Install. After that, open R and run the following:\nwriteLines('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', con = \"~/.Renviron\")\nand restart R. Then run the check above once more to confirm\nDuring the training I will be calling on learners with different Operating Systems and/or Text Editors to share their experience!\nThe structure: flour and sugar\n\nNo cookies without carbs\n\nAn R package is essentially a folder on your computer with specific structure. We will begin by creating an empty R package and taking a tour!\nOpen your R code editor, and find out where you are:\ngetwd()\nThis is to prepare for the next step, where we will choose a location for our R package folder. Please be intentional about where you place your R package! Do not place it in the same space as another package, Rstudio project or other project. Create a new and isolated location for it.\nI am working from an existing R project in my typical R Projects folder, so I go up one level:\nusethis::create_package(\"../netwerk\")\n\nwe are sticking with usethis because we want to keep this general. All of these steps can be manual, and indeed for many years they were!\nLet’s run R CMD CHECK right away. We will do this MANY TIMES.\ndevtools::check()\nWe should see some warnings! let’s keep these in mind as we continue our tour.\nThe DESCRIPTION\nThe most important file to notice is the DESCRIPTION. This gives the general page for the entire package.\nAdd your name here.\nAdd a license\nusethis::use_mit_license(copyright_holder = \"\")\nnote about the different roles taht R package authors can have. Funny ones. but creator and maintainer are the key ones.\nNote the R folder. We’ll get much more into that later\nRbuildignore\nkeeping notes\ncreate an R file\nusethis::use_build_ignore(\"dev.R\")\nthe docs folder\nhere we have a very minimal version of an R packages we’re going to be adding to it as the course progresses.\nOne thing we can do right away is build and check the R package\nWhat exactly is happining here? slide from R package tutorial.\nLots of checkpoints and progress confrimations along the way.\nOK so what is that all about? we have compiled the R package and it has gone to where the R packages on our computer go.\nThere is a natural cycle to how the different steps in an R package workflow proceed – see the documentation for this lesson – we will be following this process (TK another pictures?\nOk so now that we ahve the basic structure, let’s talk about some content for the R package. I received the donation of a little R function already that we can use to create this workflow in a nice way\nThis R function (explain what the function does)\nOK so let’s focus on just one part of this function.\nload all – shortcut\n\nhow do we do this in VScode?\n\n\nhow to add something to the .Rbuildignore? it would be nice to have a little .dev script as a space to create all the ohter dependencies that are involved in making an R package.\n\n\n\n\nUseful links\nThis workshop borrows heavily from some excellent sources:\nthe R packages book especially the “Whole Game” chapter!\nrOpenSci Packages: Development, Maintenance, and Peer Review\nhttps://builder.r-hub.io/about.html\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-21T13:17:50-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-25-point-count-data-analysis/",
    "title": "Point-count Data Analysis",
    "description": "Analysis of point-count data in the presence of variable survey methodologies and detection error offered by Peter Solymos to BIOS2 Fellows in March 2021.",
    "author": [
      {
        "name": "Peter Solymos",
        "url": {}
      }
    ],
    "date": "2021-03-25",
    "categories": [
      "Technical",
      "EN"
    ],
    "contents": "\n\nContents\nInstructor\nOutline\nGet course materials\nInstall required software\nGet the notes\n\nUseful resources\nReferences\nLicense\n\n\nThis course is aimed towards researchers analyzing field observations, who are often faced by data heterogeneities due to field sampling protocols changing from one project to another, or through time over the lifespan of projects, or trying to combine ‘legacy’ data sets with new data collected by recording units.\nSuch heterogeneities can bias analyses when data sets are integrated inadequately, or can lead to information loss when filtered and standardized to common standards. Accounting for these issues is important for better inference regarding status and trend of species and communities.\nAnalysts of such ‘messy’ data sets need to feel comfortable with manipulating the data, need a full understanding the mechanics of the models being used (i.e. critically interpreting the results and acknowledging assumptions and limitations), and should be able to make informed choices when faced with methodological challenges.\nThe course emphasizes critical thinking and active learning through hands on programming exercises. We will use publicly available data sets to demonstrate the data manipulation and analysis. We will use freely available and open-source R packages.\nThe expected outcome of the course is a solid foundation for further professional development via increased confidence in applying these methods for field observations.\nInstructor\nDr. Peter SolymosBoreal Avian Modelling Project and the Alberta Biodiversity Monitoring InstituteDepartment of Biological Sciences, University of Alberta\nOutline\nEach day will consist of 3 sessions, roughly one hour each, with short breaks in between.\n\nThe video recordings from the workshop can be found on YouTube.\n\nSession\nTopic\nFiles\nVideos\nDay 1\nNaive techniques\n\n\n\n1. Introductions\nSlides\nVideo\n\n2. Organizing point count data\nNotes\nPart 1, Part 2\n\n3. Regression techniques\nNotes\nPart 1, Part 2\nDay 2\nBehavioral complexities\n\n\n\n1. Statistical assumptions and nuisance variables\nSlides\nVideo\n\n2. Behavioral complexities\nNotes\nbSims, Video\n\n3. Removal modeling techniques\nNotes\nVideo\n\n4. Finite mixture models and testing assumptions\nNotes\nMixtures, Testing\nDay 3\nThe detection process\n\n\n\n1. The detection process\nSlides\nVideo\n\n2. Distance sampling and density\nNotes\nVideo\n\n3. Estimating population density\nNotes\nVideo\n\n4. Assumptions\nNotes\nVideo\nDay 4\nComing full circle\n\n\n\n1. QPAD overview\nSlides\nVideo\n\n2. Models with detectability offsets\nNotes\nOffsets, Models\n\n3. Model validation and error propagation\nNotes\nValidation, Error\n\n4. Recordings, roadsides, closing remarks\nNotes\nVideo\nGet course materials\nInstall required software\nFollow the instructions at the R website to download and install the most up-to-date base R version suitable for your operating system (the latest R version at the time of writing these instructions is 4.0.4).\nThen run the following script in R:\nsource(\"https://raw.githubusercontent.com/psolymos/qpad-workshop/main/src/install.R\")\nHaving RStudio is not absolutely necessary, but it will make life easier. RStudio is also available for different operating systems. Pick the open source desktop edition from here (the latest RStudio Desktop version at the time of writing these instructions is 1.4.1106).\nPrior exposure to R programming is not necessary, but knowledge of basic R object types and their manipulation (arrays, data frames, indexing) is useful for following hands-on exercises. Software Carpentry’s Data types and structures in R is a good resource to brush up your R skills.\nGet the notes\nIf you don’t want to use git:\nDownload the workshop archive release into a folder\nExtract the zip archive\nOpen the workshop.Rproj file in RStudio (or open any other R GUI/console and setwd() to the directory where you downloaded the file)\n(You can delete the archive)\nIf you want to use git: fork or clone the repository\ncd into/your/dir\ngit clone https://github.com/psolymos/qpad-workshop.git\nUseful resources\nUsing the QPAD package to get offsets based on estimates from the Boreal Avian Modelling Project’s database\nNA-POPS: Point count Offsets for Population Sizes of North America landbirds\nReferences\nSólymos, P., Toms, J. D., Matsuoka, S. M., Cumming, S. G., Barker, N. K. S., Thogmartin, W. E., Stralberg, D., Crosby, A. D., Dénes, F. V., Haché, S., Mahon, C. L., Schmiegelow, F. K. A., and Bayne, E. M., 2020. Lessons learned from comparing spatially explicit models and the Partners in Flight approach to estimate population sizes of boreal birds in Alberta, Canada. Condor, 122: 1-22. PDF\nSólymos, P., Matsuoka, S. M., Cumming, S. G., Stralberg, D., Fontaine, P., Schmiegelow, F. K. A., Song, S. J., and Bayne, E. M., 2018. Evaluating time-removal models for estimating availability of boreal birds during point-count surveys: sample size requirements and model complexity. Condor, 120: 765-786. PDF\nSólymos, P., Matsuoka, S. M., Stralberg, D., Barker, N. K. S., and Bayne, E. M., 2018. Phylogeny and species traits predict bird detectability. Ecography, 41: 1595-1603. PDF\nVan Wilgenburg, S. L., Sólymos, P., Kardynal, K. J. and Frey, M. D., 2017. Paired sampling standardizes point count data from humans and acoustic recorders. Avian Conservation and Ecology, 12(1):13. PDF\nYip, D. A., Leston, L., Bayne, E. M., Sólymos, P. and Grover, A., 2017. Experimentally derived detection distances from audio recordings and human observers enable integrated analysis of point count data. Avian Conservation and Ecology, 12(1):11. PDF\nSólymos, P., and Lele, S. R., 2016. Revisiting resource selection probability functions and single-visit methods: clarification and extensions. Methods in Ecology and Evolution, 7:196-205. PDF\nMatsuoka, S. M., Mahon, C. L., Handel, C. M., Sólymos, P., Bayne, E. M., Fontaine, P. C., and Ralph, C. J., 2014. Reviving common standards in point-count surveys for broad inference across studies. Condor 116:599-608. PDF\nSólymos, P., Matsuoka, S. M., Bayne, E. M., Lele, S. R., Fontaine, P., Cumming, S. G., Stralberg, D., Schmiegelow, F. K. A. & Song, S. J., 2013. Calibrating indices of avian density from non-standardized survey data: making the most of a messy situation. Methods in Ecology and Evolution 4:1047-1058. PDF\nMatsuoka, S. M., Bayne, E. M., Sólymos, P., Fontaine, P., Cumming, S. G., Schmiegelow, F. K. A., & Song, S. A., 2012. Using binomial distance-sampling models to estimate the effective detection radius of point-counts surveys across boreal Canada. Auk 129:268-282. PDF\nLicense\nThe course material is licensed under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Source code is under MIT license.\n\n\n\n",
    "preview": "posts/2021-03-25-point-count-data-analysis/thumb.jpg",
    "last_modified": "2021-06-21T13:15:07-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-introduction-aux-concepts-edi-en-contexte-scientifique/",
    "title": "Introduction to EDI concepts in a scientific context",
    "description": "A short introduction to EDI concepts in a scientific context.",
    "author": [
      {
        "name": "Agathe Riallan",
        "url": {}
      },
      {
        "name": "Marie-José Naud",
        "url": {}
      }
    ],
    "date": "2021-01-22",
    "categories": [
      "Transversal competencies",
      "FR",
      "EN"
    ],
    "contents": "\n\nContents\nIntroduction to EDI concepts in a scientific context\nIntroduction aux concepts EDI en contexte scientifique\n\nTexte en français à la suite.\nIntroduction to EDI concepts in a scientific context\nIn 2021, the BIOS2 training program will be holding a series of training and reflection activities on equity, diversity and inclusion issues. The goal is to develop an EDI action plan for the program in order to consolidate a more inclusive, respectful and open environment.\nThe objectives of this workshop are:\nDefine the concepts of equity, diversity and inclusion\nIdentify the benefits and challenges of EDI in the university context\nRecongnize how to become an EDI bearer during one’s university career\nRaise awareness of intercultural communication (professional competence of tomorrow)\nThe workshop is developed by Agathe Riallan, Faculty Coordinator for Equity, Diversity and Inclusion (EDI) at the Faculty of Science, Université de Sherbrooke, in collaboration with Marie-José Naud, Equity, Diversity and Inclusion Advisor and Coordinator at the Centre d’études nordiques (CEN).\n\n\n\nIntroduction aux concepts EDI en contexte scientifique\nEn 2021, nous aurons une série de formations et d’activités de réflexion sur les questions d’équité, diversité et d’inclusion. Notre objectif est de mettre en place un plan d’action EDI pour le programme afin de consolider un environnement plus inclusif, respectueux et ouvert.\nLes objectifs de cet ateliers sont:\nDéfinir les concepts d’équité, de diversité et d’inclusion\nIdentifier les avantages et les défis de l’ÉDI en contexte universitaire\nIdentifier comment être porteuse ou porteur de l’ÉDI lors de son parcours universitaire\nSe sensibiliser à la communication interculturelle (compétence professionnelle de demain)\nL’atelier est développé par Agathe Riallan, Coordinatrice facultaire de l’Équité, de la Diversité et de l’Inclusion (ÉDI) de la Faculté des Sciences à Université de Sherbrooke, en collaboration avec Marie-José Naud, Conseillère en équité, diversité et inclusion et coordonnatrice au Centre d’études nordiques (CEN).\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-21T13:15:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-12-4-day-training-in-spatial-statistics-with-philippe-marchand/",
    "title": "4-Day Training in Spatial Statistics with Philippe Marchand",
    "description": "Training session about statistical analysis of spatial data in ecology, hosted by Philippe Marchand (UQAT). | \nSession de formation sur l’analyse statistique des données spatiales en écologie, animée par Pr. Philippe Marchand (UQAT).",
    "author": [
      {
        "name": "Philippe Marchand",
        "url": {}
      }
    ],
    "date": "2021-01-12",
    "categories": [
      "FR",
      "EN"
    ],
    "contents": "\n\nContents\nSpatial statistics in ecology\nStatistiques spatiales en écologie\n\n\n\nTexte en français à la suite.\nSpatial statistics in ecology\nBIOS² hosted an online training session about statistical analysis of spatial data in ecology, led by Pr. Philippe Marchand (UQAT). This 12-hour training was conducted in 4 sessions: January 12, 14, 19 & 21 (2021) from 1:00 to 4:00 pm EST.\nThe content included three types of spatial statistical analyses and their applications to ecology: (1) point pattern analysis to study the distribution of individuals or events in space; (2) geostatistical models to represent the spatial correlation of variables sampled at geolocated points; and (3) areal data models, which apply to measurements taken on areas in space and model spatial relationships as networks of neighbouring regions. The training also included practical exercises using the R statistical programming environment.\nPhilippe Marchand is a professor in ecology and biostatistics at Institut de recherche sur les forêts, Université du Québec en Abitibi-Témiscamingue (UQAT) and BIOS² academic member. His research focuses on modeling processes that influence the spatial distribution of populations, including: seed dispersal and seedling establishment, animal movement, and the spread of forest diseases.\nIf you wish to consult the lesson materials and follow the exercises at your own pace, you can access them through this link. Basic knowledge of linear regression models and experience fitting them in R is recommended. Original repository can be found here.\nCourse outline\nDay\nTopics\nLinks\n1\n• Introduction to spatial statistics  • Point pattern analysis\nSpatial Statistics in Ecology, Part 1\n2\n• Spatial correlation  • Geoestatistical models\nSpatial Statistics in Ecology, Part 2\n3\n• Areal data  • Moran’s I  • Spatial autoregression models  • Analysis of areal data in R\nSpatial Statistics in Ecology, Part 3\n4\n• GLMM with spatial Gaussian process  • GLMM with spatial autoregression\nSpatial Statistics in Ecology, Part 4\nStatistiques spatiales en écologie\nBIOS² a organisé une session de formation en ligne sur l’analyse statistique des données spatiales en écologie, animée par le Pr. Philippe Marchand (UQAT). Cette formation de 12 heures s’est déroulée en 4 sessions : 12, 14, 19 & 21 janvier (2021) de 13h00 à 16h00 HNE.\nLe contenu comprenait trois types d’analyses statistiques spatiales et leurs applications en écologie : (1) l’analyse des patrons de points qui permet d’étudier la distribution d’individus ou d’événements dans l’espace; (2) les modèles géostatistiques qui représentent la corrélation spatiale de variables échantillonnées à des points géoréférencés; et (3) les modèles de données aréales, qui s’appliquent aux mesures prises sur des régions de l’espace et qui représentent les liens spatiaux par le biais de réseaux de voisinage. La formation comprenait également des exercices pratiques utilisant l’environnement de programmation statistique R.\nPhilippe Marchand est professeur d’écologie et de biostatistique à l’Institut de recherche sur les forêts, Université du Québec en Abitibi-Témiscamingue (UQAT) et membre académique de BIOS². Ses travaux de recherche portent sur la modélisation de processus qui influencent la distribution spatiale des populations, incluant: la dispersion des graines et l’établissement des semis, le mouvement des animaux, et la propagation des épidémies forestières.\nSi vous souhaitez consulter le matériel pédagogique et suivre les exercices à votre propre rythme, vous pouvez y accéder par ce lien. Une connaissance de base des modèles de régression linéaire et une expérience de l’ajustement de ces modèles dans R sont recommandées. Le repositoire original se trouve ici.\nPlan du cours\nJour\nSujets\nLiens\n1\n• Introduction aux statistiques spatiales  • Analyse des patrons de points\nStatistiques spatiales en écologie, Partie 1\n2\n• Corrélation spatiale d’une variable  • Modèles géostatistiques\nSpatial Statistics in Ecology, Part 2\n3\n• Données aréales  • Indice de Moran  • Modèles d’autorégression spatiale  • Analyse des données aréales dans R\nStatistiques spatiales en écologie, Partie 3\n4\n• GLMM avec processus spatial gaussien  • GLMM avec autorégression spatiale\nStatistiques spatiales en écologie, Partie 4\n\n\n\n",
    "preview": "https://bios2.usherbrooke.ca/wp-content/uploads/2020/12/bernard-hermant-XyzPOIMqWfc-unsplash-2-1-768x512.jpg",
    "last_modified": "2021-06-21T13:15:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-07-making-websites-with-hugo/",
    "title": "Making websites with HUGO",
    "description": "This workshop provides a general introduction to HUGO, a popular open source framework for building websites without requiring a knowledge of HTML/CSS or web programming.",
    "author": [
      {
        "name": "Dominique Gravel",
        "url": {}
      },
      {
        "name": "Guillaume Larocque",
        "url": {}
      }
    ],
    "date": "2020-12-07",
    "categories": [
      "Technical",
      "Transversal competencies",
      "EN"
    ],
    "contents": "\n\nContents\nWhy this training workshop ?\nMake sure Hugo is installed and check version\nBe Timothée Poisot for fun\nExercise : Edit the toml file to include your own information.\n\nBuild the static html files\nBuild for local development\nBuild for publishing your website\n\nEdit content\nExercise : take 15 minutes to remove Tim’s material and replace it by the three chapters of your thesis.\n\nHosting the website on a server\nGitHub User or Organization Pages\nStep-by-step Instructions\nPut it Into a Script\n\n\nPush source and build repos.\nUsing a theme\nExercise 1\n\nCustomizing a theme\nBasics of HTML\nA divider, used to organize content into blocks\nA span, used to organize content or text into sections with different styles. Usually on the same line.\nA paragraph\nHeadings at different levels\nAn image\nA link\n\nLink between HTML and CSS\nIn html\nIn CSS\n\nBasics of CSS\nExercise 2\n\nPartials\nExercise 3\n\nNow a bit of GO lang to make the featured species different.\nExercise 4\n\niFrames\nExercise 5\n\n\n\nWhy this training workshop ?\nI am only 10 hours of a crash course in web development ahead of you. As part of a major research project on setting a biodiversity observation network, I had to develop a prototype of a portal for the project, for biodiversity information and bunch of dashboards on biodiversity trends. Never made a website before. I know how to code in a few langages, and I know that I hate playing with boxes, menus, importing images manually, and most of all, dealing with a crash of the system and having to redo the whole thing because I made a mistake somewhere. Not that a bug when I try to compile is better, but at least it is more tractable.\nHugo made it very easily because of its fundamental feature (which is the same reason I edit papers with LaTeX): the distinction between the view and the content. Once you have set up the rules defining the visual aspects of the pages, then you can focus on the content and let the software automatically constructing the html code for you. It’s fast, accessible, scriptable and could be version-controlled. All qualities for an open and reproducible science.\nTook me a few hours to learn the basics (much harder to get the higher level skills, especially to write your own Go scripts), I took some tricks here and there in different templates and at looking what others do, and that was it I had my website. Realized that it could be a good entry level course to BIOS2 fellows and decided to turn that experience into a training workshop.\nYou will find below basic instructions to install and run a template. The following is not a full tutorial, for that I recommend simply to take time looking at the documentation provided on the Hugo page (https://gohugo.io/). I also consulted the online book Hugo in action (https://www.manning.com/books/hugo-in-action). There are many other references, all of them with goods and bads. But it’s nice to have multiple ones because sometimes the description of a concept may be obscure in one reference but better in the other and it’s by comparing and switching between them that you can make progress.\nMake sure Hugo is installed and check version\nFirst step, you have to make sure that it is properly installed on you computer. Type the following command in terminal to make sure :\nhugo version\nYou can access to the help menu with the simple command :\nhugo help\nBe Timothée Poisot for fun\nWe will use Tim’s website, which is a simple but efficient example of what we could achieve with Hugo. The strenght of the website is that it automatically updates with the addition of new content, such as publications, lab members and projects. The only thing you have to do, once the template is properly set up, is to update the content. That way, yo can focus on the material you want to put on, without struggling on how to place the boxes, format the police and all of the complicate stuff that comes with html and css. The content, written in markdown, is human readable and therefore could be easily edited by lab members. Further, since it’s all scripted, it’s easy to maintain and control versions.\nTake few minutes to look at the final webpage at https://poisotlab.io/\nNow you will clone the repository on your own computer so that you could start playing with the content, edit the files, modify list of papers and so on.\nYou can either use the clone button on the top of the page or the following command :\ngit clone https://github.com/bios2/Hugo-training-workshop.git\nWe will take a few minutes to look at the content of the different folders. This structure is common to most of the Hugo templates. You will find multiple folders, it’s useful to understand what’s located where because the compiler expects this structure when it looks for specific information.\narchetypes (not in here, but usually in most templates). These are basic instructions to generate new content with the hugo new command. We won’t use this feature today, but information about this feature is easy to find.\nassets contains the css files where the controls for visual aspect of the pages are specified. That’s where you’ll search for the different items and how to specify things such as box sizes, font colors and dimensions etc…. Note: assets directory is not created by default.\ncontent holds all of the .md files where the main content of the pages is provided. It’s divided in several subfolders, corresponding to the different pages from the menu. Each top-level folder in Hugo is considered a content section (which is described usually in the config file). For instance, you have one folder called Research where the projects are described. You’ll find one .md file per projec tin this folder. Note also that the folders contain systematically a _index.md file where the metadata and the top level information of the page are specified. We’ll come back to that later.\ndata stores specific information that will be consulted by the parser during compilation (configurationfiles). There are also data templates, and at the moment, there is one json file where the papers are listed and two toml files with a list of the students, past and present. json files could be edited with a text editor (not so fun), but there are some tools to do it efficiently.\nlayouts contains the core files to compile the website. You will find in them instructions, in a strange blend of html and Go langages. No so easy and pleasant to play with, but looking at them tells you a bit about what the compiler does (a good example is for people). list.html for instance contains a loop that goes through the toml files in order to create the icons, the text and the link to the full markdown page where you have description for each student. You will find layouts for the main pages, as well as for partials (like the header menu).\nresources also contains css instructions for the template. We won’t work with this one.\nstatic contains bunch of little things that are called during compilation. You’ll find the logo for the lab, the pictures for students, pdf files for applications, images for each research project …\nThere is also one very important file in the main folder the config.toml file. Inside, you will find a lot of the metadata that will control the structure of the main page. This find can be very simple for some templates, much more complicated for other ones. Note that for some templates, the config file may be in a distinct folder. Not all templates have exactly the same folder structure.\ntoml is a file format for configuration files, it contains key parameters for the webpage. It consists of key = “value” pairs, [section names], and # comments. Let’s open this one to have a closer look.\nExercise : Edit the toml file to include your own information.\nYou may want to change the section People to Collaborators and also provide a proper reference to your on github page. You can also add or remove sections, this will affect the menu at the top of the page. For instance, you can add a blog section.\nBuild the static html files\nBuild for local development\nHugo will use all of the material to generate static html files that will be displayed on your browser. The command is really easy to use to run it on your own computer, you simply have to type the following in the main folder :\nhugo server\nAnd that’s it, it compiles and you can simply open it in your browser by clicking on the adress indicated in the terminal. Congratulations for your first Hugo webste !\nThere are useful information in the terminal about the building process.\nBuild for publishing your website\nThe command hugo server is very fast and useful to test your website while you develop it. But once you’ll be ready to distribute it, you’ll need all of the html files and related material to distribute the website. This is easily done with the even simpler command\nhugo\nYou will find in the directory that a new folder named public appeared, with all of the material needed to deploy the website. If you click on the index.html file, you’ll get to the home page of the website. It is interesting to open this file in your text editor, you’ll get a sense of the html code that hugo generated automatically for you. You can also take a look at other files.\nEdit content\nEditing content is the easier thing to do. First thing to do, is to modify the content of the introduction paragraph on the main page. You’ll find it in the *_index.md* file in the content folder. Open it and modify the text. You can after build the main page again to see the update.\nYou can also add material, with new md files. We will do so with a new research project (note the following could be done manually):\nhugo new research/chapter1.md\nThis will generate a new markdown file, in which you can start adding material. But those files do have a particular structure, so before editing it, we’ll take a quick look at another one, datascience.md.\nThe header section is typical of a markdown file with metadata (in toml or yaml format). You have to specify information to the parser about the title, the image and associated papers. Note that it will work if some of these (e.g. papers) are missing. You can modify the image as well.\nThe file here also a particular structure, with the  marker between two paragraphs. This command indicates that only the first paragraph is displayed on the main page of the Research tab, and the full content follows if you click to know more about the project.\nNote that here you can use the basic features of markdown, with headers, bold, italics and so on. You can also include html code directly into the markdown and it should work. That said, it may conflict with higher level instructions in the layout or in the theme and may cause difficulties at building. While it is feasible to add such command, it is not recommended to do so. People rather suggest to use shortcodes (Tomorrow) or to modify the layout of the website.\nExercise : take 15 minutes to remove Tim’s material and replace it by the three chapters of your thesis.\nHosting the website on a server\nThere are many options to host your new website on a server. An easy one, free, and that could be coupled with version control is to run it on github. Full instructions are available here :\nhttps://gohugo.io/hosting-and-deployment/hosting-on-github/\nWe will simply follow the instructions copied here for hosting a personal page. Note that you can also develop a page for a project.\nGitHub User or Organization Pages\nStep-by-step Instructions\nCreate a  (e.g. blog) repository on GitHub. This repository will contain Hugo’s content and other source files.\nCreate a .github.io GitHub repository. This is the repository that will contain the fully rendered version of your Hugo website.\ngit clone  && cd \nPaste your existing Hugo project into the new local  repository. Make sure your website works locally (hugo server or hugo server -t ) and open your browser to http://localhost:1313.\nOnce you are happy with the results: Press Ctrl+C to kill the server Before proceeding run rm -rf public to completely remove the public directory\ngit submodule add -b main https://github.com//.github.io.git public. This creates a git submodule. Now when you run the hugo command to build your site to public, the created public directory will have a different remote origin (i.e. hosted GitHub repository).\nMake sure the baseURL in your config file is updated with: .github.io\nPut it Into a Script\nYou’re almost done. In order to automate next steps create a deploy.sh script. You can also make it executable with chmod +x deploy.sh.\nThe following are the contents of the deploy.sh script:\n    #!/bin/sh\n\n    # If a command fails then the deploy stops\n    set -e\n\n    printf \"\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\"\n\n    # Build the project.\n    hugo # if using a theme, replace with `hugo -t <YOURTHEME>`\n\n    # Go To Public folder\n    cd public\n\n    # Add changes to git.\n    git add .\n\n    # Commit changes.\n    msg=\"rebuilding site $(date)\"\n    if [ -n \"$*\" ]; then\n        msg=\"$*\"\n    fi\n    git commit -m \"$msg\"\nPush source and build repos.\ngit push origin main\nYou can then run ./deploy.sh \"Your optional commit message\" to send changes to .github.io. Note that you likely will want to commit changes to your  repository as well.\nThat’s it! Your personal page should be up and running at https://.github.io within a couple minutes.\nUsing a theme\nIt is usually a good idea to not modify a template directly, but to have the template and the site in a separate folder. The basic concept when doing this is that the config.toml file of the site has to link to the proper folder of the theme.\nFor example\ntheme = \"template-site\"\nthemesDir = \"../..\"\nThis means that the template site is in a folder named template-site which is a parent folder of the site folder. Other options are possible.\nUsually, all the content should go in the site folder, not in the theme folder.\nExercise 1\nStart modifying the theme to make it look like a website for a Zoo. Choose your preferred color scheme by changing the style= parameter in the config.toml file.\nFeel free to download some images from unsplash and save them in the static/img folder. You can then use these images in the carrousel, as “testimonial” photos or as background images for some of the sections. You can add or remove sections from the home page by editing the config.toml file and changing the enable= parameter in the params. segment at the bottom.\nYou can also try to create a new blog entry by adding a new file in the content/blog folder. This file will have a .md extension and will be written in markdown format.\nCustomizing a theme\nBasics of HTML\nCore structure of an HTML page\n<!DOCTYPE html>\n<html>\n<head>\n<title>This is my great website<\/title>\n<style>\n.css_goes_here{\n\n}\n<\/style>\n<\/head>\n<body>\n<h1>Main title<\/h1>\n<div>Main content goes here<\/div>\n<\/body>\n<\/html>\nA divider, used to organize content into blocks\n<div><\/div>\nA span, used to organize content or text into sections with different styles. Usually on the same line.\n<span><\/span>\nA paragraph\n<p><\/p>\nHeadings at different levels\n<h1>Main title<\/h1>\n<h2>Second level<\/h2>\n<h3>Third level<\/h3>\nAn image\n<img src='img/image_name.jpg'>\nA link\n<a href=\"https://bios2.github.io\">Great website here!<\/a>\nLink between HTML and CSS\nIn html\nid is always unique. Class is not.\n<div id=\"this-div-only\" class=\"this-type-of-div\">\nOne great div!\n<\/div>\nIn CSS\n“#” is applied to id and “.” is applied to class. When nothing is specified, applies to tag.\n#this-div-only{\n    font-size:24px;\n}\n\n.this-type-of-div{\n    color: #bb0000;\n}\n\ndiv{\n    display:block;\n}\nBasics of CSS\nW3 Schools CSS reference\nProperty\nDescription\nExample\nwidth, height\nwidth of item\n200px, 200pt, 100%, 100vw/vh\nmin-width, min-height\nminimum size of item\n200px, 200pt, 100%, 100vw\ncolor\nfont color\n#aa0000, red or rgb(255,0,0)\nbackground-color\ncolor of background\n#aa0000, red or rgb(255,0,0)\nborder-color\ncolor of border\n#aa0000, red or rgb(255,0,0)\nborder\nsize, type and color of border\n1px solid black\nmargin\nmargin around item (top right bottom left)\n1px, or 1px 2px 2px 1px\npadding\npadding within item, inside div for example\n10px\nfont-family\nname of font\nVerdana, Arial\nfont-size\nsize of text\n14px, 2em\ndisplay\nshould item be on the same line, or in a separate block?\ninline, block, inline-block, flex, …\nExercise 2\nCreate a file named custom.css under template-site/my-site/static/css/.\nRight-click on elements on the web page that you want to modify, then click on Inspect element and try to find CSS properties that you could modify to improve the look of the page. Then, choosing the proper class, add entries in the custom.css file that start with a dot (.) followed by the proper class names.\n.this-class {\n    font-size:28px;\n}\nPartials\nPartials are snippets of HTML code that could be reused on different places on the website. For example, you will see that the layouts/index.html file in the template-site folder lists all the partials that create the home page.\nAn important point to remember is that Hugo will look for files first in the site’s folders, and if it doesn’t find the files there, it will look for them in the theme’s folder. So site folder layouts and CSS take priority over the theme folder.\nExercise 3\nCreate a new folder template-site/my-site/layouts. In this folder, create a new file named index.html and copy the content of the template-site/layouts/index.html file into it. Remove the testimonials section from the newly created file.\nCreate a new folder template-site/my-site/layouts/partials. In this folder, create a new file named featured-species.html put the following content into it, replacing the information with the species you selected.\n<div class=\"featured-species\">\n<img src=\"img/species/frog.jpg\" class=\"species-image\" alt=\"\" >\n<div class=\"species-description\">\n<h3>Red-Eyed Tree Frog<\/h3>\n<p>This frog can be found in the tropical rain forests of Costa Rica.<\/p>\n<\/div>\n<\/div>\nThen, add this section to the index.html file created above.\n{{ partial \"featured_species.html\" . }}\nYou will probably need to restart the Hugo server to see the changes appear on the site.\nNow, you need to edit the CSS! In your custom.css file, add the following lines.\n\n.featured-species{\n    height:300px;\n    background-color: #1d1f20;\n    color:white;\n}\n\n.species-image{\n    height:300px;\n    float:left;\n}\n\n.featured-species h3{\n    color:white;\n    font-size:1.5em;\n}\n\n.species-description{\n    float:left;\n    padding:20px;\n    font-size:2em;\n}\nModify this as you see fit!\nNow a bit of GO lang to make the featured species different.\nIntroduction to Hugo templating\nExercise 4\nReplace your partial featured-species.html content with this one\n{{ range .Site.Data.species }}\n    {{ if eq (.enable) true }}\n            <div class=\"featured-species\">\n            <img src=\"img/species/{{ .image }}\" class=\"species-image\" alt=\"\" >\n            <div class=\"species-description\">\n            <h3>{{ .name }}<\/h3>\n            <p> {{ .description }}<\/p>\n            <\/div>\n            <\/div>\n    {{end}}\n{{end}}\nNow, create a new folder /template-site/my-site/data/species.\nIn this folder, create new file named frog.yaml with the following content.\nenable: true\nname: \"Red-eyed tree frog\"\ndescription: \"This frog can be found in the forests of Costa Rica\"\nimage: \"frog.jpg\"\nFind other species photos and add them to the img folder. Then you can add new .yaml files in the data/species folder for each species.\niFrames\nAn iFrame is a HTML tag that essentially allows you to embed another web page inside of your site.\nExercise 5\nFind a Youtube video and click on the share option below the video. Find the Embed option and copy the code that starts with <iframe> to a new partial that will be shown on a new page. Surround the iframe with a div tag with class=\"video\". For example:\n<div class=\"video\">\n<iframe \nwidth=\"560\" \nheight=\"315\" \nsrc=\"https://www.youtube.com/embed/42GAn4v5MgE\" \nframeborder=\"0\" \nallow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \nallowfullscreen>\n<\/iframe>\n<\/div>\nEdit the custom.css file and add this section\n.video{\n    width:100%;\n    background-color:black;\n    text-align:center;\n}\n\n\n\n",
    "preview": "posts/2020-12-07-making-websites-with-hugo/thumb.png",
    "last_modified": "2021-06-21T13:15:06-04:00",
    "input_file": {},
    "preview_width": 3887,
    "preview_height": 1018
  },
  {
    "path": "posts/2020-09-21-data-visualization/",
    "title": "Data Visualization",
    "description": "General principles of visualization and graphic design, and techniques of tailored visualization. This training was developed and delivered by Alex Arkilanian and Katherine Hébert on September 21st and 22nd, 2020.",
    "author": [
      {
        "name": "Alex Arkilanian",
        "url": {}
      },
      {
        "name": "Katherine Hébert",
        "url": {}
      }
    ],
    "date": "2020-09-21",
    "categories": [
      "Technical",
      "Fellow contributed",
      "EN"
    ],
    "contents": "\n\nContents\nTraining material\nInteractive examples\nExample figures\nAnnotated resource library\n\n\nWelcome!\nThis training covers the general principles of visualization and graphic design, and techniques of tailored visualization. More specifically, the objectives of the training are:\nMake an overview of basic data visualization principles, including shapes, sizes, colours, and fonts.\nDiscuss how to choose the right visualization for your data, what you want to communicate, and who you want to communicate to.\nTools and principles to tailor visualizations, particularly in making interpretable, interactive, and honest visualizations.\nTraining material\nClick on “Show code” to learn how to do each plot!\nInteractive examples\nStreamgraph\n\n\nShow code\n\n# Script to make a streamgraph of the top 10 most popular dog breeds in \n# New York City from 1999 to 2015\n\n# load libraries\nlibrary(magrittr) # piping\nlibrary(lubridate) # dealing with dates\nlibrary(dplyr) # data manipulation\nlibrary(streamgraph) #devtools::install_github(\"hrbrmstr/streamgraph\")\nlibrary(htmlwidgets) # to save the widget!\n\n# load the dataset\n# more information about this dataset can be found here:\n# https://www.kaggle.com/smithaachar/nyc-dog-licensing-clean\nnyc_dogs <- read.csv(\"data/nyc_dogs.csv\")\n\n# convert birth year to date format (and keep only the year)\nnyc_dogs$AnimalBirthYear <- mdy_hms(nyc_dogs$AnimalBirthMonth) %>% year()\n\n# identify 10 most common dogs\ntopdogs <- nyc_dogs %>% count(BreedName) \ntopdogs <- topdogs[order(topdogs$n, decreasing = TRUE),]\n# keep 10 most common breeds (and remove last year of data which is incomplete)\ndf <- filter(nyc_dogs, BreedName %in% topdogs$BreedName[2:11] & AnimalBirthYear < 2016) %>% \n  group_by(AnimalBirthYear) %>% \n  count(BreedName) %>% ungroup()\n\n# get some nice colours from viridis (magma)\ncols <- viridis::viridis_pal(option = \"magma\")(length(unique(df$BreedName)))\n\n# make streamgraph!\npp <- streamgraph(df, \n                  key = BreedName, value = n, date = AnimalBirthYear, \n                  height=\"600px\", width=\"1000px\") %>%\n  sg_legend(show=TRUE, label=\"names: \") %>%\n  sg_fill_manual(values = cols) \n# saveWidget(pp, file=paste0(getwd(), \"/figures/dogs_streamgraph.html\"))\n\n# plot\npp\n\n\n\n\n\nInteractive plot\n\n\nShow code\n\n# Script to generate plots to demonstrate how combinations of information dimensions\n# can become overwhelming and difficult to interpret.\n\n# set-up & data manipulation ---------------------------------------------------\n\n# load packages\nlibrary(ggplot2) # for plots, built layer by layer\nlibrary(dplyr) # for data manipulation\nlibrary(magrittr) # for piping\nlibrary(plotly) # interactive plots\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# import data\n# more info on this dataset: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-28/readme.md\npenguins <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\n# get some nice colours from viridis (magma)\nsp_cols <- viridis::viridis_pal(option = \"magma\")(5)[2:4]\n\n\n#### Day 1 ####\n\n# 1. Similarity\n\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, col = species), size = 2.5) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Species\") + # labels\n  scale_color_manual(values = sp_cols) # sets the colour scale we created above \n\n\n\nShow code\n\nggsave(\"figures/penguins_similarity.png\", width = 6, height = 3, units = \"in\")\n\n# 2. Proximity\n\ndf <- penguins %>% group_by(sex, species) %>% \n  summarise(mean_mass = mean(body_mass_g, na.rm = TRUE)) %>% na.omit() \nggplot(df) +\n  geom_bar(aes(y = mean_mass, x = species, fill = sex), \n           position = \"dodge\", stat = \"identity\") +\n  labs(x = \"Species\", y = \"Mean body mass (g)\", col = \"Sex\") + # labels\n  scale_fill_manual(values = sp_cols) # sets the colour scale we created above\n\n\n\nShow code\n\nggsave(\"figures/penguins_proximity.png\", width = 6, height = 3, units = \"in\")\n\n# 3. Enclosure (Ellipses over a fake PCA)\nggplot(data = penguins, \n       aes(y = bill_length_mm, x = bill_depth_mm)) +\n  geom_point(size = 2.1, col = \"grey30\") +\n  stat_ellipse(aes(col = species), lwd = .7) +\n  labs(x = \"PCA1\", y = \"PCA2\", col = \"Species\") + # labels\n  scale_color_manual(values = sp_cols) + # sets the colour scale we created above\n  theme(axis.text = element_blank(), axis.ticks = element_blank())\n\n\n\nShow code\n\nggsave(\"figures/penguins_enclosure.png\", width = 6, height = 3, units = \"in\")\n\n# 4. Mismatched combination of principles\ntemp_palette <- rev(c(sp_cols, \"#1f78b4\", \"#33a02c\"))\nggplot(data = penguins, \n       aes(y = bill_length_mm, x = bill_depth_mm)) +\n  geom_point(aes(col = sex), size = 2.1) +\n  stat_ellipse(aes(col = species), lwd = .7) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"?\") + # labels\n  scale_color_manual(values = temp_palette) # sets the colour scale we created above\n\n\n\nShow code\n\nggsave(\"figures/penguins_mismatchedgestalt.png\", width = 6, height = 3, units = \"in\")\n\n\n\n#### Day 2 ####\n\n# 1. Ineffective combinations: Luminance & shading -----------------------------\n\n# create the plot\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, \n                 col = species, # hue\n                 alpha = log(body_mass_g)), # luminance\n             size = 2.5) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       col = \"Species\", alpha = \"Body mass (g)\") +\n  scale_color_manual(values = sp_cols)\n\n\n\nShow code\n\nggsave(\"figures/penguins_incompatible1.png\", width = 6, height = 3, units = \"in\")\n\n# 2. Ineffective combinations: Sizes and shapes --------------------------------\n\nggplot(penguins) +\n  geom_point(aes(y = bill_length_mm, x = bill_depth_mm, \n                 shape = species, # shape\n                 size = log(body_mass_g)), alpha = .7) + # size\n  scale_size(range = c(.1, 5)) + # make sure the sizes are scaled by area and not by radius\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       shape = \"Species\", size = \"Body mass (g)\") \n\n\n\nShow code\n\nggsave(\"figures/penguins_incompatible2.png\", width = 6, height = 3, units = \"in\")\n\n# 3. Cognitive overload --------------------------------------------------------\n\n# get some nice colours from viridis (magma)\nsex_cols <- viridis::viridis_pal(option = \"magma\")(8)[c(3,6)]\n\nggplot(na.omit(penguins)) +\n  geom_point(aes(y = bill_length_mm, # dimension 1: position along y scale\n                 x = bill_depth_mm, # dimension 2: position along x scale\n                 shape = species, # dimension 3: shape\n                 size = log(body_mass_g), # dimension 4: size\n                 col = sex), # dimension 5: hue\n             alpha = .7) + # size\n  scale_size(range = c(.1, 5)) + # make sure the sizes are scaled by area and not by radius\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", \n       shape = \"Species\", size = \"Body mass (g)\", col = \"Sex\") +\n  scale_color_manual(values = sex_cols)\n\n\n\nShow code\n\nggsave(\"figures/penguins_5dimensions.png\", width = 7, height = 4, units = \"in\")\n\n\n# 4. Panels -------------------------------------------------------------------\n\nggplot(na.omit(penguins)) +\n  geom_point(aes(y = bill_length_mm, # dimension 1: position along y scale\n                 x = bill_depth_mm, # dimension 2: position along x scale\n                 col = log(body_mass_g)), # dimension 3: hue\n             alpha = .7, size = 2) + \n  facet_wrap(~ species) + # dimension 4: species!\n  # this will create a separate panel for each species\n  # note: this also automatically uses the same axes for all panels! If you want \n  # axes to vary between panels, use the argument scales = \"free\"\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"magma\", end = .9, direction = -1) +\n  theme_linedraw() + theme(panel.grid = element_blank()) # making the panels prettier\n\n\n\nShow code\n\nggsave(\"figures/penguins_dimensions_facets.png\", width = 7, height = 4, units = \"in\")\n\n\n# 5. Interactive ---------------------------------------------------------------\n\np <- na.omit(penguins) %>%\n  ggplot(aes(y = bill_length_mm, \n             x = bill_depth_mm, \n             col = log(body_mass_g))) +\n  geom_point(size = 2, alpha = .7) + \n  facet_wrap(~ species) +\n  labs(x = \"Bill depth (mm)\", y = \"Bill length (mm)\", col = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"magma\", end = .9, direction = -1) +\n  theme_linedraw() + theme(panel.grid = element_blank()) # making the panels prettier\np <- ggplotly(p)\n#setwd(\"figures\")\nhtmlwidgets::saveWidget(as_widget(p), \"figures/penguins_interactive.html\")\np\n\n\n\n\nExample figures\n\n\nShow code\n\n# Script to make animated plot of volcano eruptions over time\n\n# Load libraries:\nlibrary(dplyr) # data manipulation\nlibrary(ggplot2) # plotting\nlibrary(gganimate) # animation\nlibrary(gifski) # creating gifs\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# function to floor a year to the decade\nfloor_decade = function(value){return(value - value %% 10)}\n\n# read data \neruptions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/eruptions.csv')\n\n# select top 5 most frequently exploding volcanoes\ntemp <- group_by(eruptions, volcano_name) %>% tally() \ntemp <- temp[order(temp$n, decreasing = TRUE),]\n\n# make a time series dataset (number of eruptions per year)\neruptions$start_decade = floor_decade(eruptions$start_year)\n\n# filter dataset to subset we want to visualize\ndf <- eruptions %>% \n  filter(between(start_decade, 1900, 2019)) %>%\n  filter(volcano_name %in% temp$volcano_name[1:5]) %>%\n  group_by(start_decade) %>%\n  count(volcano_name) %>% ungroup()\n\n# plot!\np <- ggplot(df, aes(x = start_decade, y = n, fill = volcano_name)) +\n  geom_area() +\n  geom_vline(aes(xintercept = start_decade)) + # line that follows the current decade\n  scale_fill_viridis_d(option = \"magma\", end = .8) +\n  labs(x = \"\", y = \"Number of eruptions\", fill = \"Volcano\",\n       title = 'Eruptions of the top 5 most frequently erupting volcanos worldwide') +\n  # gganimate part: reveals each decade\n  transition_reveal(start_decade) \nanimate(p, duration = 5, fps = 20, width = 800, height = 300, renderer = gifski_renderer())\n\n\n\nShow code\n\n#anim_save(\"figures/volcano_eruptions.gif\")\n\n\n\n\n\nShow code\n\n# Script to generate plots with various ways of representing uncertainty, based \n# Coffee & Code dataset from https://www.kaggle.com/devready/coffee-and-code/data\n\n# set-up & data manipulation ---------------------------------------------------\n\n# load packages\nlibrary(ggplot2) # for plots, built layer by layer\nlibrary(dplyr) # for data manipulation\nlibrary(magrittr) # for piping\nlibrary(ggridges) # for density ridge plots\nlibrary(patchwork) # great package for \"patching\" plots together!\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# import data\ndf <- read.csv(\"data/coffee_code.csv\")\n\n# set labels to be used in all plots\ncoffee_labels <- labs(title = \"Does coffee help programmers code?\",\n                      x = \"Coffee while coding\", \n                      y = \"Time spent coding \\n(hours/day)\") \n\n# the variable CodingWithoutCoffee is negative, which is harder to understand\n# (i.e. \"No\" means they drink coffee...). So, let's transform it into a more \n# intuitive variable!\ndf$CodingWithCoffee <- gsub(\"No\", \"Usually\", df$CodingWithoutCoffee)\ndf$CodingWithCoffee <- gsub(\"Yes\", \"Rarely\\n or never\", df$CodingWithCoffee)\n# convert to factor and set levels so they show up in a logical order\ndf$CodingWithCoffee <- factor(df$CodingWithCoffee,\n                              levels = c(\"Rarely\\n or never\", \n                                         \"Sometimes\", \n                                         \"Usually\"))\n\n# calculate summary statistics for the variable of choice\ndf_summary <- group_by(df, CodingWithCoffee) %>%\n  summarise(\n    # mean\n    mean_codinghours = mean(CodingHours), \n    # standard deviation\n    sd_codinghours = sd(CodingHours), \n    # standard error\n    se_codinghours = sd(CodingHours)/sqrt(length(CodingHours)))\n\n\n# 1. Error bars (standard error) -----------------------------------------------\n\nggplot(df_summary) +\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - se_codinghours,\n                    ymax = mean_codinghours + se_codinghours), \n                width = .2) +\n  geom_point(aes(x = CodingWithCoffee, y = mean_codinghours), \n             size = 3) +\n  coffee_labels + ylim(0,10)\n\n\n\nShow code\n\nggsave(\"figures/coffee_errorbars.png\", width = 5, height = 3, units = \"in\")\n\n# 2. Boxplot -------------------------------------------------------------------\n\nggplot(df) +\n  geom_boxplot(aes(x = CodingWithCoffee, y = CodingHours)) +\n  coffee_labels\n\n\n\nShow code\n\nggsave(\"figures/coffee_boxplot.png\", width = 5, height = 3, units = \"in\")\n\n\n# 3. Error bar demonstration ---------------------------------------------------\n\n# get some nice colours from viridis (magma)\nerror_cols <- viridis::viridis_pal(option = \"magma\")(5)[2:4]\n# set labels to be used in the palette\nerror_labels = c(\"standard deviation\",\"95% confidence interval\",\"standard error\")\n\nggplot(df_summary) +\n  # show the raw data\n  geom_jitter(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours),\n              alpha = .5, width = .05, col = \"grey\") +\n  # standard deviation\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - sd_codinghours,\n                    ymax = mean_codinghours + sd_codinghours,\n                    col = \"SD\"), width = .2, lwd = 1) +\n  # 95% confidence interval\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - 1.96*se_codinghours,\n                    ymax = mean_codinghours + 1.96*se_codinghours, \n                    col = \"CI\"), width = .2, lwd = 1) +\n  # standard error\n  geom_errorbar(aes(x = CodingWithCoffee, \n                    ymin = mean_codinghours - se_codinghours,\n                    ymax = mean_codinghours + se_codinghours, \n                    col = \"SE\"), width = .2, lwd = 1) +\n  geom_point(aes(x = CodingWithCoffee, y = mean_codinghours), \n             size = 3) +\n  coffee_labels + ylim(c(0,11)) +\n  # manual palette/legend set-up!\n  scale_colour_manual(name = \"Uncertainty metric\", \n                      values = c(SD = error_cols[1], \n                                 CI = error_cols[2], \n                                 SE = error_cols[3]),\n                      labels = error_labels) +\n  theme(legend.position = \"top\")\n\n\n\nShow code\n\nggsave(\"figures/coffee_bars_demo.png\", width = 7, height = 5, units = \"in\")\n\n\n# 4. Jitter plot with violin ---------------------------------------------------\n\nggplot() +\n  geom_jitter(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours),\n              alpha = .5, width = .05, col = \"grey\") +\n  geom_violin(data = df, aes(x = CodingWithCoffee, \n                             y = CodingHours), alpha = 0) +\n  geom_linerange(data = df_summary,\n                 aes(x = CodingWithCoffee, \n                     ymin = mean_codinghours - se_codinghours,\n                     ymax = mean_codinghours + se_codinghours)) +\n  geom_point(data = df_summary, \n             aes(x = CodingWithCoffee, \n                 y = mean_codinghours), size = 3) +\n  coffee_labels\n\n\n\nShow code\n\nggsave(\"figures/coffee_violin_jitter.png\", width = 5, height = 3, units = \"in\")\n\n\n# 5. Density ridge plot --------------------------------------------------------\n\nggplot(df) + \n  aes(y = CodingWithCoffee, x = CodingHours, fill = stat(x)) +\n  geom_density_ridges_gradient(scale = 1.9, size = .2, rel_min_height = 0.005) +\n  # colour palette (gradient according to CodingHours)\n  scale_fill_viridis_c(option = \"magma\", direction = -1) +\n  # remove legend - it's not necessary here!\n  theme(legend.position = \"none\") +\n  labs(title = coffee_labels$title, \n       x = coffee_labels$y, \n       y = \"Coffee \\nwhile coding\") + \n  theme(axis.title.y = element_text(angle=0, hjust = 1, vjust = .9, \n                                    margin = margin(t = 0, r = -50, b = 0, l = 0)))\n\n\n\nShow code\n\nggsave(\"figures/coffee_density_ridges.png\", width = 5, height = 3, units = \"in\")\n\n# 6. Jitter vs. Rug plot ------------------------------------------------------------------\n\njitterplot <- ggplot(df, aes(x = CoffeeCupsPerDay, y = CodingHours)) +\n  geom_jitter(alpha = .2) +\n  geom_smooth(fill = error_cols[1], col = \"black\", method = lm, lwd = .7) +\n  coffee_labels + ylim(c(0,11)) + labs(x = \"Cups of coffee (per day)\")\n\nrugplot <- ggplot(df, aes(x = CoffeeCupsPerDay, y = CodingHours)) +\n  geom_smooth(fill = error_cols[1], col = \"black\", method = lm, lwd = .7) +\n  geom_rug(position=\"jitter\", alpha = .7) + ylim(c(0,11)) +\n  coffee_labels + labs(x = \"Cups of coffee (per day)\")\n\n# patch the two plots together\njitterplot + rugplot\n\n\n\nShow code\n\n#ggsave(\"figures/coffee_jitter_vs_rugplot.png\", width = 10, height = 4, units = \"in\")\n\n\n\n\n\nShow code\n\n# Script to generate 95% confidence intervals of a generated random normal distribution\n# as an example in Day 2: Visualizing uncertainty.\n\n# load library\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(dplyr)\n\n# set ggplot theme\ntheme_set(theme_classic() +\n            theme(axis.title = element_text(size = 11, face = \"bold\"),\n                  axis.text = element_text(size = 11),\n                  plot.title = element_text(size = 13, face = \"bold\"),\n                  legend.title = element_text(size = 11, face = \"bold\"),\n                  legend.text = element_text(size = 10)))\n\n# set random seed\nset.seed(22)\n\n# generate population (random normal distribution)\ndf <- data.frame(\"value\" = rnorm(50, mean = 0, sd = 1))\n\n# descriptive stats for each distribution\ndesc_stats = df %>% \n  summarise(mean_val = mean(value, na.rm = TRUE),\n            se_val = sqrt(var(value)/length(value)))\n\n# build density plot!\np <- ggplot(data = df, aes(x = value, y = ..count..)) +\n  geom_density(alpha = .2, lwd = .3) +\n  xlim(c(min(df$value-1), max(df$value+1))) \n# extract plotted values\nbase_p <- ggplot_build(p)$data[[1]]\n# shade the 95% confidence interval\np + \n  geom_area(data = subset(base_p, \n                          between(x, \n                                  left = (desc_stats$mean_val - 1.96*desc_stats$se_val),\n                                  right = (desc_stats$mean_val + 1.96*desc_stats$se_val))),\n            aes(x = x, y = y), fill = \"cadetblue3\", alpha = .6) +\n  # add vertical line to show population mean\n  geom_vline(aes(xintercept = 0), lty = 2) +\n  annotate(\"text\", x = 0.9, y = 19, label = \"True mean\", fontface = \"italic\") +\n  # label axis!\n  labs(x = \"Variable of interest\", y = \"\") \n\n\n\nShow code\n\n#ggsave(\"figures/confidenceinterval_example.png\", width = 5, height = 3.5, units = \"in\")\n\n\n\nAnnotated resource library\nThis is an annotated library of data visualization resources we used to build the BIOS² Data Visualization Training, as well as some bonus resources we didn’t have the time to include. Feel free to save this page as a reference for your data visualization adventures!\nBooks & articles\nFundamentals of Data Visualization A primer on making informative and compelling figures. This is the website for the book “Fundamentals of Data Visualization” by Claus O. Wilke, published by O’Reilly Media, Inc.\nData Visualization: A practical introduction An accessible primer on how to create effective graphics from data using R (mainly ggplot). This book provides a hands-on introduction to the principles and practice of data visualization, explaining what makes some graphs succeed while others fail, how to make high-quality figures from data using powerful and reproducible methods, and how to think about data visualization in an honest and effective way.\nData Science Design (Chapter 6: Visualizing Data) Covers the principles that make standard plot designs work, show how they can be misleading if not properly used, and develop a sense of when graphs might be lying, and how to construct better ones.\nGraphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods Cleveland, William S., and Robert McGill. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association, vol. 79, no. 387, 1984, pp. 531–554. JSTOR, www.jstor.org/stable/2288400. Accessed 9 Oct. 2020.\nGraphical Perception and Graphical Methods for Analyzing Scientific Data Cleveland, William S., and Robert McGill. “Graphical perception and graphical methods for analyzing scientific data.” Science 229.4716 (1985): 828-833.\nFrom Static to Interactive: Transforming Data Visualization to Improve Transparency Weissgerber TL, Garovic VD, Savic M, Winham SJ, Milic NM (2016) designed an interactive line graph that demonstrates how dynamic alternatives to static graphics for small sample size studies allow for additional exploration of empirical datasets. This simple, free, web-based tool demonstrates the overall concept and may promote widespread use of interactive graphics.\nData visualization: ambiguity as a fellow traveler Research that is being done about how to visualize uncertainty in data visualizations. Marx, V. Nat Methods 10, 613–615 (2013). https://doi.org/10.1038/nmeth.2530\nData visualization standards Collection of guidance and resources to help create better data visualizations with less effort.\nDesign principles\nGestalt Principles for Data Visualization: Similarity, Proximity & Enclosure Short visual guide to the Gestalt Principles.\nWhy scientists need to be better at data visualization A great overview of principles that could improve how we visualize scientific data and results.\nA collection of graphic pitfalls A collection of short articles about common issues with data visualizations that can mislead or obscure your message.\nChoosing a visualization\nData Viz Project This is a great place to get inspiration and guidance about how to choose an appropriate visualization. There are many visualizations we are not used to seeing in ecology!\nFrom data to Viz | Find the graphic you need Interactive tool to choose an appropriate visualization type for your data.\nColour\nWhat to consider when choosing colors for data visualization A short, visual guide on things to keep in mind when using colour, such as when and how to use colour gradients, the colour grey, etc.\nColorBrewer: Color Advice for Maps Tool to generate colour palettes for visualizations with colorblind-friendly options. You can also use these palettes in R using the RColorBrewer package, and the scale_*_brewer() (for discrete palettes) or scale_*_distiller() (for continuous palettes) functions in ggplot2.\nColor.review Tool to pick or verify colour palettes with high relative contrast between colours, to ensure your information is readable for everyone.\nCoblis — Color Blindness Simulator Tool to upload an image and view it as they would appear to a colorblind person, with the option to simulate several color-vision deficiencies.\n500+ Named Colours with rgb and hex values List of named colours along with their hex values.\nCartoDB/CartoColor CARTOColors are a set of custom color palettes built on top of well-known standards for color use on maps, with next generation enhancements for the web and CARTO basemaps. Choose from a selection of sequential, diverging, or qualitative schemes for your next CARTO powered visualization using their online module.\nTools\nR\nThe R Graph Gallery A collection of charts made with the R programming language. Hundreds of charts are displayed in several sections, always with their reproducible code available. The gallery makes a focus on the tidyverse and ggplot2.\nBase R\nCheatsheet: Margins in base R Edit your margins in base R to accommodate axis titles, legends, captions, etc.!\nCustomizing tick marks in base R Seems like a simple thing, but it can be so frustrating! This is a great post about customizing tick marks with base plot in R.\nAnimations in R (for time series) If you want to use animations but don’t want to use ggplot2, this demo might help you!\nggplot2\nCheatsheet: ggplot2 Cheatsheet for ggplot2 in R - anything you want to do is probably covered here!\nCoding Club tutorial: Data Viz Part 1 - Beautiful and informative data visualization Great tutorial demonstrating how to customize titles, subtitles, captions, labels, colour palettes, and themes in ggplot2.\nCoding Club tutorial: Data Viz Part 2 - Customizing your figures Great tutorial demonstrating how to customize titles, subtitles, captions, labels, colour palettes, and themes in ggplot2.\nggplot flipbook A flipbook-style demonstration that builds and customizes plots line by line using ggplot in R.\ngganimate: A Grammar of Animated Graphics Package to create animated graphics in R (with ggplot2).\nPython\nThe Python Graph Gallery This website displays hundreds of charts, always providing the reproducible python code.\nPython Tutorial: Intro to Matplotlib Introduction to basic functionalities of the Python’s library Matplotlib covering basic plots, plot attributes, subplots and plotting the iris dataset.\nThe Art of Effective Visualization of Multi-dimensional Data Covers both univariate (one-dimension) and multivariate (multi-dimensional) data visualization strategies using the Python machine learning ecosystem.\nJulia\nJulia Plots Gallery Display of various plots with reproducible code in Julia.\nPlots in Julia Documentation for the Plots package in Julia, including demonstrations for animated plots, and links to tutorials.\nAnimations in Julia How to start making animated plots in Julia.\nCustomization\nChart Studio Web editor to create interactive plots with plotly. You can download the image as .html, or static images, without coding the figure yourself.\nPhyloPic Vector images of living organisms. This is great for ecologists who want to add silhouettes of their organisms onto their plots - search anything, and you will likely find it!\nAdd icons on your R plot Add special icons to your plot as a great way to customize it, and save space with labels!\nInspiration (pretty things!)\nInformation is Beautiful Collection of beautiful original visualizations about a variety of topics!\nTidyTuesday A weekly data project aimed at the R ecosystem, where people wrangle and visualize data in loads of creative ways. Browse what people have created (#TidyTuesday on Twitter is great too!), and the visualizations that have inspired each week’s theme.\nWind currents on Earth Dynamic and interactive map of wind currents on Earth.\nA Day in the Life of Americans Dynamic visualisation of how Americans spend their time in an average day.\n2019: The Year in Visual Stories and Graphics Collection of the most popular visualizations by the New York Times in 2019.\n\n\n\n",
    "preview": "posts/2020-09-21-data-visualization/thumb.jpg",
    "last_modified": "2021-06-21T13:15:04-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-15-science-communication/",
    "title": "Science Communication",
    "description": "Recordings, content and handouts from a 6-hour Science Communication workshop held over two days on 15 and 16 June 2020.",
    "author": [
      {
        "name": "Gracielle Higino",
        "url": {}
      },
      {
        "name": "Katherine Hébert",
        "url": {}
      }
    ],
    "date": "2020-06-15",
    "categories": [
      "Career",
      "Fellow contributed",
      "EN"
    ],
    "contents": "\n\nContents\nDay 1\nDay 2\nSession 1: The basics of science communication\nSession 2: Social media as a science communication tool\n\nThe objective of this training is to share and discuss the concepts and tools that contribute to effective science communication. The training is split into two sessions, which cover the basic concepts of effective science communication and how social media tools can be used to boost the signal of your research and extend your research network. Each training takes the form of a presentation interspersed with several short activity modules, where participants are invited to use the tools we will be discussing to kickstart their own science communication.\nThis training was given on June 1 and 2, 2020. You can view recordings of each session here:\nDay 1\n\n\nDay 2\n\n\nSession 1: The basics of science communication\nObjectives:\nDiscuss what science communication (or SciComm) can be, and its potential role in boosting the signal of your research\nMake an overview of basic concepts and tools that you can use in any medium (blog posts, presentations, conversations, twitter, etc.) to do effective science communication\nDuring this session, we:\nDiscuss the potential pitfalls of science communication (notably, diversity and inclusivity problems).\nCover the basic concepts of science communication, including the Golden Circle method, the creation of personas, and storytelling techniques.\nHave short activities where participants can try to use some of the techniques we will be covering, such as filling in their own Golden Circle and explaining a blog post as a storyboard.\n\n\n\nSession 2: Social media as a science communication tool\nObjectives:\nRethink the way we write about science by exploring the world of blog posts\nClarify the mechanics of Twitter and how it can be used effectively for science communication\nDuring this session, we:\nDiscuss how to create a story structure using titles and the flow of ideas in blog posts, especially when we are used to writing scientific articles\nCover the basics of how Twitter works (retweets, threads, replies, hashtags, photo captions, etc.) and how to find helpful connections\nHave short activities where participants will be invited to write their own Twitter biographies and to create a Twitter thread explaining a project of their choice.\n\n\n\n\n\n\n",
    "preview": "posts/2020-06-15-science-communication/scicomm_training.png",
    "last_modified": "2021-04-29T10:20:52-04:00",
    "input_file": {},
    "preview_width": 1548,
    "preview_height": 800
  },
  {
    "path": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/",
    "title": "Sensibilisation aux réalités autochtones et recherche collaborative",
    "description": "Série de deux webinaires sur la sensibilisation aux réalités autochtones et la recherche en collaboration avec les Autochtones, offert du 28 au 30 avril 2020 par Catherine-Alexandra Gagnon, PhD.",
    "author": [
      {
        "name": "Dr Catherine-Alexandra Gagnon",
        "url": {}
      }
    ],
    "date": "2020-04-28",
    "categories": [
      "Transversal competencies",
      "FR"
    ],
    "contents": "\n\nContents\nPartie 1 - Sensibilisation aux réalités autochtones\nObjectifs de la formation :\nDurant ce webminaire, nous allons: \n\nPartie 2 - Recherche en collaboration avec les communautés autochtones\nObjectifs de la formation :\nDurant ce webminaire, nous allons: \n\nRessources\nBalados\nLectures, sites internet\nFilmographie\nRéférences\nFormatrice :\n\n\nPartie 1 - Sensibilisation aux réalités autochtones\nObjectifs de la formation :\nAméliorer notre compréhension du passé et de ses impacts sur nos relations entre le avec les Peuples Autochtones.\nDévelopper des notions et compétences afin d’agir contre les préjugés et le racisme.\nDurant ce webminaire, nous allons: \nFaire un survol des événements historiques importants et de leurs impacts à ce jour (Loi sur les Indiens, politiques d’assimilation, les pensionnats, etc.). \nAcquérir des connaissances sur la terminologie autochtone.\nFaire un survol de certains procès et contextes légaux et voir comment ils affectent notre travail en territoire autochtone.\nDans une optique de réconciliation, faire une prise de conscience des préjugés persistants et discuter de stratégies pour améliorer nos relations avec les communautés.\nPartie 2 - Recherche en collaboration avec les communautés autochtones\nObjectifs de la formation :\nEntamer une réflexion collective envers nos pratiques de recherche et comment s’engager de manière significative avec les communautés autochtones.\nDévelopper une meilleure compréhension des perceptions et attentes des communautés envers la recherche et les chercheurs.\nDurant ce webminaire, nous allons: \nMieux comprendre la nécessité de prendre en compte les connaissances autochtones dans divers aspects de la gestion environnementale au Canada; \nDiscuter du désir des communauté d’avoir une présence accrue dans le milieu de la recherche : comment faire?\nAborder et débattre des différentes approches méthodologiques pour établir des ponts en les connaissances autochtones et scientifiques.\nRessources\nBalados\n\nLectures, sites internet\nTruth and Reconciliation Commission Reports\nFinal Report: National Inquiry into Missing and Murdered Indigenous Women and Girls\nReport of the Royal Commission on Aboriginal Peoples\nMythes et réalités sur les peuples autochtones\nWhere are the children? Healing the legacy of the residential schools\nIndigenous Canada (Coursera)\n“Indigenous Writes,” by Chelsea Vowel\n“21 things you might not know about the Indian Act: Helping Canadians make reconciliation with Indigenous Peoples a reality.”\nNorthern Quebec: Issues, Spaces and Cultures (MOOC Université Laval)\nFilmographie\nNational Film Board of Canada\nPeuples autochtones au Canada (Premières Nations et Métis)\nIndigenous Peoples in Canada (Inuit)\nWapikoni\nRéférences\n(Armitage et al. 2011): Co-managements and the co-production of knowledge: learning to adapt in Canada’s Arctic.\n(Berkes 1999): Sacred ecology: traditional ecological knowledge and resource management.1\n(Berkes 2009): Evolution of co-management: role of knowledge generation, bridging organizations and social learning.\n(Berkes, Colding, and Folke 2000): Rediscovery of traditional ecological knowledge as adaptive management.\n(Gagnon et al. 2020): Merging indigenous and scientific knowledge links climate with the growth of a large migratory caribou population.\n(Gagnon and Berteaux 2009): Integrating traditional ecological knowledge and ecological science: a question of scale.\n(Gearheard and Shirley 2009): Challenges in community-research relationships: learning from natural science in Nunavut.\n(Nickels, Shirley, and Laidler 2006): Negotiating research relationships with Inuit communities: a guide for researchers.\n(Kendrick, Lyver, and K’é Dene First Nation 2005): Denésqliné (Chipewyan) knowledge of barren-ground caribou (Rangifer tarandus groenlandicus) movements.\n(Kofinas et al. 2003): Towards a protocol for community monitoring of caribou body condition.\n(Kofinas G. 2002): Community contributions to ecological monitoring: knowledge co-production in the U.S.-Canada Arctic Borderlands.\n(Kofinas et al. 2000): Research planning in the face of change: the human role in reindeer/caribou systems.\n(Lyver and Dene First Nation 2010): Monitoring barren-ground caribou body condition with Denésôåiné traditional knowledge.\n(M. Mallory, Akearok, and Gilchrist 2006): Local ecological knowledge of the Sleeper and Split Islands.\n(M. L. Mallory et al. 2003): Local Ecological Knowledge of Ivory Gull Declines in Arctic Canada.\n(Ostrom and Ahn 2001): A social science perspective on social capital: social capital and collective action.\n(Plummer and FitzGibbon 2006): People matter: the importance of social capital in the co-management of natural resources.\n(Plummer and FitzGibbon 2007): Connecting adaptive co-management, social learning, and social capital through theory and practice.\n(Reid and Millennium Ecosystem Assessment (Program) 2006): Bridging scales and knowledge systems: concepts and applications in ecosystem assessment.\nFormatrice :\nCatherine-Alexandra Gagnon possède une expertise dans le travail collaboratif en milieux autochtones. Elle s’intéresse particulièrement à la mise en commun des savoirs locaux, autochtones et scientifiques. Elle détient un doctorat en Sciences de l’environnement et une maîtrise en Gestion de la faune de l’Université du Québec à Rimouski, un baccalauréat en biologie faunique de l’université McGill ainsi qu’un certificat en Études autochtones de l’université de Montréal. Durant ses études, elle a travaillé sur les connaissances locales et ancestrales des Aîné(e)s et chasseurs Inuit, Inuvialuit et Gwich’in du Nunavut, des Territoires du Nord-Ouest et du Yukon.\n\n\n\nArmitage, Derek, Fikret Berkes, Aaron Dale, Erik Kocho-Schellenberg, and Eva Patton. 2011. “Co-Management and the Co-Production of Knowledge: Learning to Adapt in Canada’s Arctic.” Global Environmental Change 21 (3): 995–1004. https://doi.org/10.1016/j.gloenvcha.2011.04.006.\n\n\nBerkes, Fikret. 1999. Sacred Ecology: Traditional Ecological Knowledge and Resource Management. Philadelphia, PA: Taylor & Francis.\n\n\n———. 2009. “Evolution of Co-Management: Role of Knowledge Generation, Bridging Organizations and Social Learning.” Journal of Environmental Management 90 (5): 1692–702. https://doi.org/10.1016/j.jenvman.2008.12.001.\n\n\nBerkes, Fikret, Johan Colding, and Carl Folke. 2000. “REDISCOVERY OF TRADITIONAL ECOLOGICAL KNOWLEDGE AS ADAPTIVE MANAGEMENT.” Ecological Applications 10 (5): 1251–62. https://doi.org/10.1890/1051-0761(2000)010[1251:ROTEKA]2.0.CO;2.\n\n\nGagnon, Catherine A., and Dominique Berteaux. 2009. “Integrating Traditional Ecological Knowledge and Ecological Science: A Question of Scale.” Ecology and Society 14 (2): art19. https://doi.org/10.5751/ES-02923-140219.\n\n\nGagnon, Catherine A., Sandra Hamel, Don E. Russell, Todd Powell, James Andre, Michael Y. Svoboda, and Dominique Berteaux. 2020. “Merging Indigenous and Scientific Knowledge Links Climate with the Growth of a Large Migratory Caribou Population.” Edited by Meredith Root‐Bernstein. Journal of Applied Ecology 57 (9): 1644–55. https://doi.org/10.1111/1365-2664.13558.\n\n\nGearheard, Shari, and Jamal Shirley. 2009. “Challenges in Community-Research Relationships: Learning from Natural Science in Nunavut.” ARCTIC 60 (1): 62–74. https://doi.org/10.14430/arctic266.\n\n\nKendrick, A, P O’B Lyver, and Łutsël K’é Dene First Nation. 2005. “Denésqliné (chipewyan) Knowledge of Barren-Ground Caribou (rangifer Tarandus Groenlandicus) Movements.” Arctic, 175–91.\n\n\nKofinas G., Aklavik. 2002. “Community Contributions to Ecological Monitoring: Knowledge Co-Production in the U.S.-Canada Arctic Borderlands.” In The Earth Is Faster Now: Indigenous Observations of Arctic Environmental Change, 55–91. Fairbanks, USA: Arctic Research Consortium of the United States.\n\n\nKofinas, Gary, Phil Lyver, Don Russell, Robert White, Augie Nelson, and Nicholas Flanders. 2003. “Towards a Protocol for Community Monitoring of Caribou Body Condition.” Rangifer 23 (5): 43. https://doi.org/10.7557/2.23.5.1678.\n\n\nKofinas, Gary, Gail Osherenko, David Klein, and Bruce Forbes. 2000. “Research Planning in the Face of Change: The Human Role in Reindeer/Caribou Systems.” Polar Research 19 (1): 3–21. https://doi.org/10.1111/j.1751-8369.2000.tb00323.x.\n\n\nLyver, P. O.’B., and Lutsël K’é Dene First Nation. 2010. “Monitoring Barren-Ground Caribou Body Condition with Denésôåiné Traditional Knowledge.” ARCTIC 58 (1): 44–54. https://doi.org/10.14430/arctic388.\n\n\nMallory, M., J. Akearok, and G. Gilchrist. 2006. “Local Ecological Knowledge of the Sleeper and Split Islands.” In Climate Change: Integrating Traditional and Scientific Knowledge, 203–8. Winnipeg, Manitoba, Canada: Aboriginal Issues Press.\n\n\nMallory, M. L., H. Grant Gilchrist, Alain J. Fontaine, and Jason A. Akearok. 2003. “Local Ecological Knowledge of Ivory Gull Declines in Arctic Canada.” ARCTIC 56 (3): 293–98. https://doi.org/10.14430/arctic625.\n\n\nNickels, Scot, Jamal Shirley, and Gita Laidler. 2006. Negotiating Research Relationships with Inuit Communities: A Guide for Researchers. Inuit Tapiriit Kanatami Ottawa, Ont.\n\n\nOstrom, Elinor, and TK Ahn. 2001. “A SOCIAL SCIENCE PERSPECTIVE ON SOCIAL CAPITAL: SOCIAL CAPITAL AND COLLECTIVE ACTION.”\n\n\nPlummer, Ryan, and John FitzGibbon. 2006. “People Matter: The Importance of Social Capital in the Co-Management of Natural Resources.” Natural Resources Forum 30 (1): 51–62. https://doi.org/10.1111/j.1477-8947.2006.00157.x.\n\n\n———. 2007. “Connecting Adaptive Co-Management, Social Learning, and Social Capital Through Theory and Practice.” Adaptive Co-Management: Collaboration, Learning, and Multi-Level Governance. University of British Columbia Press, Vancouver, British Columbia, Canada, 38–61.\n\n\nReid, Walter V., and Millennium Ecosystem Assessment (Program), eds. 2006. Bridging Scales and Knowledge Systems: Concepts and Applications in Ecosystem Assessment. A Contribution to the Millennium Ecosystem Assessment. Washington, D.C: Island Press.\n\n\nA more recent edition exists.↩︎\n",
    "preview": "posts/2020-04-28-sensibilisation-aux-ralits-autochtones-et-recherche-collaborative/thumb.jpg",
    "last_modified": "2021-06-21T13:15:04-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution/",
    "title": "Mathematical Modeling in Ecology and Evolution",
    "description": "This workshop will introduce participants to the logic behind modeling in biology, focusing on developing equations, finding equilibria, analyzing stability, and running simulations.Techniques will be illustrated with the software tools, Mathematica and Maxima. This workshop was held in two parts: January 14 and January 16, 2020.",
    "author": [
      {
        "name": "Dr Sarah P. Otto",
        "url": {}
      }
    ],
    "date": "2020-01-14",
    "categories": [
      "Technical",
      "EN"
    ],
    "contents": "\n\nContents\nContent\nSoftware\nMaterial\nOther resources\nThanks\n\nIn this workshop, I introduce various modelling techniques, using mostly ecological and evolutionary examples, with a focus on how computer software programs can help biologists analyze such models.\nContent\nPart 1: Classic one-variable models in ecology and evolutionPart 2: Equilibria and their stabilityPart 3: Beyond equilibriaPart 4: Example of building a model from scratchPart 5: Extending to models with more than one variablePart 6: Another example of building a model from scratch\nSoftware\nIn my research, I primarily use Mathematica, which is a powerful software package to organize and conduct analytical modelling, but it is not free (at UBC, we have some licenses available). I will also show some example code and provide translation of most of what I present in a free software package called Maxima.\nMathematica installation\nThere is a free trial version that you can use for 15 days, if you don’t have a copy (click here to access), or you can buy a student version online. If you want to make sure that all is working, copy the code below, put your cursor over each of the following lines and press enter (on some computers, “enter” is a separate button, on others, press “shift” and “return” at the same time):\nD[x^3,x]\nListPlot[Table[x, {x,1,10}],Joined->True]\nRSolve[{x[t+1]\\[Equal]A x[t],x[0]\\[Equal]x0},x[t],t]\nPDF[NormalDistribution[0,1],x]\nYou should see (a) \\(3x^2\\), (b) a plot of a line, (c) \\({{x[t]->A^t x0}}\\), and (d) \\(\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi }}\\).\nMaxima installation:\nOn a Mac, install using the instructions here. For other file systems, download here.\nMaxima testing\nWhen you first open Maxima, it will give you a choice of GUIs, chose wxMaxima. Once wxMaxima is launched type this command and hit return to see if it answers 4:\n2+2;\nIf it doesn’t, then scan the installation document for the error that you run into.\nIf it does return 4, then type in and enter these commands:\ndiff(x^3, x);\n\nwxplot2d (3*x, [x, 0, 2*%pi]);\n\nload(\"solve_rec\")$\nsolve_rec(x[t+1] = A*x[t], x[t], x[0]=x0);\n\nload(\"distrib\")$\npdf_normal(x,0,1);\nYou should see (a) \\(3x^2\\), (b) a plot of a line, (c) \\({{x[t]->A^t x0}}\\), and (d) \\(\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi }}\\).\nMaterial\nMathematica\nMaxima\nPDF\nNotebook\nNotebook\nEmbeded below\nHints and solutions\nHints and solutions\n\n\n\nHomework\nHomework answers\n\nHomework answers\nGuide\nGuide\n\nFollow along PDF\nThis PDF was generated from the Mathematica notebook linked above. It doesn’t include dynamic plots, but it’s a good alternative if you want to print out or have a quick reference at hand.\n\n\n\nStability analysis of a recursion equation in a discrete-time model.Other resources\nAn Introduction to Mathematical Modeling in Ecology and Evolution (Otto and Day 2007).\nBiomathematical modeling lecture notes.\nMathematica labs UBC.\nThanks\nNiki Love and Gil Henriques did a great job of translating the code into wxMaxima, with limited help from me. Thanks, Niki and Gil!!\n\n\n\nOtto, Sarah P, and Troy Day. 2007. A Biologist’s Guide to Mathematical Modeling in Ecology and Evolution. Vol. 13. Princeton University Press.\n\n\n\n\n",
    "preview": "posts/2020-01-14-mathematical-modeling-in-ecology-and-evolution/StabilityPictures.jpg",
    "last_modified": "2021-04-30T10:46:01-04:00",
    "input_file": {}
  }
]
